{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22a0d9-4eca-40a0-b92b-885af2ddaf12",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e107b99-e062-44ac-8182-f6da6c4b66b7",
   "metadata": {},
   "source": [
    "input_fpath = r\"E:\\030_CHOUSHUI_2024\\000_INSCALDEFO_2_INSSTACKPSI\\PROCESS_003\\14_POST-PROCESSING\\ras2pnt_MODIFIED_oChoushui_CUMDISP_LOS_mm.pkl\"\n",
    "df = pd.read_pickle(input_fpath, compression=\"zip\")\n",
    "df.head(3)\n",
    "\n",
    "input_fpath = r\"E:\\030_CHOUSHUI_2024\\000_INSCALDEFO_2_INSSTACKPSI\\PROCESS_003\\14_POST-PROCESSING\\ras2pnt_MODIFIED_oChoushui_CUMDISP_LOS_mm.pkl\"\n",
    "df = pd.read_pickle(input_fpath, compression=\"zip\")\n",
    "df.head(3)\n",
    "\n",
    "max_rows_per_split = 10_000\n",
    "df_split = np.array_split(df, np.ceil(len(df) / max_rows_per_split))\n",
    "type(df_split), len(df_split)\n",
    "\n",
    "start = time.time()  # Record the start time for performance tracking\n",
    "\n",
    "select_df = df_split[0]\n",
    "\n",
    "info_columns = select_df.iloc[:, :3]\n",
    "cumdisp_df = select_df.iloc[:, 3:-1]\n",
    "\n",
    "datetime_cols = cumdisp_df.columns.map(\n",
    "    datetime_handle.convert_to_datetime\n",
    ")\n",
    "cumdisp_df.columns = datetime_cols\n",
    "\n",
    "fulltime_arr = pd.date_range(datetime_cols[0], datetime_cols[-1])\n",
    "\n",
    "fulltime_df = pd.DataFrame(\n",
    "    columns=[_date for _date in fulltime_arr if _date not in datetime_cols],\n",
    "    index=np.arange(len(cumdisp_df)),\n",
    ")\n",
    "\n",
    "fulltime_df = fulltime_df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "fulltime_df = pd.concat([fulltime_df, cumdisp_df], axis=1)\n",
    "\n",
    "fulltime_cumdisp = fulltime_df[fulltime_arr]\n",
    "\n",
    "# fulltime_linear_trend_cumdisp = pd.concat([ele[0] for ele in fulltime_cumdisp.apply(analysis.get_linear_trend, axis=1)], axis=1)\n",
    "\n",
    "cumdisp_average_velocity = pd.DataFrame([ele[1] for ele in fulltime_cumdisp.apply(analysis.get_linear_trend, axis=1)], columns=[\"velo_mm_day\"])\n",
    "cumdisp_average_velocity[\"velo_mm_year\"] = cumdisp_average_velocity[\"velo_mm_day\"]*365.25\n",
    "\n",
    "cumdisp_average_velocity = pd.concat([info_columns, cumdisp_average_velocity], axis=1)\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# Performance Logging\n",
    "# ------------------------------------------------------------------------------\n",
    "# Record the end time of the process\n",
    "end = time.time()\n",
    "\n",
    "# Calculate elapsed time in seconds\n",
    "ellapse_time_seconds = round((end - start), 2)\n",
    "\n",
    "# Convert elapsed time to minutes and round to two decimal places\n",
    "ellapse_time_minutes = round((ellapse_time_seconds / 60), 2)\n",
    "\n",
    "# Convert elapsed time to hours and round to two decimal places\n",
    "ellapse_time_hours = round((ellapse_time_seconds / 3600), 2)\n",
    "\n",
    "# Print the total processing time in seconds\n",
    "print(\"Processing time (seconds): \" + str(ellapse_time_seconds))\n",
    "\n",
    "# Print the total processing time in minutes\n",
    "print(\"Processing time (minutes): \" + str(ellapse_time_minutes))\n",
    "\n",
    "# Print the total processing time in houzrs\n",
    "print(\"Processing time (hours): \" + str(ellapse_time_hours))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 3))\n",
    "ax = fig.add_subplot(111)dd\n",
    "pd.concat([ele[0] for ele in fulltime_displacement.apply(analysis.get_linear_trend, axis=1)], axis=1).T.iloc[0, :].plot(ax=ax)\n",
    "df_to_get_velocity.iloc[0, :].plot(ax=ax, marker='o', linestyle='--')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e45d1ede-56e9-4256-924b-28ad54431e55",
   "metadata": {},
   "source": [
    "def process_dataframe_chunk(chunk, date_converter, trend_calculator):\n",
    "    \"\"\"\n",
    "    Process a chunk of a DataFrame to calculate cumulative displacement velocities.\n",
    "\n",
    "    Parameters:\n",
    "    - chunk (pd.DataFrame): A chunk of the original DataFrame.\n",
    "    - date_converter (function): Function to convert column names to datetime.\n",
    "    - trend_calculator (function): Function to calculate the linear trend and velocity.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with cumulative displacement velocities.\n",
    "    \"\"\"\n",
    "    # Separate metadata and cumulative displacement data\n",
    "\n",
    "    \n",
    "    metadata_columns = chunk.iloc[:, :3]\n",
    "    displacement_data = chunk.iloc[:, 3:-1]\n",
    "\n",
    "    # Convert column names to datetime\n",
    "    datetime_indices = displacement_data.columns.map(date_converter)\n",
    "    displacement_data.columns = datetime_indices\n",
    "\n",
    "    # Generate a full range of dates\n",
    "    full_date_range = pd.date_range(datetime_indices[0], datetime_indices[-1])\n",
    "\n",
    "    # Create a DataFrame with missing dates filled with NaN\n",
    "    full_time_df = pd.DataFrame(\n",
    "        columns=[date for date in full_date_range if date not in datetime_indices],\n",
    "        index=np.arange(len(displacement_data))\n",
    "    ).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Combine the original and the full date range DataFrames\n",
    "    combined_df = pd.concat([full_time_df, displacement_data], axis=1)\n",
    "    combined_df = combined_df[full_date_range]\n",
    "\n",
    "    # Calculate average velocity using the trend calculator function\n",
    "    velocities = combined_df.apply(trend_calculator, axis=1)\n",
    "    velocity_df = pd.DataFrame(\n",
    "        [velocity[1] for velocity in velocities], \n",
    "        columns=[\"velocity_mm_per_day\"]\n",
    "    )\n",
    "    velocity_df[\"velocity_mm_per_year\"] = velocity_df[\"velocity_mm_per_day\"] * 365.25\n",
    "\n",
    "    # Combine metadata with calculated velocities\n",
    "    result_df = pd.concat([metadata_columns, velocity_df], axis=1)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec94c03a-e834-46c6-a374-8d294a1dbf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe_chunk(chunk, date_converter, trend_calculator):\n",
    "    \"\"\"\n",
    "    Process a chunk of a DataFrame to calculate cumulative displacement velocities.\n",
    "\n",
    "    Parameters:\n",
    "    - chunk (pd.DataFrame): A chunk of the original DataFrame.\n",
    "    - date_converter (function): Function to convert column names to datetime.\n",
    "    - trend_calculator (function): Function to calculate the linear trend and velocity.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Processed DataFrame with cumulative displacement velocities.\n",
    "    \"\"\"\n",
    "    # Separate metadata and cumulative displacement data\n",
    "    metadata_columns = chunk.iloc[:, :3]\n",
    "    displacement_data = chunk.iloc[:, 3:-1]\n",
    "\n",
    "    # Convert column names to datetime\n",
    "    datetime_indices = displacement_data.columns.map(date_converter)\n",
    "    displacement_data.columns = datetime_indices\n",
    "\n",
    "    # Generate a full range of dates\n",
    "    full_date_range = pd.date_range(datetime_indices[0], datetime_indices[-1])\n",
    "\n",
    "    # Create a DataFrame with missing dates filled with NaN\n",
    "    full_time_df = pd.DataFrame(\n",
    "        columns=[date for date in full_date_range if date not in datetime_indices],\n",
    "        index=np.arange(len(displacement_data))\n",
    "    ).apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Combine the original and the full date range DataFrames\n",
    "    combined_df = pd.concat([full_time_df, displacement_data], axis=1)\n",
    "    combined_df = combined_df[full_date_range]\n",
    "\n",
    "    # Calculate average velocity using the trend calculator function\n",
    "    velocities = combined_df.apply(trend_calculator, axis=1)\n",
    "    velocity_df = pd.DataFrame(\n",
    "        [velocity[1] for velocity in velocities], \n",
    "        columns=[\"velocity_mm_per_day\"]\n",
    "    )\n",
    "    velocity_df[\"velocity_mm_per_year\"] = velocity_df[\"velocity_mm_per_day\"] * 365.25\n",
    "\n",
    "    # Combine metadata with calculated velocities\n",
    "    result_df = pd.concat([metadata_columns, velocity_df], axis=1)\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed37476-24a8-4b7b-9697-59140e4f7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_fpath = r\"E:\\030_CHOUSHUI_2024\\000_INSCALDEFO_2_INSSTACKPSI\\PROCESS_003\\14_POST-PROCESSING\\ras2pnt_MODIFIED_oChoushui_CUMDISP_LOS_mm.pkl\"\n",
    "df = pd.read_pickle(input_fpath, compression=\"zip\")\n",
    "\n",
    "subdf = df.iloc[:10_000, :]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204e675-36d7-4414-89e7-ed06a06f9be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rows_per_split = 1_000\n",
    "subdf_split = np.array_split(subdf, np.ceil(len(subdf) / max_rows_per_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1979dc18-02e0-46d0-b041-52ac1edd9972",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk = subdf_split[1]\n",
    "\n",
    "# Separate metadata and cumulative displacement data\n",
    "metadata_columns = chunk.iloc[:, :3]\n",
    "displacement_data = chunk.iloc[:, 3:-1]\n",
    "\n",
    "# Convert column names to datetime\n",
    "datetime_indices = displacement_data.columns.map(datetime_handle.convert_to_datetime)\n",
    "displacement_data.columns = datetime_indices\n",
    "\n",
    "# Generate a full range of dates\n",
    "full_date_range = pd.date_range(datetime_indices[0], datetime_indices[-1])\n",
    "\n",
    "# Create a DataFrame with missing dates filled with NaN\n",
    "full_time_df = pd.DataFrame(\n",
    "    columns=[date for date in full_date_range if date not in datetime_indices],\n",
    "    index=np.arange(len(displacement_data))\n",
    ").apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Combine the original and the full date range DataFrames\n",
    "combined_df = pd.concat([full_time_df, displacement_data], axis=1)\n",
    "combined_df = combined_df[full_date_range]\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f891ec-9773-4c13-a748-44a63224b789",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d146b5f2-7419-4cf6-82dd-57e8bec2a69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dad3a00-cc38-445a-a9e7-2f54bbc4cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa19e4ec-c3dc-49eb-9225-00594ad654a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
