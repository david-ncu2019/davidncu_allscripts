{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb39a147-b7ed-4534-8945-4abd66a413e0",
   "metadata": {},
   "source": [
    "**2025/11/12**\n",
    "\n",
    "Shit, I have to come back to process the data, since I want to use more stations\n",
    "\n",
    "**2025/3/13**\n",
    "\n",
    "I have changed the way of calculating the `ref2base`, insteading using differences between 2 rings counting from top (ring 2 - ring 1), I will take the last ring at the reference, so it must be absolute of (ring 1 - ring 2), until (ring N-1 - ring N), so the last ring will be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0724d6a-d37f-4fe7-9301-81857bd08d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6e585-3ff2-4c07-9c05-50d9c81762aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_hdf5_metadata(hdf5_fpath, updates_dict):\n",
    "    \"\"\"\n",
    "    Updates an HDF5 file with new metadata and returns the final metadata.\n",
    "\n",
    "    This function reads the existing metadata, recursively merges it with the\n",
    "    new updates, and writes the complete, updated metadata back to the file.\n",
    "\n",
    "    Args:\n",
    "        hdf5_fpath (str): Path to the HDF5 file to update.\n",
    "        updates_dict (dict): A dictionary containing ONLY the new or\n",
    "                             updated metadata.\n",
    "\n",
    "    Returns:\n",
    "        dict: The complete, merged metadata dictionary.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the hdf5_fpath does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(hdf5_fpath):\n",
    "        raise FileNotFoundError(f\"File not found at: {hdf5_fpath}\")\n",
    "\n",
    "    # 1. Read existing data and metadata using your tool\n",
    "    # We only need the metadata, but open_HDF5 returns both\n",
    "    _, current_metadata_dict = h5pytools.open_HDF5(hdf5_fpath)\n",
    "\n",
    "    # 2. Merge old and new metadata\n",
    "    # merge_dicts will recursively update/add keys\n",
    "    updated_metadata = h5pytools.merge_dicts(\n",
    "        current_metadata_dict, updates_dict\n",
    "    )\n",
    "\n",
    "    # 3. Write the complete updated metadata back to the file\n",
    "    try:\n",
    "        with h5py.File(hdf5_fpath, \"a\") as hdf5_file:\n",
    "            # metadata_to_hdf5 overwrites all attributes and groups\n",
    "            # with the new, merged dictionary structure.\n",
    "            h5pytools.metadata_to_hdf5(hdf5_file, updated_metadata)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file writing: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 4. Return the new metadata dictionary as requested\n",
    "    return updated_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb20da-bc52-4f67-938a-95b504921899",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcb727b-01dc-45b8-90dd-af9cad1d5410",
   "metadata": {},
   "source": [
    "modify Dongshi file to match the structure of other file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07c7a80c-6e03-4b56-baac-f50cc6efe519",
   "metadata": {},
   "source": [
    "fpath = \"DONGSHI_MW_DSES.xlsx\"\n",
    "df = pd.read_excel(fpath, parse_dates=[0], index_col=[0])\n",
    "df.columns = [eval(col.split(\"_\")[-1].split(\" \")[0]) for col in df.columns]\n",
    "df = df.T\n",
    "df.to_excel(\n",
    "    r\"D:\\1000_SCRIPTS\\003_Project002\\20251111_GTWR003\\1_PrepareDatasets\\MLCW\\MLCW_Excel\\DONGSHI_MW_DSES.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da2f3b0-7a33-456d-8b87-e9fe660dad6d",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90e5749-eec4-4735-8530-bfd47b3fc62b",
   "metadata": {},
   "source": [
    "create new HDF5 containing original MLCW data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42ef021d-6803-4add-b94b-fc22f5b123a5",
   "metadata": {},
   "source": [
    "# --- HELPER FUNCTION TO FIX HDF5 ATTRIBUTE TYPES ---\n",
    "def clean_metadata(meta_dict):\n",
    "    import datetime  # Ensure this is imported for the cleaner function\n",
    "\n",
    "    \"\"\"\n",
    "    Converts types incompatible with HDF5 attributes (like Timestamps)\n",
    "    into compatible types (like strings).\n",
    "    \"\"\"\n",
    "    cleaned = {}\n",
    "    for key, value in meta_dict.items():\n",
    "        # Convert Timestamp to string\n",
    "        if isinstance(value, (pd.Timestamp, datetime.date, datetime.datetime)):\n",
    "            cleaned[key] = value.strftime(\"%Y-%m-%d\")\n",
    "        # Convert numpy scalars to native Python types (int/float)\n",
    "        elif isinstance(value, (np.integer, np.floating)):\n",
    "            cleaned[key] = value.item()\n",
    "        # Handle NaN (h5py doesn't like np.nan in attributes sometimes)\n",
    "        elif pd.isna(value):\n",
    "            cleaned[key] = \"NaN\"\n",
    "        else:\n",
    "            cleaned[key] = value\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "raw",
   "id": "429de811-5c70-4479-af0a-9c6291359b45",
   "metadata": {},
   "source": [
    "# read the geospatial data to include them into metadata\n",
    "mlcw_gdf = gpd.read_file(\n",
    "    r\"D:\\1000_SCRIPTS\\003_Project002\\20251111_GTWR003\\1_PrepareDatasets\\MLCW\\shapefiles\\mlcw_Nov2025_twd97.shp\",\n",
    "    read_geometry=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b605d1dc-aed8-4da8-9d4a-f7aa32155678",
   "metadata": {},
   "source": [
    "# 1. Setup paths and containers\n",
    "folder = r\"MLCW_Excel\"\n",
    "files = glob(os.path.join(folder, \"*.xlsx\"))\n",
    "\n",
    "# Dictionaries to hold data for ALL stations\n",
    "all_mlcw_data = {}\n",
    "all_mlcw_metadata = {}\n",
    "\n",
    "# 2. Iterate through all files\n",
    "for fpath in tqdm(files[:]):\n",
    "    try:\n",
    "        # --- Filename Parsing ---\n",
    "        base = os.path.basename(fpath).split(\".\")[0]\n",
    "        # Join parts after the first underscore to get code (adjust based on your specific filename format)\n",
    "        station_code = \"MW_\" + base.split(\"_\")[-1]\n",
    "\n",
    "        # --- Metadata Extraction ---\n",
    "        # Optimization: Using .iloc[0] assumes unique codes. Ensure your GDF has unique codes.\n",
    "        station_info = mlcw_gdf.query(\"Code==@station_code\")\n",
    "\n",
    "        if station_info.empty:\n",
    "            print(\n",
    "                f\"Warning: Station code {station_code} not found in GDF. Skipping {base}.\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        station_name = station_info.Ename.iloc[0]\n",
    "        # metadata_station = station_info.to_dict(orient=\"records\")[0]\n",
    "\n",
    "        # --- FIX APPLIED HERE ---\n",
    "        # 1. Extract raw dictionary\n",
    "        raw_metadata = station_info.to_dict(orient=\"records\")[0]\n",
    "        # 2. Clean it to remove Timestamps/Objects\n",
    "        metadata_station = clean_metadata(raw_metadata)\n",
    "\n",
    "        # --- Data Processing ---\n",
    "        df = pd.read_excel(fpath, index_col=[0])\n",
    "        df.columns = pd.to_datetime(df.columns)\n",
    "        df = df.sort_index(axis=1)\n",
    "        df = df * 1000  # Convert to desired unit\n",
    "\n",
    "        # if 2024 not in df.columns.year.unique():\n",
    "        #     continue\n",
    "\n",
    "        # Prepare arrays\n",
    "        depth_arr = df.index.tolist()\n",
    "        # Convert Timestamps to string format '%Y%m%d' for HDF5 compatibility (HDF5 handles strings better than objects)\n",
    "        date_arr = df.columns.strftime(\"%Y%m%d\").to_numpy().astype(\"S\")\n",
    "\n",
    "        # Calculations\n",
    "        ring_diff = df.diff(axis=0, periods=-1).abs()\n",
    "        ring_diff_ref2first = ring_diff.subtract(ring_diff.iloc[:, 0], axis=0)\n",
    "        ring_diff_ref2first_arr = ring_diff_ref2first.to_numpy()\n",
    "\n",
    "        # ring_diff_ref2first.to_excel(os.path.join(\"temp\", base + \".xlsx\"))\n",
    "\n",
    "        # Cumulative summation logic\n",
    "        # Note: Ensure your slicing [::-1] aligns with your physical depth logic (bottom-up vs top-down)\n",
    "        cumcompact_ref2base_arr = np.nancumsum(\n",
    "            ring_diff_ref2first_arr[::-1], axis=0\n",
    "        )[::-1]\n",
    "\n",
    "        # --- Aggregate into Main Dictionary ---\n",
    "        # We build the structure directly for this station\n",
    "        all_mlcw_data[station_name] = {\n",
    "            \"values\": {\n",
    "                \"original\": df.to_numpy(),\n",
    "                \"ref2base\": cumcompact_ref2base_arr,\n",
    "                \"ringbyring\": ring_diff_ref2first_arr,\n",
    "            },\n",
    "            \"date\": date_arr,\n",
    "            \"depth\": depth_arr,\n",
    "        }\n",
    "\n",
    "        # Store metadata\n",
    "        all_mlcw_metadata[station_name] = metadata_station\n",
    "\n",
    "        # print(f\"Processed station: {station_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {fpath}: {e}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e72e609b-2552-404f-b810-b271c2dd9604",
   "metadata": {},
   "source": [
    "# Metadata for the ROOT of the HDF5 file\n",
    "global_notes = {\n",
    "    \"File_Version\": \"1.0\",\n",
    "    \"Processing_Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Description_1\": (\n",
    "        \"I found that previously I removed some stations that I thought were not suitable for the GTWR method, \"\n",
    "        \"but when I changed to the GWR approach, I needed to utilize all stations I had because \"\n",
    "        \"I only needed the differential displacement.\"\n",
    "    ),\n",
    "    \"Created_by\": \"20251113_A_PrepareMLCW_again_new_ref2base\",\n",
    "    \"Source_Files_Count\": len(files),\n",
    "}\n",
    "\n",
    "# Define a key in all_mlcw_metadata to store the global notes separately\n",
    "all_mlcw_metadata[\"/\"] = global_notes\n",
    "\n",
    "# Note: The global notes dictionary is keyed by \"/\", which is valid input for recursive writing.\n",
    "combined_metadata = h5pytools.merge_dicts(\n",
    "    all_mlcw_metadata, {\"/\": global_notes}\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28a4b562-9e62-4131-b4ca-1579d5f79900",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_MLCW_CRFP_v13.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, all_mlcw_data)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, all_mlcw_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0852120c-e0a9-42fa-ae4c-e1fe5ff477f4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b196c94b-c8b5-4d3c-afcd-87169c75a7ec",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "now I need to classify the rings at each MLCW station"
   ]
  },
  {
   "cell_type": "raw",
   "id": "662cf1ed-887a-463e-88e7-bf8b1b231d88",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "h5_fpath = \"20251113_MLCW_CRFP_v13.h5\"\n",
    "# initiate MLCW class object\n",
    "mlcw_obj = MLCW(h5_fpath=h5_fpath)\n",
    "mlcw_data, mlcw_metadata = mlcw_obj.get_data()\n",
    "available_stations = mlcw_obj.list_stations()\n",
    "available_stations[:5], len(available_stations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55978b6b-4e2d-4041-8782-c344ecd72132",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "key2search = \"Code\"\n",
    "file_folder = (\n",
    "    r\"E:\\SUBSIDENCE_PROJECT_DATA\\2024_MLCW_ANALYSIS\\megnaticRingsLayer\"\n",
    ")\n",
    "\n",
    "# select_station = available_stations[0]\n",
    "for select_station in tqdm(available_stations):\n",
    "\n",
    "    station_code = mlcw_metadata[select_station][key2search]\n",
    "    filename2search = station_code.split(\"_\")[-1] + \"*.xlsx\"\n",
    "\n",
    "    found_files = glob(os.path.join(file_folder, filename2search))\n",
    "\n",
    "    if found_files:\n",
    "        file2open = found_files[0]\n",
    "    else:\n",
    "        print(f\"{select_station} {station_code},  File not found\")\n",
    "        # break\n",
    "\n",
    "    df = pd.read_excel(file2open)\n",
    "    new_colnames = [col.capitalize() for col in df.columns.tolist()]\n",
    "    df.columns = new_colnames\n",
    "\n",
    "    classify_dict = {\n",
    "        \"classify\": {\n",
    "            \"depth\": df[\"Depth(m)\"].tolist(),\n",
    "            \"layer\": df[\"Layer\"].tolist(),\n",
    "        }\n",
    "    }\n",
    "    mlcw_data[select_station].update(classify_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "901753f0-9ca5-479d-b35c-59906d79abb0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "new_node_attributes = {\n",
    "    \"Processing_Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Description_2\": \"I add the layer classification\",\n",
    "}\n",
    "\n",
    "# Run the function\n",
    "final_metadata = update_hdf5_metadata(hdf5_fpath=h5_fpath, updates_dict=new_node_attributes)\n",
    "\n",
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_MLCW_CRFP_v14.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, mlcw_data)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, final_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf292660-98f3-4015-864f-593cedb39866",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6f4cad-6467-43e4-b536-e7c58a82819a",
   "metadata": {},
   "source": [
    "now we need to compute the compaction by layers, using the layer formation we get from previous step"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57a449c5-969d-40e3-8040-e9ab1ed61358",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "h5_fpath = \"20251113_MLCW_CRFP_v14.h5\"\n",
    "# initiate MLCW class object\n",
    "mlcw_obj = MLCW(h5_fpath=h5_fpath)\n",
    "mlcw_data, mlcw_metadata = mlcw_obj.get_data()\n",
    "available_stations = mlcw_obj.list_stations()\n",
    "available_stations[:5], len(available_stations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40a1893b-f3ec-4de4-8ddd-d52a0c216447",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "for select_station in tqdm(available_stations):\n",
    "\n",
    "    # extract compaction ring by ring from the corresponding station\n",
    "    mlcw_ringbyring = mlcw_obj.build_dataframe(\n",
    "        station=select_station, value_type=\"ringbyring\"\n",
    "    )\n",
    "\n",
    "    # extract measurement data from station\n",
    "    data_byStation = mlcw_data[select_station]\n",
    "\n",
    "    classify_info = pd.DataFrame(data_byStation[\"classify\"])\n",
    "    # Decode byte strings in 'layer' column\n",
    "    classify_info[\"layer\"] = classify_info[\"layer\"].apply(\n",
    "        lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n",
    "    )\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    sum_comp_allLayer = pd.DataFrame(data=None)\n",
    "\n",
    "    # - - - - loop - - - -\n",
    "\n",
    "    # layer2process = sorted(set([ele for ele in classify_info[\"layer\"].tolist() if len(ele) <= 2]))\n",
    "\n",
    "    layer_order = 1\n",
    "    # for layer_order in [\"1\", \"2\", \"3\"]:\n",
    "    for filter_cond in [[\"F1\", \"T1\"], [\"F2\", \"T2\"], [\"F3\", \"T3\"], [\"F3隞乩?\"]]:\n",
    "\n",
    "        # filter_cond = [ele for ele in layer2process if layer_order in ele]\n",
    "\n",
    "        # extract info by select layer\n",
    "        classify_info_byLayer = classify_info[\n",
    "            classify_info[\"layer\"].isin(filter_cond)\n",
    "        ]\n",
    "        if len(classify_info_byLayer) > 0:\n",
    "\n",
    "            start_depth = classify_info_byLayer.iloc[0, 0]\n",
    "            end_depth = classify_info_byLayer.iloc[-1, 0]\n",
    "\n",
    "            mlcw_byLayer = mlcw_ringbyring.query(\n",
    "                \"index>=@start_depth & index<=@end_depth\"\n",
    "            )\n",
    "\n",
    "            sum_comp_byLayer = mlcw_byLayer.sum(axis=0)\n",
    "\n",
    "            sum_comp_byLayer.name = \"L\" + str(layer_order)\n",
    "\n",
    "            sum_comp_allLayer = pd.concat(\n",
    "                [sum_comp_allLayer, sum_comp_byLayer], axis=1\n",
    "            )\n",
    "\n",
    "        layer_order += 1\n",
    "\n",
    "    data_io.save_df_to_excel(\n",
    "        df_to_save=sum_comp_allLayer,\n",
    "        filepath=\"compact_by_layers.xlsx\",\n",
    "        sheet_name=select_station,\n",
    "        verbose=False,\n",
    "        index=True,\n",
    "    )\n",
    "    mlcw_data[select_station][\"values\"][\n",
    "        \"compactbylayer\"\n",
    "    ] = sum_comp_allLayer.T.to_numpy()\n",
    "\n",
    "    # --------- plotting to compare -------------------------------------\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(11.7, 8.3 * 2 / 3))\n",
    "    visualize.base_plot(\n",
    "        ax=ax,\n",
    "        data=sum_comp_allLayer.sum(axis=1),\n",
    "        label=\"Three Layers Cumulative Compaction\",\n",
    "        title=select_station,\n",
    "        marker=\"o\",\n",
    "        linestyle=\"--\",\n",
    "        color=\"gray\",\n",
    "        alpha=1,\n",
    "    )\n",
    "    visualize.base_plot(\n",
    "        ax=ax,\n",
    "        data=mlcw_ringbyring.sum(axis=0),\n",
    "        label=\"All Ring Cumulative Compaction\",\n",
    "        title=select_station,\n",
    "        marker=\"\",\n",
    "        linestyle=\"--\",\n",
    "        color=\"blue\",\n",
    "        lw=3,\n",
    "        alpha=1,\n",
    "    )\n",
    "    visualize.configure_axis(\n",
    "        ax=ax, tick_direction=\"out\", hide_spines=[\"top\", \"right\"]\n",
    "    )\n",
    "    visualize.configure_legend(ax=ax, fontsize_base=14)\n",
    "    visualize.configure_datetime_ticks(ax=ax, grid=True)\n",
    "    visualize.save_figure(\n",
    "        fig=fig, savepath=f\"figs_ringbyring\\\\{select_station}_compare.png\"\n",
    "    )\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "732bb2c4-9bc0-4107-a8ec-8feebbb87b05",
   "metadata": {},
   "source": [
    "new_node_attributes = {\n",
    "    \"Processing_Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Description_3\": \"Compute compaction by layer information obtained from previous step\",\n",
    "}\n",
    "\n",
    "# Run the function\n",
    "final_metadata = update_hdf5_metadata(\n",
    "    hdf5_fpath=h5_fpath, updates_dict=new_node_attributes\n",
    ")\n",
    "\n",
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_MLCW_CRFP_v15.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, mlcw_data)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, final_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b03543-f1d5-4e7d-b2a3-a1e44ac44f66",
   "metadata": {},
   "source": [
    "#### 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbaf78b-5cb1-40cf-a724-3bfd3940b3cd",
   "metadata": {},
   "source": [
    "add the monthly data into the HDF5 file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c2ba83c-2167-4784-a40b-6b9aa56819da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "h5_fpath = \"20251114_MLCW_CRFP_v15.h5\"\n",
    "# initiate MLCW class object\n",
    "mlcw_obj = MLCW(h5_fpath=h5_fpath)\n",
    "mlcw_data, mlcw_metadata = mlcw_obj.get_data()\n",
    "available_stations = mlcw_obj.list_stations()\n",
    "available_stations[:5], len(available_stations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dbd9e7f0-40db-48a8-9506-70006001b471",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# select_station = available_stations[0]\n",
    "for select_station in tqdm(available_stations):\n",
    "\n",
    "    value_types = [\"ref2base\", \"ringbyring\", \"compactbylayer\"]\n",
    "\n",
    "    mlcw_data[select_station][\"monthly_values\"] = {}\n",
    "\n",
    "    for value_type in value_types:\n",
    "        df_byValueType = mlcw_obj.build_dataframe(\n",
    "            station=select_station, value_type=value_type\n",
    "        )\n",
    "        monthly_df_byValueType = df_byValueType.resample(\"MS\", axis=1).mean()\n",
    "        mlcw_data[select_station][\"monthly_values\"][\n",
    "            value_type\n",
    "        ] = monthly_df_byValueType.to_numpy()\n",
    "\n",
    "    mlcw_data[select_station][\"monthly_date\"] = np.array(\n",
    "        monthly_df_byValueType.columns.strftime(\"%Y%m%d\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8fa89dd-11aa-4cd3-be93-dc43be4ace53",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "new_node_attributes = {\n",
    "    \"Processing_Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Description_4\": \"Add monthly data, first day of the month\",\n",
    "}\n",
    "\n",
    "# Run the function\n",
    "final_metadata = update_hdf5_metadata(\n",
    "    hdf5_fpath=h5_fpath, updates_dict=new_node_attributes\n",
    ")\n",
    "\n",
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_MLCW_CRFP_v16.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, mlcw_data)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, final_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60eaca6f-f2e8-49c6-ac50-53d4b16bdc74",
   "metadata": {},
   "source": [
    "#### 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78a39a-9866-4791-a837-53b70609e439",
   "metadata": {},
   "source": [
    "extract data since 2014 for applying into GWR"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f280409e-0dce-44bb-af3e-f50d691899df",
   "metadata": {},
   "source": [
    "h5_fpath = \"20251114_MLCW_CRFP_v16.h5\"\n",
    "# initiate MLCW class object\n",
    "mlcw_obj = MLCW(h5_fpath=h5_fpath)\n",
    "mlcw_data, mlcw_metadata = mlcw_obj.get_data()\n",
    "available_stations = mlcw_obj.list_stations()\n",
    "available_stations[:5], len(available_stations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2a929716-db83-4e10-8c12-f294f7e50ba6",
   "metadata": {},
   "source": [
    "cache = []\n",
    "\n",
    "# select_station = station2process[0]\n",
    "for select_station in tqdm(available_stations):\n",
    "\n",
    "    temp = [\n",
    "        {select_station: {segment: mlcw_data[select_station][segment]}}\n",
    "        for segment in [\"depth\", \"classify\", \"monthly_date\", \"monthly_values\"]\n",
    "    ]\n",
    "    cache.extend(temp)\n",
    "\n",
    "new_mlcw_data = merge_dicts(*cache)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0085c926-6bbb-42a6-8e6e-43d5ecc9a7c8",
   "metadata": {},
   "source": [
    "new_node_attributes = {\n",
    "    \"Processing_Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Description_5\": \"Extract monthly values only, export to MLCW_CRFP_monthly\",\n",
    "}\n",
    "\n",
    "# Run the function\n",
    "final_metadata = update_hdf5_metadata(\n",
    "    hdf5_fpath=h5_fpath, updates_dict=new_node_attributes\n",
    ")\n",
    "\n",
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_MLCW_CRFP_monthly_v3.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, new_mlcw_data)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, mlcw_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c8efe3-da43-4fbe-9569-32304169f600",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
