{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4a65c88-bd3c-4eda-9c87-831d12f79bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from run_workflow import *\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e9e1d-0870-45c7-8000-002fc182fe11",
   "metadata": {},
   "source": [
    "#### 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe39a82b-f679-488b-80c7-b81603207f87",
   "metadata": {},
   "source": [
    "we have some gaps in the original data, so we need to fill them with PCA"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ba304f1-03a4-4982-b5e3-b69c3916bb18",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def update_hdf5_metadata(hdf5_fpath, updates_dict):\n",
    "    \"\"\"\n",
    "    Updates an HDF5 file with new metadata and returns the final metadata.\n",
    "\n",
    "    This function reads the existing metadata, recursively merges it with the\n",
    "    new updates, and writes the complete, updated metadata back to the file.\n",
    "\n",
    "    Args:\n",
    "        hdf5_fpath (str): Path to the HDF5 file to update.\n",
    "        updates_dict (dict): A dictionary containing ONLY the new or\n",
    "                             updated metadata.\n",
    "\n",
    "    Returns:\n",
    "        dict: The complete, merged metadata dictionary.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the hdf5_fpath does not exist.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(hdf5_fpath):\n",
    "        raise FileNotFoundError(f\"File not found at: {hdf5_fpath}\")\n",
    "\n",
    "    # 1. Read existing data and metadata using your tool\n",
    "    # We only need the metadata, but open_HDF5 returns both\n",
    "    _, current_metadata_dict = h5pytools.open_HDF5(hdf5_fpath)\n",
    "\n",
    "    # 2. Merge old and new metadata\n",
    "    # merge_dicts will recursively update/add keys\n",
    "    updated_metadata = h5pytools.merge_dicts(\n",
    "        current_metadata_dict, updates_dict\n",
    "    )\n",
    "\n",
    "    # 3. Write the complete updated metadata back to the file\n",
    "    try:\n",
    "        with h5py.File(hdf5_fpath, \"a\") as hdf5_file:\n",
    "            # metadata_to_hdf5 overwrites all attributes and groups\n",
    "            # with the new, merged dictionary structure.\n",
    "            h5pytools.metadata_to_hdf5(hdf5_file, updated_metadata)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file writing: {e}\")\n",
    "        raise\n",
    "\n",
    "    # 4. Return the new metadata dictionary as requested\n",
    "    return updated_metadata"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b4f80ecf-bf3b-4b9b-8ca4-3cebac925c5a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "mlcw_fpath = r\"20251114_MLCW_CRFP_monthly_v3.h5\"\n",
    "mlcw_obj = MLCW(mlcw_fpath)\n",
    "\n",
    "mlcw_measures, mlcw_metadata = mlcw_obj.get_data()\n",
    "\n",
    "available_stations = mlcw_obj.list_stations()\n",
    "available_stations[:5], len(available_stations)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "29a95254-c79f-4606-9973-78ac364f38cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def run_processing_loop(\n",
    "    available_stations, mlcw_measures, fig_savefld=\"refine_timeseries/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Redesigned workflow to use the robust 'run_pca_imputation_workflow'\n",
    "    for outlier removal and imputation.\n",
    "    \"\"\"\n",
    "\n",
    "    if not os.path.exists(fig_savefld):\n",
    "        os.makedirs(fig_savefld)\n",
    "\n",
    "    cache = []\n",
    "    string_decoder = lambda arr: [x.decode(\"utf-8\") for x in arr]\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for select_station in tqdm(available_stations, desc=\"Processing Stations\"):\n",
    "        try:\n",
    "            # 1. --- Data Extraction (Same as your original) ---\n",
    "            measures_byStation = mlcw_measures[select_station]\n",
    "            monthly_date_arr = pd.to_datetime(\n",
    "                string_decoder(measures_byStation[\"monthly_date\"])\n",
    "            )\n",
    "            monthly_values_arr = measures_byStation[\"monthly_values\"][\n",
    "                \"compactbylayer\"\n",
    "            ]\n",
    "\n",
    "            cdisp_mlcw_df = pd.DataFrame(data={\"time\": monthly_date_arr})\n",
    "            n_layers = monthly_values_arr.shape[0]\n",
    "\n",
    "            for i in range(n_layers):\n",
    "                cdisp_mlcw_df[f\"Layer_{i+1}\"] = monthly_values_arr[i]\n",
    "\n",
    "            cdisp_mlcw_df = cdisp_mlcw_df.set_index(\"time\")\n",
    "\n",
    "            # This will hold the final, clean data\n",
    "            refined_output_df = pd.DataFrame(\n",
    "                data=None, index=cdisp_mlcw_df.index\n",
    "            )\n",
    "\n",
    "            # 2. --- New Layer-by-Layer Loop ---\n",
    "            for layer in cdisp_mlcw_df.columns:\n",
    "                target_array = cdisp_mlcw_df[layer]\n",
    "\n",
    "                # If the series is all zeros, just copy it and skip processing\n",
    "                if target_array.sum() == 0:\n",
    "                    refined_output_df[layer] = target_array\n",
    "                    continue\n",
    "\n",
    "                # 3. --- Outlier Removal (Your Z-Score Logic) ---\n",
    "                # We identify outliers on the *original* data and mark them as NaN\n",
    "\n",
    "                # Reshape for scaler\n",
    "                original_values = target_array.values.reshape(-1, 1)\n",
    "\n",
    "                # We need to scale *ignoring* existing NaNs to get a valid Z-score\n",
    "                valid_mask = ~np.isnan(original_values).flatten()\n",
    "\n",
    "                # If no valid data, just copy\n",
    "                if not valid_mask.any():\n",
    "                    refined_output_df[layer] = target_array\n",
    "                    continue\n",
    "\n",
    "                # Fit scaler only on valid data\n",
    "                scaler.fit(original_values[valid_mask, :])\n",
    "\n",
    "                # Transform all data (NaNs will remain NaN, which is fine)\n",
    "                scaled_values = scaler.transform(original_values)\n",
    "\n",
    "                # Identify outliers (Z-score > 3 or < -3)\n",
    "                filter_cond = (\n",
    "                    (scaled_values > 3) | (scaled_values < -3)\n",
    "                ).flatten()\n",
    "\n",
    "                # Create the new input series: original data + outliers marked as NaN\n",
    "                target_with_outliers_as_nan = target_array.copy()\n",
    "                target_with_outliers_as_nan[filter_cond] = np.nan\n",
    "\n",
    "                num_outliers = filter_cond.sum()\n",
    "                if num_outliers > 0:\n",
    "                    print(\n",
    "                        f\"  [{select_station} - {layer}] Marked {num_outliers} outliers as NaN.\"\n",
    "                    )\n",
    "\n",
    "                # 4. --- One-Shot Imputation (Our Tool) ---\n",
    "                # This single function now does all the work:\n",
    "                # - Auto-tunes window\n",
    "                # - Linear detrends\n",
    "                # - Grid-searches components\n",
    "                # - Imputes ALL NaNs (original + outliers)\n",
    "                # - Re-trends\n",
    "\n",
    "                # Note: We pass the *Pandas Series* directly\n",
    "                results = run_pca_imputation_workflow(\n",
    "                    target_with_outliers_as_nan\n",
    "                )\n",
    "\n",
    "                # The final, clean series\n",
    "                reconstructed_target = results[\"imputed_series\"]\n",
    "\n",
    "                # Store the result\n",
    "                refined_output_df[layer] = reconstructed_target\n",
    "\n",
    "            # 5. --- Store Results (Same as your original) ---\n",
    "            measures_byStation[\"monthly_values\"][\n",
    "                \"compactbylayer_PCA\"\n",
    "            ] = refined_output_df.T.values\n",
    "\n",
    "            cache.append({select_station: measures_byStation})\n",
    "\n",
    "            # 6. --- Visualization (Using standard Matplotlib) ---\n",
    "            fig, axes = plt.subplots(\n",
    "                nrows=n_layers, ncols=1, sharex=True, figsize=(11.7, 8.3)\n",
    "            )\n",
    "            if n_layers == 1:  # Handle case of a single layer\n",
    "                axes = [axes]\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "\n",
    "            for idx, layer in enumerate(cdisp_mlcw_df.columns):\n",
    "                # Plot the final, clean series\n",
    "                axes[idx].plot(\n",
    "                    refined_output_df.loc[:, layer],\n",
    "                    color=\"blue\",\n",
    "                    linestyle=\"--\",\n",
    "                    marker=\"o\",\n",
    "                    ms=2,\n",
    "                    label=\"Refined (Imputed + Outliers Removed)\",\n",
    "                )\n",
    "                # Plot the original, \"dirty\" data\n",
    "                axes[idx].plot(\n",
    "                    cdisp_mlcw_df.loc[:, layer],\n",
    "                    label=f\"Original ({layer})\",\n",
    "                    color=\"black\",\n",
    "                    alpha=0.7,\n",
    "                )\n",
    "\n",
    "                # --- Placeholder for your 'visualize' calls ---\n",
    "                # visualize.configure_axis(...)\n",
    "                axes[idx].grid(True, linestyle=\"--\", alpha=0.5)\n",
    "                axes[idx].spines[\"right\"].set_visible(False)\n",
    "                axes[idx].spines[\"top\"].set_visible(False)\n",
    "                axes[idx].legend(fontsize=10)\n",
    "                # --- End Placeholder ---\n",
    "\n",
    "            fig.suptitle(\n",
    "                t=select_station, y=0.95, fontweight=\"bold\", fontsize=16\n",
    "            )\n",
    "            fig.tight_layout()\n",
    "            # fig.autofmt_xdate(ha=\"center\", rotation=90) # This is good\n",
    "\n",
    "            save_path = os.path.join(fig_savefld, f\"{select_station}.png\")\n",
    "            # fig.savefig(save_path)\n",
    "            visualize.save_figure(fig=fig, savepath=save_path)\n",
    "\n",
    "            plt.close()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"!!! FAILED processing {select_station}: {e}\")\n",
    "            pass\n",
    "\n",
    "    return cache"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f4d2979-374d-48e5-86a5-a232b42f175b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "cache = []\n",
    "string_decoder = lambda arr: [x.decode(\"utf-8\") for x in arr]\n",
    "fig_savefld = \"refine_timeseries/\"\n",
    "\n",
    "if not os.path.exists(fig_savefld):\n",
    "    os.makedirs(fig_savefld)\n",
    "\n",
    "cache_results = run_processing_loop(\n",
    "    available_stations, mlcw_measures, fig_savefld\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40b2623a-2156-4340-9364-3620e0ad5cb6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "new_mlcw_measures = merge_dicts(*cache_results)\n",
    "\n",
    "new_node_attributes = {\n",
    "    \"Processing_Date\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"Description_6\": \"model and fill gap for the monthly values with PCA imputation\",\n",
    "}\n",
    "\n",
    "# Run the function\n",
    "final_metadata = update_hdf5_metadata(\n",
    "    hdf5_fpath=mlcw_fpath, updates_dict=new_node_attributes\n",
    ")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_MLCW_CRFP_monthly_v4.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, new_mlcw_measures)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, final_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14906939-972f-4ba6-8b72-09fc5bc01101",
   "metadata": {},
   "source": [
    "#### 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d36568e4-7e01-4d9a-ae01-258fc226b4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANHE', 'ANNAN', 'BEICHEN', 'CANLIN', 'DONGGUANG']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlcw_fpath = r\"20251114_MLCW_CRFP_monthly_v4.h5\"\n",
    "mlcw_obj = MLCW(mlcw_fpath)\n",
    "\n",
    "mlcw_measures, mlcw_metadata = mlcw_obj.get_data()\n",
    "\n",
    "available_stations = mlcw_obj.list_stations()\n",
    "available_stations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35fe87a0-b5ff-4f63-a84f-cb2c9416ac89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e44b0409232b4b6490f385a7b5b1067d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/39 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "combined_df = pd.DataFrame(data=None, index=None, dtype=np.float32)\n",
    "\n",
    "# select_station = available_stations[0]\n",
    "\n",
    "number_of_layers = []\n",
    "for select_station in tqdm(available_stations[:]):\n",
    "\n",
    "    string_decoder = lambda arr: [x.decode(\"utf-8\") for x in arr]\n",
    "\n",
    "    measures_byStation = mlcw_measures[select_station]\n",
    "    monthly_date_arr = pd.to_datetime(\n",
    "        string_decoder(measures_byStation[\"monthly_date\"])\n",
    "    )\n",
    "    monthly_values_arr = measures_byStation[\"monthly_values\"][\n",
    "        \"compactbylayer_PCA\"\n",
    "    ]\n",
    "\n",
    "    cdisp_mlcw_df = pd.DataFrame(data={\"time\": monthly_date_arr})\n",
    "\n",
    "    n_layers = monthly_values_arr.shape[0]\n",
    "    number_of_layers.append(n_layers)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        cdisp_mlcw_df[f\"Layer_{i+1}\"] = monthly_values_arr[i]\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # 2025/4/15 : decide to keep working with cumulative displacement\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    cdisp_mlcw_df = cdisp_mlcw_df.set_index(\"time\")\n",
    "    # cdisp_mlcw_df = cdisp_mlcw_df.interpolate(method=\"time\")\n",
    "\n",
    "    cdisp_mlcw_df = cdisp_mlcw_df.loc[:, :]\n",
    "\n",
    "    first_datetime = cdisp_mlcw_df.first_valid_index()\n",
    "    last_datetime = cdisp_mlcw_df.last_valid_index()\n",
    "\n",
    "    cdisp_mlcw_df = cdisp_mlcw_df.loc[first_datetime:last_datetime, :]\n",
    "\n",
    "    y_twd97 = mlcw_metadata[select_station][\"X_TWD97\"]\n",
    "    x_twd97 = mlcw_metadata[select_station][\"Y_TWD97\"]\n",
    "\n",
    "    cdisp_mlcw_df.insert(\n",
    "        loc=0, column=\"Y_TWD97\", value=[y_twd97] * len(cdisp_mlcw_df)\n",
    "    )\n",
    "    cdisp_mlcw_df.insert(\n",
    "        loc=0, column=\"X_TWD97\", value=[x_twd97] * len(cdisp_mlcw_df)\n",
    "    )\n",
    "    cdisp_mlcw_df.insert(\n",
    "        loc=0, column=\"STATION\", value=[select_station] * len(cdisp_mlcw_df)\n",
    "    )\n",
    "\n",
    "    combined_df = pd.concat([combined_df, cdisp_mlcw_df], axis=0)\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # 2025/4/8 : convert to displacement time series\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # disp_mlcw_df = cdisp_mlcw_df.diff(axis=0)\n",
    "    # disp_mlcw_df = disp_mlcw_df.loc[\"2014\":, :]\n",
    "\n",
    "    # first_datetime = disp_mlcw_df.first_valid_index()\n",
    "    # last_datetime = disp_mlcw_df.last_valid_index()\n",
    "\n",
    "    # disp_mlcw_df = disp_mlcw_df.loc[first_datetime:last_datetime, :]\n",
    "    # disp_mlcw_df.insert(loc=0, column=\"STATION\", value=[select_station]*len(disp_mlcw_df))\n",
    "\n",
    "    # combined_df = pd.concat([combined_df, disp_mlcw_df], axis=0)\n",
    "\n",
    "combined_df.to_pickle(r\"Monthly_MLCW_pca_CUMDISP_v3.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3eddbac-9abf-4208-8ac2-8f89d15011d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>STATION</th>\n",
       "      <th>X_TWD97</th>\n",
       "      <th>Y_TWD97</th>\n",
       "      <th>Layer_1</th>\n",
       "      <th>Layer_2</th>\n",
       "      <th>Layer_3</th>\n",
       "      <th>Layer_4</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2004-11-01</th>\n",
       "      <td>ANHE</td>\n",
       "      <td>2.602035e+06</td>\n",
       "      <td>179539.204623</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2004-12-01</th>\n",
       "      <td>ANHE</td>\n",
       "      <td>2.602035e+06</td>\n",
       "      <td>179539.204623</td>\n",
       "      <td>-3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-01</th>\n",
       "      <td>ANHE</td>\n",
       "      <td>2.602035e+06</td>\n",
       "      <td>179539.204623</td>\n",
       "      <td>-0.293140</td>\n",
       "      <td>-2.031953</td>\n",
       "      <td>3.554529</td>\n",
       "      <td>1.041336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-02-01</th>\n",
       "      <td>ANHE</td>\n",
       "      <td>2.602035e+06</td>\n",
       "      <td>179539.204623</td>\n",
       "      <td>-1.286995</td>\n",
       "      <td>-3.254393</td>\n",
       "      <td>1.902571</td>\n",
       "      <td>0.811722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-03-01</th>\n",
       "      <td>ANHE</td>\n",
       "      <td>2.602035e+06</td>\n",
       "      <td>179539.204623</td>\n",
       "      <td>-2.537195</td>\n",
       "      <td>-4.753250</td>\n",
       "      <td>-0.359285</td>\n",
       "      <td>0.440431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01</th>\n",
       "      <td>ZHUTANG</td>\n",
       "      <td>2.639635e+06</td>\n",
       "      <td>191944.074820</td>\n",
       "      <td>-28.000000</td>\n",
       "      <td>-116.000000</td>\n",
       "      <td>-88.000000</td>\n",
       "      <td>-41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-01</th>\n",
       "      <td>ZHUTANG</td>\n",
       "      <td>2.639635e+06</td>\n",
       "      <td>191944.074820</td>\n",
       "      <td>-29.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>-89.000000</td>\n",
       "      <td>-41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01</th>\n",
       "      <td>ZHUTANG</td>\n",
       "      <td>2.639635e+06</td>\n",
       "      <td>191944.074820</td>\n",
       "      <td>-29.000000</td>\n",
       "      <td>-117.000000</td>\n",
       "      <td>-89.000000</td>\n",
       "      <td>-41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-01</th>\n",
       "      <td>ZHUTANG</td>\n",
       "      <td>2.639635e+06</td>\n",
       "      <td>191944.074820</td>\n",
       "      <td>-31.000000</td>\n",
       "      <td>-118.000000</td>\n",
       "      <td>-90.000000</td>\n",
       "      <td>-43.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-01</th>\n",
       "      <td>ZHUTANG</td>\n",
       "      <td>2.639635e+06</td>\n",
       "      <td>191944.074820</td>\n",
       "      <td>-31.000000</td>\n",
       "      <td>-119.000000</td>\n",
       "      <td>-89.000000</td>\n",
       "      <td>-43.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7170 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            STATION       X_TWD97        Y_TWD97    Layer_1     Layer_2  \\\n",
       "time                                                                      \n",
       "2004-11-01     ANHE  2.602035e+06  179539.204623   0.000000    0.000000   \n",
       "2004-12-01     ANHE  2.602035e+06  179539.204623  -3.000000    4.000000   \n",
       "2005-01-01     ANHE  2.602035e+06  179539.204623  -0.293140   -2.031953   \n",
       "2005-02-01     ANHE  2.602035e+06  179539.204623  -1.286995   -3.254393   \n",
       "2005-03-01     ANHE  2.602035e+06  179539.204623  -2.537195   -4.753250   \n",
       "...             ...           ...            ...        ...         ...   \n",
       "2021-07-01  ZHUTANG  2.639635e+06  191944.074820 -28.000000 -116.000000   \n",
       "2021-08-01  ZHUTANG  2.639635e+06  191944.074820 -29.000000 -117.000000   \n",
       "2021-09-01  ZHUTANG  2.639635e+06  191944.074820 -29.000000 -117.000000   \n",
       "2021-10-01  ZHUTANG  2.639635e+06  191944.074820 -31.000000 -118.000000   \n",
       "2021-11-01  ZHUTANG  2.639635e+06  191944.074820 -31.000000 -119.000000   \n",
       "\n",
       "              Layer_3    Layer_4  \n",
       "time                              \n",
       "2004-11-01   0.000000   0.000000  \n",
       "2004-12-01  -1.000000   4.000000  \n",
       "2005-01-01   3.554529   1.041336  \n",
       "2005-02-01   1.902571   0.811722  \n",
       "2005-03-01  -0.359285   0.440431  \n",
       "...               ...        ...  \n",
       "2021-07-01 -88.000000 -41.000000  \n",
       "2021-08-01 -89.000000 -41.000000  \n",
       "2021-09-01 -89.000000 -41.000000  \n",
       "2021-10-01 -90.000000 -43.000000  \n",
       "2021-11-01 -89.000000 -43.000000  \n",
       "\n",
       "[7170 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "08f32fa97b5943e09ad7c9d7ad1c8b38": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "370c7561851c4db2904e09fa03b204b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5b70bd215c7e41f9bb5951c4570f1385": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_370c7561851c4db2904e09fa03b204b7",
       "style": "IPY_MODEL_a66768f106794384bff5ce637791ed0a",
       "value": " 39/39 [00:00&lt;00:00, 321.16it/s]"
      }
     },
     "88920bcf5fbf403aac3c4c2f85b95e77": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8bc574a8d24448f9b5701af146d6f97e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9b19f0a2802d4870b1a3f06b4f50e455": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_08f32fa97b5943e09ad7c9d7ad1c8b38",
       "style": "IPY_MODEL_baa589a34a944441a320e501c60191a6",
       "value": "100%"
      }
     },
     "a66768f106794384bff5ce637791ed0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "baa589a34a944441a320e501c60191a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bafc35b4d83d4e96b08fea05bb7996aa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_8bc574a8d24448f9b5701af146d6f97e",
       "max": 39,
       "style": "IPY_MODEL_88920bcf5fbf403aac3c4c2f85b95e77",
       "value": 39
      }
     },
     "cc7a1869a94441b9990f471937dab4fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "e44b0409232b4b6490f385a7b5b1067d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9b19f0a2802d4870b1a3f06b4f50e455",
        "IPY_MODEL_bafc35b4d83d4e96b08fea05bb7996aa",
        "IPY_MODEL_5b70bd215c7e41f9bb5951c4570f1385"
       ],
       "layout": "IPY_MODEL_cc7a1869a94441b9990f471937dab4fa"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
