{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd39f15-f65f-4501-b493-8905b7d9d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8994a246-907a-4de2-97d8-951244d53912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Filter leveling benchmarks which are used for InSAR correction in ChiaYi"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7671fb85-f48e-48ef-bf32-705373a34b5d",
   "metadata": {},
   "source": [
    "hdf5_fpath = r\"D:\\1000_SCRIPTS\\003_Project002\\20241219_LevelingData_to_HDF5\\20251111_All_LevelingData_LandSubsidence-wra-gov-tw.h5\"\n",
    "data_dict, metadata = h5pytools.open_HDF5(hdf5_fpath)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b45925f5-1c41-4b0e-ab27-3b699f0c066f",
   "metadata": {},
   "source": [
    "# now we look for stations in need only\n",
    "select_benchmark = gpd.read_file(\n",
    "    r\"D:\\1000_SCRIPTS\\003_Project002\\20251111_GTWR003\\Shapefiles\\benchmark_2correctInSAR_ChiaYi.shp\",\n",
    "    read_geometry=False,\n",
    ")\n",
    "select_benchmark_code = select_benchmark.iloc[:, 0].unique().tolist()\n",
    "mutual_code = sorted(set(metadata.keys()).intersection(select_benchmark_code))\n",
    "\n",
    "len(mutual_code)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a34591fb-a21d-4c46-8218-a49258eff3bb",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# test to see if information is correct\n",
    "for key in metadata.keys():\n",
    "    temp = metadata[key][\"CE_Name\"]\n",
    "    if temp == \"WRA02\":\n",
    "        print(key)\n",
    "        print(metadata[key])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a13dd0a1-cf6c-403f-b82b-b71beea7603d",
   "metadata": {},
   "source": [
    "combine_df = pd.DataFrame(data=None, index=None)\n",
    "\n",
    "# select_code = mutual_code[0]\n",
    "for select_code in tqdm(mutual_code[:]):\n",
    "    df_benchmarkcode = pd.DataFrame(data_dict[select_code])\n",
    "    df_benchmarkcode[\"date\"] = [\n",
    "        \"N\" + ele.decode(\"utf-8\") for ele in df_benchmarkcode[\"date\"]\n",
    "    ]\n",
    "    df_benchmarkcode = df_benchmarkcode.rename({\"values\": select_code}, axis=1)\n",
    "    df_benchmarkcode = df_benchmarkcode.set_index(\"date\").T\n",
    "    df_benchmarkcode[\"ALTCODE\"] = metadata[select_code][\"CE_Name\"]\n",
    "    df_benchmarkcode[\"X_TWD97\"] = metadata[select_code][\"X_TWD97\"]\n",
    "    df_benchmarkcode[\"Y_TWD97\"] = metadata[select_code][\"Y_TWD97\"]\n",
    "    df_benchmarkcode.columns.name = None\n",
    "    combine_df = pd.concat([combine_df, df_benchmarkcode], axis=0)\n",
    "\n",
    "len(combine_df.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b8f24b1-724b-483b-863d-78f93b393db4",
   "metadata": {},
   "source": [
    "info_cols = [\"ALTCODE\", \"X_TWD97\", \"Y_TWD97\"]\n",
    "measure_cols = [col for col in sorted(combine_df.columns) if col not in info_cols]\n",
    "final_output = combine_df.loc[:, info_cols+measure_cols]\n",
    "final_output = final_output.reset_index(drop=False)\n",
    "final_output = final_output.set_index(\"ALTCODE\")\n",
    "final_output = final_output.rename({\"index\":\"CODE\"}, axis=1)\n",
    "final_output.to_excel(\"Leveling_2correctInSAR_2016_2024_ChiaYi.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3d9bb7-90ed-4d96-b31b-a6ccf9e4f26e",
   "metadata": {},
   "source": [
    "#### InSAR points located within ChiaYi leveling buffer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c5729ac3-9cf0-48c5-82ae-34fe8638a87b",
   "metadata": {},
   "source": [
    "# filepath to all points\n",
    "fpath = (\n",
    "    r\"D:\\1000_SCRIPTS\\003_Project002\\20251111_GTWR003\\20251111_LevelingWork.gdb\"\n",
    ")\n",
    "layers = gpd.list_layers(fpath)\n",
    "print(layers)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f5151409-b99b-48f8-afd3-6d4ae9e40cae",
   "metadata": {},
   "source": [
    "insar_allpoints_gdf = gpd.read_file(\n",
    "    fpath, layer=\"CUMDISP_AllPoints_saveqgis_Oct2025\"\n",
    ")\n",
    "show(insar_allpoints_gdf)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e713f51e-bed4-400f-9d0e-8e401fe2b8ea",
   "metadata": {},
   "source": [
    "chiayi_leveling_gdf = gpd.read_file(fpath, layer=\"Leveling_ChiaYi\")\n",
    "chiayi_leveling_gdf.head(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b59a2935-ed93-4094-9fe9-5bc99d938871",
   "metadata": {},
   "source": [
    "insarpoints_in_buffer = chiayi_leveling_gdf.apply(\n",
    "    lambda row: find_point_neighbors(row, insar_allpoints_gdf, \"ALTCODE\", 200),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "result_gdf = pd.concat(insarpoints_in_buffer.tolist(), ignore_index=True)\n",
    "result_gdf.loc[:, result_gdf.columns[:-1]].to_pickle(\n",
    "    \"CUMDISP_in_ChiaYiLeveling_200m.xz\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9983a1-db55-4fd3-b471-4a4b2eb6b1a4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Calculate Slope"
   ]
  },
  {
   "cell_type": "raw",
   "id": "25692e3b-6a01-484f-98ab-7cb1e4d775a0",
   "metadata": {},
   "source": [
    "def get_slope(input_series, force=True):\n",
    "    fulltime_arr = datetime_handle.get_fulltime(input_series.index)\n",
    "    fulltime_timeseries = datetime_handle.fulltime_table(\n",
    "        df=input_series, fulltime_series=fulltime_arr\n",
    "    )\n",
    "    linear_trend, slope = analysis.get_linear_trend(\n",
    "        series=fulltime_timeseries, force_zero_intercept=force\n",
    "    )\n",
    "    return linear_trend, slope"
   ]
  },
  {
   "cell_type": "raw",
   "id": "717e88d3-faa1-4a0e-9e1a-0f5e6bcb19bd",
   "metadata": {},
   "source": [
    "# load timeseries of cumulative displacement of all points\n",
    "cumdisp_df = pd.read_pickle(\n",
    "    r\"D:\\1000_SCRIPTS\\003_Project002\\20250917_GTWR002\\2_KrigingInterpolation\\5_PostKriging\\Monthly_CUMDISP_saveqgis_Oct2025.xz\"\n",
    ")\n",
    "cumdisp_df = cumdisp_df.set_index(\"PointKey\")\n",
    "show(cumdisp_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b073efc-9552-401b-9506-7cd92a78ecd2",
   "metadata": {},
   "source": [
    "leveling_fpath = r\"Leveling_2correctInSAR_ChiaYi_2016_2024.xz\"\n",
    "leveling_df = pd.read_pickle(leveling_fpath)\n",
    "show(leveling_df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6efae08d-7747-4a0e-ab12-359066d5f5f3",
   "metadata": {},
   "source": [
    "points_byLeveling = pd.read_pickle(r\"CUMDISP_in_ChiaYiLeveling_200m.xz\")\n",
    "show(points_byLeveling)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc7ab0ec-cd56-42a9-b951-1747fb9868bd",
   "metadata": {},
   "source": [
    "cache = {\"ALTCODE\": [], \"Lev_slope\": [], \"InSAR_slope\": [], \"Delta_slope\": []}\n",
    "\n",
    "all_leveling_code = points_byLeveling[\"ALTCODE\"].unique()\n",
    "valid_leveling_code = leveling_df.index.unique()\n",
    "mutual_leveling_code = sorted(\n",
    "    set(all_leveling_code).intersection(set(valid_leveling_code))\n",
    ")\n",
    "\n",
    "select_leveling_code = mutual_leveling_code[0]\n",
    "\n",
    "for select_leveling_code in tqdm(mutual_leveling_code):\n",
    "    \n",
    "    cache[\"ALTCODE\"].append(select_leveling_code)\n",
    "    \n",
    "    # using the select benchmark code, we get the pointkey located within the leveling's buffer\n",
    "    pointkey_byCode = points_byLeveling.query(\"ALTCODE==@select_leveling_code\")[\n",
    "        \"PointKey\"\n",
    "    ].tolist()\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # deal with Leveling survey timeseries\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    \n",
    "    \n",
    "    leveling_measure_cols = [\n",
    "        col for col in leveling_df.columns if col.startswith(\"N\")\n",
    "    ]\n",
    "    \n",
    "    leveling_timeseries = leveling_df.loc[\n",
    "        select_leveling_code, leveling_measure_cols\n",
    "    ]\n",
    "    \n",
    "    leveling_timeseries = leveling_timeseries.astype(np.float64)\n",
    "    leveling_timeseries = leveling_timeseries.dropna()\n",
    "    \n",
    "    # reference the measurement to the first acquisition\n",
    "    # and convert to milimeter\n",
    "    leveling_timeseries = (leveling_timeseries - leveling_timeseries.iloc[0]) * 1000\n",
    "    \n",
    "    leveling_timeseries.index = pd.to_datetime(\n",
    "        [idx[1:] for idx in leveling_timeseries.index]\n",
    "    )\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # deal with CUMDISP timeseries\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    \n",
    "    cumdisp_byLevelingCode = cumdisp_df.loc[pointkey_byCode, :]\n",
    "    \n",
    "    cumdisp_measure_cols = [\n",
    "        col for col in cumdisp_byLevelingCode.columns if col.startswith(\"D\")\n",
    "    ]\n",
    "    \n",
    "    cumdisp_byLevelingCode = cumdisp_byLevelingCode.loc[:, cumdisp_measure_cols]\n",
    "    \n",
    "    average_cumdisp_byLevelingCode = cumdisp_byLevelingCode.mean(axis=0)\n",
    "    \n",
    "    # we add one day to convert datetime to the beginning of the month\n",
    "    average_cumdisp_byLevelingCode.index = [\n",
    "        pd.to_datetime(idx[1:]) + relativedelta(days=1)\n",
    "        for idx in average_cumdisp_byLevelingCode.index\n",
    "    ]\n",
    "    \n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # calculate slope value for each\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    \n",
    "    # leveling timeseries\n",
    "    leveling_trend, leveling_slope = get_slope(leveling_timeseries)\n",
    "    \n",
    "    # average cumdisp timeseries\n",
    "    cumdisp_trend, cumdisp_slope = get_slope(\n",
    "        average_cumdisp_byLevelingCode, force=False\n",
    "    )\n",
    "    \n",
    "    # get slope difference\n",
    "    delta_slope = leveling_slope - cumdisp_slope\n",
    "    \n",
    "    # Add information to dictionary for later saving\n",
    "    cache[\"Lev_slope\"].append(leveling_slope)\n",
    "    cache[\"InSAR_slope\"].append(cumdisp_slope)\n",
    "    cache[\"Delta_slope\"].append(delta_slope)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc3129f8-c846-4727-a325-d06e7b7de50f",
   "metadata": {},
   "source": [
    "output_df = pd.DataFrame(cache)\n",
    "output_df = output_df.set_index(\"ALTCODE\")\n",
    "for col in [\"X_TWD97\", \"Y_TWD97\"]:\n",
    "    output_df[col] = output_df.index.map(leveling_df[col])\n",
    "\n",
    "# show(output_df)\n",
    "output_df.to_pickle(leveling_fpath.replace(\".xz\", \"_deltaSlope.xz\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c24125-329d-4005-ad0a-fe26b789e663",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Merge all output table into one, and convert them to geospatial dataframe"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2d7c6a7-4101-4b38-8e54-2965ed6efd93",
   "metadata": {},
   "source": [
    "changhua_df = pd.read_pickle(\n",
    "    r\"Changhua_Leveling_2024_modified2016_2024_deltaSlope.xz\"\n",
    ")\n",
    "changhua_df = changhua_df.reset_index(drop=False)\n",
    "\n",
    "yunlin_df = pd.read_pickle(\n",
    "    r\"Yunlin_Leveling_2024_modified2016_2024_deltaSlope.xz\"\n",
    ")\n",
    "yunlin_df = yunlin_df.reset_index(drop=False)\n",
    "\n",
    "chiayi_df = pd.read_pickle(\n",
    "    r\"Leveling_2correctInSAR_ChiaYi_2016_2024_deltaSlope.xz\"\n",
    ")\n",
    "chiayi_df = chiayi_df.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f4eb3d2-f5d0-49c2-8b2d-77f7a8d68bd6",
   "metadata": {},
   "source": [
    "merged = pd.concat([changhua_df, yunlin_df, chiayi_df], ignore_index=True)\n",
    "merged_gdf = geospatial.convert_to_geodata(\n",
    "    df=merged, xcoord_col=\"X_TWD97\", ycoord_col=\"Y_TWD97\", crs_epsg=\"EPSG:3826\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b494333b-c8e4-4ff3-8092-aac899912787",
   "metadata": {},
   "source": [
    "merged_gdf.to_file(r\"Leveling_ChangYunChia_deltaSlope.shp\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56c7aa4c-0ab4-4368-9446-bed91b365b09",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "merged_gdf.explore(s='o', markersize=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b090f7d4-e835-48af-b207-958caa0c61b0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### side work"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cccfcf01-4e16-43f0-83d4-c5dde06d7aa9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### 2025/11/12 : I export all the points from MintPy results, keep coordinates and create a pointkey column\n",
    "\n",
    "gdf = gpd.read_file(r\"D:\\1000_SCRIPTS\\003_Project002\\20250917_GTWR002\\2_KrigingInterpolation\\export_vert_timeseries\\vertical_timeseries_twd97.shp\", read_geometry=False, columns=[\"POINT_X\", \"POINT_Y\"])\n",
    "\n",
    "gdf = gdf.rename({\"POINT_X\":\"X_TWD97\", \"POINT_Y\":\"Y_TWD97\"}, axis=1)\n",
    "pointkey_arr = [f\"X{int(x*1000)}Y{int(y*1000)}\" for x, y in zip(gdf[\"X_TWD97\"], gdf[\"Y_TWD97\"])]\n",
    "gdf.insert(loc=0, column=\"PointKey\", value=pointkey_arr)\n",
    "\n",
    "gdf = geospatial.convert_to_geodata(df=gdf, xcoord_col=\"X_TWD97\", ycoord_col=\"Y_TWD97\", crs_epsg=\"EPSG:3826\")\n",
    "\n",
    "gdf.to_file(\"CUMDISP_AllPoints_saveqgis_Oct2025.shp\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "76bbff1d-5032-46e1-9375-e784cf2aff6b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "#### Extract leveling data from 2021\n",
    "\n",
    "fpath = r\"Leveling_2correctInSAR_ChiaYi_AllTime.xlsx\"\n",
    "base = os.path.basename(fpath)\n",
    "\n",
    "df = pd.read_excel(fpath)\n",
    "\n",
    "df = df.set_index(\"ALTCODE\")\n",
    "\n",
    "info_cols = [col for col in df.columns if not col.startswith(\"N\")]\n",
    "datetime_cols = [col for col in df.columns if col.startswith(\"N\")]\n",
    "\n",
    "measure_df = df.loc[:, datetime_cols]\n",
    "# measure_df.columns = pd.to_datetime([col[1:] for col in measure_df.columns])\n",
    "\n",
    "temp = measure_df.loc[:, \"N2016\":]\n",
    "\n",
    "temp = temp.dropna(how=\"all\")\n",
    "\n",
    "\n",
    "valid_index = []\n",
    "\n",
    "select_idx = temp.index[0]\n",
    "for select_idx in temp.index:\n",
    "    temp2 = temp.loc[select_idx, :]\n",
    "    temp2 = temp2.dropna()\n",
    "    years = int(temp2.index[-1][1:5]) - int(temp2.index[0][1:5])\n",
    "    if years >= 6:\n",
    "        valid_index.append(select_idx)\n",
    "\n",
    "temp3 = temp.loc[valid_index, :]\n",
    "\n",
    "info_df = df.loc[:, info_cols]\n",
    "final_df = pd.concat([info_df.loc[temp3.index, :], temp3], axis=1)\n",
    "\n",
    "final_df.to_pickle(base.replace(\"AllTime.xlsx\", \"2016_2024.xz\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
