{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30deca91-2b1c-4e0e-b1ce-88927a988d4b",
   "metadata": {},
   "source": [
    "Below is an **extensive, plain-text pseudocode** describing the **workflow** of the `gwr.predict` function **step-by-step**, in a manner accessible even to a high school student. We focus on **what** the code is doing **sequentially** rather than the exact R syntax.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pseudocode: `gwr.predict` Function**\n",
    "\n",
    "```\n",
    "1.  Start Timer\n",
    "    - Create a record that notes the time when the function begins.\n",
    "\n",
    "2.  Receive Inputs\n",
    "    - formula: an R “formula” that describes the dependent and independent variables\n",
    "    - data: the dataset used for fitting GWR (the known observations)\n",
    "    - predictdata: the dataset (or locations) where we want to predict the dependent variable\n",
    "    - bw: the bandwidth for the kernel function\n",
    "    - kernel: the type of kernel (e.g., \"bisquare\", \"gaussian\", etc.)\n",
    "    - adaptive: whether the bandwidth is adaptive or fixed\n",
    "    - p, theta, longlat: parameters that describe distance calculations (e.g., Minkowski distance, coordinate rotation, whether to use Great-Circle distance, etc.)\n",
    "    - dMat1: optional distance matrix between fitting data points and prediction points\n",
    "    - dMat2: optional distance matrix among the fitting data points themselves\n",
    "\n",
    "3.  Check the 'data' Object\n",
    "    - If 'data' is a Spatial or sf object:\n",
    "      (a) Extract its coordinate system (proj4string or geometry)\n",
    "      (b) Extract the (x,y) coordinates (fd.locat)\n",
    "      (c) Convert the 'data' portion to a standard data.frame\n",
    "      (d) Count how many rows (fd.n) in these coordinates\n",
    "    - Otherwise, stop with an error message.\n",
    "\n",
    "4.  Create Model Frame (X and y) from the Formula\n",
    "    - Using the formula (like y ~ x1 + x2), R sets up:\n",
    "      (a) 'y': the dependent variable values\n",
    "      (b) 'x': the design matrix for the independent variables\n",
    "    - If there is an intercept, rename that column to \"Intercept\"\n",
    "    - Record the names of the independent variables (inde_vars)\n",
    "\n",
    "5.  Check the 'predictdata' Object\n",
    "    - If predictdata is missing, the function warns: \"We will predict at the same points we used for fitting.\"\n",
    "      (a) Then sets predictdata = data\n",
    "      (b) pd.locat = fd.locat (same coordinates)\n",
    "    - Otherwise, if predictdata is a Spatial or sf object:\n",
    "      (a) Extract the coordinates of these prediction points (pd.locat)\n",
    "      (b) Convert them to data.frame if needed\n",
    "      (c) Ensure all the independent variables are present in predictdata\n",
    "    - Count how many prediction points there are (pd.n)\n",
    "    - Build 'x.p' by extracting the columns that match inde_vars, plus a column of 1s for intercept\n",
    "    - Turn x.p into a matrix\n",
    "\n",
    "6.  Distance Matrices\n",
    "    - If dMat1 (the distance matrix from fitting points to prediction points) is not given:\n",
    "      (a) Possibly compute it on the fly with a function like gw.dist\n",
    "      (b) This is only done if total points (fd.n + pd.n) is not too large\n",
    "    - If dMat2 (the distance matrix among the fitting data) is not given:\n",
    "      (a) Possibly compute it if fd.n is small enough\n",
    "\n",
    "7.  GWR Fit (Part 1) – Prediction\n",
    "    - Create an empty matrix (wt) to store weights for each prediction point\n",
    "    - Create an array (xtxinv) to store some matrix inverses for each prediction point\n",
    "    - Create a matrix (betas1) to store the local coefficients for each prediction point\n",
    "    - Loop over each prediction point i from 1 to pd.n:\n",
    "      (a) dist.vi = dMat1[, i] (the vector of distances from each fitting data point to this prediction location i)\n",
    "      (b) W.i = gw.weight(dist.vi, bw, kernel, adaptive)  # compute kernel weights\n",
    "      (c) If predictdata is actually the same as the fitting data, it might set W.i[i] = 0 to avoid self-influence\n",
    "      (d) Save these weights in wt[, i]\n",
    "      (e) gw.resi = gw_reg_1(x, y, W.i)  # a function that does Weighted Least Squares\n",
    "          * returns the local coefficients and possibly a matrix inverse\n",
    "      (f) betas1[i, ] = gw.resi[[1]]  # store the local coefficients for this prediction point\n",
    "      (g) xtxinv[i, , ] = gw.resi[[2]]  # store the inverse matrix\n",
    "\n",
    "    - Once the loop finishes, we have local coefficients for each new location\n",
    "    - gw.predict = gw_fitted(x.p, betas1)  # multiply each row in x.p by the local coefficients to get predicted y\n",
    "\n",
    "8.  GWR Fit (Part 2) – Fitting Model on Original Data\n",
    "    - betas2 = matrix to hold local coefficients for the original data points\n",
    "    - S = matrix for “hat” values\n",
    "    - Loop over each fitting point i in 1..fd.n:\n",
    "      (a) dist.vi = dMat2[, i]  # distances among the fitting data\n",
    "      (b) W.i = gw.weight(dist.vi, bw, kernel, adaptive)\n",
    "      (c) gw.resi = gw_reg(...)  # Weighted LS for the original data\n",
    "      (d) betas2[i, ] = local coefficients\n",
    "      (e) S[i, ] = row of the S (hat) matrix\n",
    "    - Summarize to compute trace(S), etc., used for AIC or CV metrics\n",
    "    - sigma.hat = estimate of residual variance\n",
    "\n",
    "9.  Prediction Variance\n",
    "    - For each prediction point i:\n",
    "      (a) w2 = wt[, i] * wt[, i]\n",
    "      (b) w2x = x * w2\n",
    "      (c) xtw2x = t(x) %*% w2x\n",
    "      (d) build s0, a matrix for variance computations\n",
    "      (e) x.pi = x.p[i, ]\n",
    "      (f) s1 = x.pi %*% s0 %*% t(x.pi)\n",
    "      (g) pse = sqrt(sigma.hat) * sqrt(1 + s1)\n",
    "      (h) pvar = pse^2\n",
    "      (i) store in predict.var[i]\n",
    "\n",
    "10.  Assemble Results\n",
    "    - Combine betas1, gw.predict, predict.var into a data frame (gwr.pred.df)\n",
    "    - Name columns like “(var1)_coef”, “(var2)_coef”, “prediction”, “prediction_var”\n",
    "    - Possibly attach geometry if the input was Spatial or sf\n",
    "\n",
    "11.  Wrap Up and Return\n",
    "    - Record the stop time\n",
    "    - Store arguments and results in a list named res\n",
    "    - Mark class(res) = \"gwrm.pred\"\n",
    "    - Return res invisibly\n",
    "\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Explanation in Plain Terms**\n",
    "\n",
    "1. **We read user inputs**: the formula, data, and new points where we want predictions.  \n",
    "2. **We parse** the `data` to find coordinates and create the design matrix `x` plus the response `y`.  \n",
    "3. **We parse** the `predictdata` to find coordinates of unsampled points and create `x.p` for them.  \n",
    "4. **We handle** distance matrices:\n",
    "   - If not provided, the code might compute them on the fly (using `gw.dist`).  \n",
    "   - `dMat1` is for distances from known data to the new prediction locations.  \n",
    "   - `dMat2` is for distances among the known data points themselves.  \n",
    "5. **We do Weighted LS** for each new location:\n",
    "   - Use `dMat1` to get distances from that location to all data points.  \n",
    "   - Convert distances to weights with the chosen kernel.  \n",
    "   - Solve Weighted LS → local coefficients → store them in `betas1`.  \n",
    "   - Multiply `betas1` by `x.p` to get final predicted values `gw.predict`.  \n",
    "6. **We also do** Weighted LS for the original data (the “calibration” part) using `dMat2`—this helps compute AIC or other diagnostics.  \n",
    "7. **We compute** the variance of predictions if needed.  \n",
    "8. **We assemble** a data frame with local coefficients, predictions, and prediction variance.  \n",
    "9. **We build** an appropriate Spatial or sf object if needed.  \n",
    "10. **We return** the final object containing the results, plus timing info, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why This Matters**\n",
    "\n",
    "- The pseudocode clarifies that **GWR** is run at each new location.  \n",
    "- The **kernel weighting** depends on distances.  \n",
    "- We do a **Weighted LS** fit for each location, producing local coefficients.  \n",
    "- Finally, we generate **predictions** for those new locations.\n",
    "\n",
    "This step-by-step approach helps you **understand** the logic behind the code, **avoid confusion** with advanced R constructs, and see how **prediction** is performed for unsampled points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3585842b-62b4-4055-96c3-7f6594a3a45a",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081530dc-646d-4158-a904-b065265de52d",
   "metadata": {},
   "source": [
    "START\n",
    "\n",
    "1. Prepare Data:\n",
    "   a. Get the spatial coordinates (locations) of all data points.\n",
    "   b. Collect the independent variables (X) and the dependent variable (y).\n",
    "\n",
    "2. Define a Function to Measure Model Quality:\n",
    "   a. Create a function called \"calculate_diagnostic(bandwidth)\".\n",
    "   b. Inside this function:\n",
    "      i. Build a Geographically Weighted Regression (GWR) model using the given bandwidth.\n",
    "      ii. Fit the GWR model with the data (coordinates, X, and y).\n",
    "      iii. Calculate a score (like AICc) that tells you how good the model is.\n",
    "      iv. Return the score.\n",
    "\n",
    "3. Set the Search Range for Bandwidth:\n",
    "   a. Determine the minimum bandwidth (bw_min). For example, it could be half of the smallest distance between any two points.\n",
    "   b. Determine the maximum bandwidth (bw_max). This might be the largest distance between any two points.\n",
    "   c. (Alternatively, use fixed rules based on the data properties if working with a fixed distance kernel.)\n",
    "\n",
    "4. Find the Best Bandwidth Using Golden Section Search:\n",
    "   a. Start with the interval from bw_min to bw_max.\n",
    "   b. Repeat until the search interval is very small or a maximum number of iterations is reached:\n",
    "      i. Choose two candidate bandwidths inside the current interval using a fixed ratio (the golden section ratio).\n",
    "      ii. Use \"calculate_diagnostic\" to get the score for each candidate.\n",
    "      iii. Compare the scores:\n",
    "           - If the left candidate has a lower (better) score, discard the right part of the interval.\n",
    "           - Otherwise, discard the left part of the interval.\n",
    "   c. Narrow the interval each time until the best bandwidth is clear.\n",
    "\n",
    "5. Choose the Optimal Bandwidth:\n",
    "   a. The final candidate with the lowest diagnostic score is chosen as the optimal bandwidth.\n",
    "\n",
    "6. Return the Optimal Bandwidth.\n",
    "\n",
    "END\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
