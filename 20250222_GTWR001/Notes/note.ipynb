{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c891a5ac-d798-4c19-8d13-b7092a3ccf64",
   "metadata": {},
   "source": [
    "# Q1\n",
    "\n",
    "could you please help explain the following column names? from the results of GTWR calibration: **Intercept, CUMDISP, y, yhat, residual, time_stamp, Stud_residual, Intercept_SE, CUMDISP_SE, Intercept_TV, CUMDISP_TV, X_TWD97, Y_TWD97**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cb8541-ea58-41de-96cf-c9535b1b1b9c",
   "metadata": {},
   "source": [
    "Of course. Understanding the output of a model is the most important part of the analysis. I'm happy to explain what each of those column names means in the context of your GTWR model.\n",
    "\n",
    "Here is a breakdown of each column from your results file, explained in a step-by-step manner as you prefer.\n",
    "\n",
    "### **Explanation of GTWR Output Columns**\n",
    "\n",
    "The output from the GTWR calibration gives you a rich set of information for each data point. We can group the columns into three categories: **1) Model Coefficients**, **2) Model Performance**, and **3) Location and Time**.\n",
    "\n",
    "***\n",
    "\n",
    "#### **1. Model Coefficients and Their Significance**\n",
    "\n",
    "These columns describe the local regression model that was calibrated at each specific data point.\n",
    "\n",
    "* `Intercept`\n",
    "    * **What it is:** This is the *local intercept* or baseline value.\n",
    "    * **In your context:** It represents the estimated value of your target variable, **`Layer_1`**, when the independent variable (`CUMDISP`) is zero, at that specific location and time. Since it's a *local* intercept, its value can change from one point to another.\n",
    "\n",
    "* `CUMDISP`\n",
    "    * **What it is:** This is the *local coefficient* for your independent variable, `CUMDISP`.\n",
    "    * **In your context:** This is a key output. It shows the strength and direction of the relationship between `CUMDISP` and `Layer_1` at each specific point. For example, if the value in this column is `1.5` for a particular point, it means that at that location, a 1-unit increase in `CUMDISP` is associated with a 1.5-unit increase in `Layer_1`.\n",
    "\n",
    "* `Intercept_SE` and `CUMDISP_SE`\n",
    "    * **What it is:** These are the **Standard Errors** (SE) for the local intercept and the local `CUMDISP` coefficient, respectively.\n",
    "    * **How to interpret:** The Standard Error measures the precision or uncertainty of the coefficient estimate. A **smaller** SE indicates a more precise and reliable estimate.\n",
    "\n",
    "* `Intercept_TV` and `CUMDISP_TV`\n",
    "    * **What it is:** These are the **t-values** for the local intercept and the local `CUMDISP` coefficient. It is calculated as `(Coefficient / Standard Error)`.\n",
    "    * **How to interpret:** The t-value is used to assess the statistical significance of the local coefficients. A general rule of thumb is that an absolute t-value **greater than 1.96** suggests that the coefficient is statistically significant at a 95% confidence level (meaning it is unlikely to be zero). This is crucial for determining if the relationship between `CUMDISP` and `Layer_1` is meaningful at a specific location.\n",
    "\n",
    "***\n",
    "\n",
    "#### **2. Model Performance and Diagnostics**\n",
    "\n",
    "These columns tell you how well the model performed at each data point.\n",
    "\n",
    "* `y`\n",
    "    * **What it is:** This is the **original, observed value** of your dependent variable.\n",
    "    * **In your context:** This is the actual value from the `Layer_1` column in your input CSV file. It's the \"ground truth\" that the model is trying to predict.\n",
    "\n",
    "* `yhat`\n",
    "    * **What it is:** This is the **predicted value** (or \"fitted\" value) generated by the GTWR model for each data point.\n",
    "    * **How to interpret:** You can compare `yhat` directly to `y`. The closer they are, the better the model's prediction was for that specific point.\n",
    "\n",
    "* `residual`\n",
    "    * **What it is:** This is the model's error, calculated as `y - yhat` (observed value minus predicted value).\n",
    "    * **How to interpret:** A small residual (close to zero) means the model's prediction was very accurate for that point. A large positive residual means the model under-predicted the value, while a large negative residual means it over-predicted.\n",
    "\n",
    "* `Stud_residual`\n",
    "    * **What it is:** This is the **Studentized Residual**. It's a standardized version of the raw residual.\n",
    "    * **How to interpret:** This is very useful for identifying **outliers**. A general rule is that any data point with a Studentized Residual greater than +2.5 or less than -2.5 is a potential outlier where the model did not fit well.\n",
    "\n",
    "***\n",
    "\n",
    "#### **3. Location and Time Identifiers**\n",
    "\n",
    "These columns provide the context for where and when each local model was calibrated.\n",
    "\n",
    "* `X_TWD97` and `Y_TWD97`\n",
    "    * **What they are:** These are the spatial coordinates for each data point, carried over directly from your input file. They define the **\"where\"** of the analysis.\n",
    "\n",
    "* `time_stamp`\n",
    "    * **What it is:** This column indicates the time period for each observation.\n",
    "    * **In your context:** This column gets its value from the `monthly` column you specified in your `CONFIG`. The `gtwr` function renames it to `time_stamp` in the output. This defines the **\"when\"** of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecbe5a5-7bdf-483e-99d8-0bdf22798f5f",
   "metadata": {},
   "source": [
    "# Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d6cf1d-c6cd-4856-97c5-3d4311c3c9bf",
   "metadata": {},
   "source": [
    "why those values are not calculated when we input the regression points?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cef864-62db-48a4-8592-40c437639089",
   "metadata": {},
   "source": [
    "That is a very sharp and important question. It gets to the very core of how Geographically Weighted Regression (and GTWR) operates in two different modes: **calibration** and **prediction**.\n",
    "\n",
    "You are correct; those diagnostic columns are not calculated when you provide a separate set of `regression.points`. This is by design. Let's walk through the logic step-by-step.\n",
    "\n",
    "### **Chain-of-Thought: Why the Columns Disappear**\n",
    "\n",
    "1.  **The Two Modes of GTWR:** Think of the `gtwr` function as having two distinct jobs:\n",
    "    * **Job 1: Calibration (Default Mode).** When you *only* provide the `data` argument (your `input_csv_path` file), the function is in \"calibration mode.\" Its goal is to build the best possible local model at **each of the data points where it already knows the answer**. It uses the known `Layer_1` values to do this.\n",
    "    * **Job 2: Prediction (When `regression.points` is used).** When you provide *both* `data` and a separate `regression.points` dataset, the function switches to \"prediction mode.\" Its goal is to use the relationships it learned from the `data` to **estimate coefficients at the new, specified `regression.points`**.\n",
    "\n",
    "2.  **Defining the \"Missing\" Columns:** Let's look at the columns that are not calculated in prediction mode. They are all **diagnostic metrics**, which means they measure how well the model performed by comparing the predicted result to the *actual* result.\n",
    "\n",
    "    * `y`: This is the **observed**, true value of `Layer_1`.\n",
    "    * `yhat`: This is the model's **predicted** value of `Layer_1`.\n",
    "    * `residual`: This is the model's error, calculated as `y - yhat`.\n",
    "    * `Stud_residual`: This is a standardized version of the residual, used to spot outliers.\n",
    "    * `Intercept_SE`, `CUMDISP_SE`: These are the **Standard Errors**, which measure the uncertainty or precision of the local coefficient estimates.\n",
    "    * `Intercept_TV`, `CUMDISP_TV`: These are the **t-values**, which measure the statistical significance of the local coefficients.\n",
    "\n",
    "3.  **Connecting the Columns to the Mode:** Now, let's see why these columns can only exist in \"Calibration Mode.\"\n",
    "\n",
    "    * **Why can't we have `residual`?** To calculate a residual (the error), you must have both the observed value (`y`) and the predicted value (`yhat`). When you provide new `regression.points`, these are locations where you **do not know the true value of `Layer_1`**. The whole point of this mode is to predict it! Since there is no known `y` to compare against at these new points, it is mathematically impossible to calculate a residual.\n",
    "\n",
    "    * **Why can't we have `Stud_residual`?** Since this is just a standardized version of the residual, it also cannot be calculated.\n",
    "\n",
    "    * **Why can't we have Standard Errors (`_SE`) and t-values (`_TV`)?** This is a more technical point. These diagnostic statistics are derived from something called the \"hat matrix\" (`S`). This matrix describes how the original, observed `y` values are transformed into the fitted `yhat` values during the model calibration. It is a fundamental part of assessing the model's internal fit and uncertainty. When you are simply predicting at new locations, you are not performing this fitting procedure, so the hat matrix and its associated diagnostics are not generated for those new points.\n",
    "\n",
    "### **An Analogy: Building a Weather Forecast Model**\n",
    "\n",
    "Think of it like this:\n",
    "\n",
    "1.  **Calibration (No `regression.points`):** You gather 30 years of historical weather data for Taoyuan (temperature, humidity, wind speed, and whether it rained). You build a model to see how well temperature and humidity can \"predict\" rain *on those past days*. For each day, you have the **observed** result (it either rained or it didn't) and your model's **predicted** result. You can then calculate the `residual` (was the model wrong?), the `t-values` (was humidity a significant predictor on that day?), etc.\n",
    "\n",
    "2.  **Prediction (With `regression.points`):** Now, you use that model to predict the weather for **tomorrow**. You input tomorrow's expected temperature and humidity (`CUMDISP`). The model gives you the coefficients (`Intercept`, `CUMDISP`) and a prediction (`yhat`). However, you **cannot** calculate the `residual` for tomorrow's forecast until tomorrow actually happens and you see if it rains. The concept of an \"error\" or a \"t-value\" for a future prediction doesn't exist yet.\n",
    "\n",
    "**In summary:** When you provide `regression.points`, you are asking the function to switch from model building and self-assessment to pure prediction. The diagnostic columns are part of the self-assessment and are therefore only calculated when the function is run in its default calibration mode."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad00ec7d-87f5-4868-abd8-8ca63f7982a2",
   "metadata": {},
   "source": [
    "# Q3\n",
    "\n",
    "in a broader view, how a person working with machine learning methods view GTWR approach?\n",
    "\n",
    "could you please imagine you are in that person's perspective and give me some words.\n",
    "\n",
    "remember to keep your words plain and intuitive. I want to learn the bigger picture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b63035-f764-4463-86de-f68616aedb9b",
   "metadata": {},
   "source": [
    "Of course. Let's step into the shoes of a machine learning practitioner and look at GTWR from that perspective. The language and concepts will be different from a traditional statistician's, focusing more on features, hyperparameters, and model behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### A Machine Learning Perspective on GTWR\n",
    "\n",
    "\"Alright, as someone who spends their days working with models like XGBoost, Random Forests, and Neural Networks, here's my take on GTWR. When I first see it, I'm trying to figure out where it fits in my toolkit.\n",
    "\n",
    "#### 1. \"It's a Massively Local Ensemble of Linear Models\"\n",
    "\n",
    "My first thought isn't about statistics, it's about the model architecture. A typical model, like a neural network, learns one complex, global function. GTWR does the opposite.\n",
    "\n",
    "At its core, it's just **running thousands of simple, interpretable linear regressions**. The \"trick\" is that it runs a unique one for every single data point. You could almost think of it as an **ensemble method**, but instead of averaging different models, it deploys a different simple model for every specific location and time it's asked to predict.\n",
    "\n",
    "From a feature engineering perspective, this is quite clever. GTWR implicitly creates interaction features between your data and the spatio-temporal coordinates. The features are not just `CUMDISP`, but `CUMDISP * weight_based_on_location_and_time`. It’s automatically handling a very complex feature interaction that would be incredibly difficult to engineer by hand.\n",
    "\n",
    "#### 2. \"The Bandwidth is Just a Critical Hyperparameter\"\n",
    "\n",
    "In machine learning, we are obsessed with tuning hyperparameters to control the bias-variance tradeoff. For a decision tree, it's `max_depth`. For a neural network, it's the learning rate or number of layers.\n",
    "\n",
    "For GTWR, the **spatio-temporal bandwidth (`st.bw`) is the master hyperparameter.** It directly controls the model's complexity and how it generalizes.\n",
    "\n",
    "* **A small bandwidth** is like having a **high-variance, low-bias** model. It's \"overfitting\" to very local conditions. It will be extremely sensitive to small clusters of data but might not generalize well to new locations.\n",
    "* **A large bandwidth** is like having a **low-variance, high-bias** model. It's \"underfitting\" by averaging over a huge area and time, smoothing over the local details we care about. If the bandwidth is infinite, GTWR just becomes a simple, global linear regression.\n",
    "\n",
    "So, when I see the `bw.gtwr()` function, I immediately think, \"Ah, that's their version of `GridSearchCV` or Optuna.\" It's a data-driven way to find the hyperparameter that gives the best performance on unseen data (or based on a criterion like AICc).\n",
    "\n",
    "#### 3. \"Its Biggest Advantage is Local Interpretability\"\n",
    "\n",
    "This is where GTWR really shines compared to many powerful ML models. If I use a big XGBoost model, I can get a great prediction. But if someone asks me *why* the model made a specific prediction for one data point, it's very difficult to answer. I have to use secondary tools like SHAP or LIME to approximate an explanation.\n",
    "\n",
    "GTWR, on the other hand, is **natively interpretable at the local level**. For any single point in your dataset, the \"model\" is a simple linear equation:\n",
    "\n",
    "`y = 2.5 + 1.5 * CUMDISP` (for example)\n",
    "\n",
    "I can look directly at those coefficients and say, \"At this specific location and time, this is exactly how the model made its decision.\" This is a huge advantage for explaining results to stakeholders. It's not a black box.\n",
    "\n",
    "#### 4. \"It's a Specialist Tool, Not a General-Purpose Predictor\"\n",
    "\n",
    "So, would I use GTWR for every problem? No.\n",
    "\n",
    "If my goal is pure predictive accuracy on a massive, tabular dataset without an explicit spatio-temporal component, a well-tuned LightGBM or CatBoost model will likely be much faster and might even be more accurate. Those models are incredible at finding complex, non-linear patterns in high-dimensional feature spaces.\n",
    "\n",
    "But GTWR isn't trying to compete on that turf. It's a **specialist tool** designed for a specific task: **explicitly modeling and explaining spatio-temporal non-stationarity.**\n",
    "\n",
    "You choose GTWR when the *primary research question* is \"How do these relationships change across my map and over time?\". If the \"where\" and \"when\" are the most important parts of your analysis, then GTWR is the right tool for the job.\n",
    "\n",
    "**In short, my ML perspective is:** GTWR is an elegant, highly interpretable ensemble of local linear models, where the spatio-temporal bandwidth is a critical hyperparameter that must be tuned. It's not a replacement for gradient boosting, but it's the superior choice when the goal is to understand and explain *how* and *why* processes vary across a landscape."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75219f2d-8453-4e58-8ff8-8ba255dee2c5",
   "metadata": {},
   "source": [
    "# Q4:\n",
    "\n",
    "how would you view the GTWR approach if you are a person working with statistics methods rather than machine learning perspectives?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad0f6963-c46d-41f1-a310-6efe511da005",
   "metadata": {},
   "source": [
    "Of course. Let's switch perspectives. If I were a statistician looking at the GTWR approach, my focus would shift from predictive performance to model specification, inference, and its relationship to other established statistical methods.\n",
    "\n",
    "Here is how I would view the GTWR approach from a statistical perspective.\n",
    "\n",
    "---\n",
    "\n",
    "### A Statistician's Perspective on GTWR\n",
    "\n",
    "\"From a statistical viewpoint, GTWR is a powerful extension of the linear modeling framework. I see it less as a predictive algorithm and more as a sophisticated exploratory tool designed to investigate **non-stationarity**, which is a core concept in spatial statistics.\n",
    "\n",
    "Here’s my breakdown of its strengths and the statistical considerations that come with it.\n",
    "\n",
    "#### 1. It's Fundamentally a Method to Test and Model Non-Stationarity\n",
    "\n",
    "The most significant feature of GTWR is that it directly relaxes the \"stationarity\" assumption that underpins global models like Ordinary Least Squares (OLS).\n",
    "\n",
    "* **Global Models (OLS):** Assume that a single relationship holds true across the entire dataset. The coefficient `β₁` is constant everywhere and at all times.\n",
    "* **GTWR:** Makes the opposite assumption. It presumes that relationships are *not* constant and can vary locally. Its primary purpose is to explore this variation.\n",
    "\n",
    "A key statistical question is whether the observed variation in the GTWR coefficients is real or just random noise. The `GWmodel` package provides formal hypothesis tests, such as the Monte Carlo method in `gwr.montecarlo`, to test the significance of this spatial variation. This is a crucial step for a statistician; we don't just want to see variation, we want to know if it's statistically meaningful.\n",
    "\n",
    "#### 2. It's an Elegant Application of Weighted Least Squares (WLS)\n",
    "\n",
    "At its heart, GTWR is a local modeling technique based on a familiar statistical method: **Weighted Least Squares**.\n",
    "\n",
    "* **The Mechanism:** Instead of using weights to correct for issues like heteroskedasticity (unequal variance in residuals), GTWR re-purposes them to define a local \"neighborhood\" in space and time. The kernel function (e.g., Bisquare, Gaussian) is simply a weighting scheme, and the bandwidth is a **smoothing parameter**.\n",
    "* **The Connection to Non-Parametric Methods:** This makes GTWR conceptually similar to other non-parametric smoothing techniques, like Kernel Density Estimation or Locally Weighted Scatterplot Smoothing (LOESS). It makes minimal assumptions about the global form of the relationship, allowing the data itself to reveal the local patterns.\n",
    "\n",
    "#### 3. It Comes with Unique Inferential Challenges and Diagnostics\n",
    "\n",
    "A statistician would be immediately cautious about interpreting the outputs. While powerful, the local nature of GTWR introduces specific challenges that require careful handling.\n",
    "\n",
    "* **Local Multicollinearity:** Even if your global model has no multicollinearity, the small, local subsets of data used for each regression might. This can lead to unstable and unreliable coefficient estimates in certain areas. This is why tools like **`gwr.collin.diagno`**, which calculates local condition numbers and Variance Inflation Factors (VIFs), are essential diagnostic checks.\n",
    "* **Inference on Coefficients:** You cannot interpret the pseudo *t*-values from a GWR/GTWR model in the same way you would with OLS. The local estimates are based on overlapping data, so they are not independent. This inflates the test statistics. The `GWmodel` package acknowledges this statistical pitfall and provides the **`gwr.t.adjust`** function, which offers formal procedures (like Bonferroni or Benjamini-Hochberg corrections) to adjust p-values for multiple hypothesis testing.\n",
    "\n",
    "#### 4. It Occupies a Specific Niche Among Other Spatial Models\n",
    "\n",
    "A statistician would want to know when to use GTWR versus other established spatial models.\n",
    "\n",
    "* **vs. Mixed-Effects Models:** A mixed model could account for variation by fitting random slopes for different regions. This assumes the coefficients for each region are drawn from a single statistical distribution. GTWR is more flexible because it makes no such assumption; it allows the coefficients to vary freely across space.\n",
    "* **vs. Spatial Autoregressive (SAR) or Conditional Autoregressive (CAR) Models:** These models are designed to handle **spatial autocorrelation** (the idea that the *value* or *error* at one location is influenced by its neighbors). GTWR is designed to handle **spatial non-stationarity** (the idea that the *relationships* or *coefficients* change with location). They address different, though often related, spatial problems. The bootstrap functions in `gwr.bootstrap.rd` even allow you to formally test if your data is better explained by GTWR or by a global model with spatially correlated errors (like ERR or LAG models).\n",
    "\n",
    "**In summary, a statistician views GTWR as:**\n",
    "A sophisticated, non-parametric method for exploring and modeling process heterogeneity. It is not just a tool for prediction but a powerful diagnostic for investigating where and when relationships change. Its main strength lies in relaxing the stationarity assumption of global models, but this flexibility requires careful statistical handling, particularly concerning local collinearity and the interpretation of significance tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7732c23-a2bc-4592-b0f6-4b591f7044d0",
   "metadata": {},
   "source": [
    "# Q5\n",
    "\n",
    "how do I explain to non-expert audiences how the coeffcients of GTWR are obtained? please suggest me some ways to explain such process, in plain, simple, and intuitive words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba4eee-b640-42ee-9114-a7cdac1c5eaf",
   "metadata": {},
   "source": [
    "Of course. Explaining a complex statistical method to a non-expert audience is best done with a simple, powerful analogy. Instead of focusing on the math, we can focus on the logic.\n",
    "\n",
    "Here are a few ways to explain how GTWR obtains its coefficients, framed for a general audience.\n",
    "\n",
    "---\n",
    "\n",
    "### Method 1: The \"Hyper-Local Expert\" Analogy (Recommended)\n",
    "\n",
    "This is a great analogy because it clearly separates the global, local, and temporal aspects.\n",
    "\n",
    "**Start by setting the scene:**\n",
    "\n",
    "\"Imagine you want to understand what drives house prices across the country. A traditional model is like a single, national expert who creates one formula for everyone. It might say, `Price = (some number) * Square_Footage`. This formula is the same for a city apartment as it is for a country farmhouse, which isn't very realistic.\n",
    "\n",
    "GTWR works differently. Think of it as hiring a massive team of **hyper-local experts**. We send one expert to every single house that was sold, at the exact time it was sold.\"\n",
    "\n",
    "**Now, explain the four steps each expert follows:**\n",
    "\n",
    "1.  **Step 1: Focus on One House, at One Moment.**\n",
    "    * \"Our first expert stands at a specific house in Zhongli District, Taoyuan, right at the moment it was sold in June 2025. Their only job is to create a price formula that is perfect for *this single house at this single moment*.\"\n",
    "\n",
    "2.  **Step 2: Look at the Neighbors in Space and Time.**\n",
    "    * \"To create this unique formula, the expert looks at the sales of all the other houses in the dataset. But they don't treat them all equally.\"\n",
    "\n",
    "3.  **Step 3: Apply the \"Rule of Proximity\".**\n",
    "    * \"The expert follows a simple but powerful rule: **the closer a house is in both distance and time, the more its information matters.**\"\n",
    "    * \"A house sold last week just down the street is given a *huge weight*. Its data is very influential.\"\n",
    "    * \"A house sold across the city six months ago gets a *medium weight*. It's still relevant, but less so.\"\n",
    "    * \"A house sold on the other side of the country five years ago gets *almost zero weight*. Its data is considered irrelevant for this specific spot.\"\n",
    "\n",
    "4.  **Step 4: Create the Unique, Local Formula.**\n",
    "    * \"Using only this 'weighted' information, our expert creates a custom formula just for that one house in Zhongli:\n",
    "        `Price_in_Zhongli_June_2025 = (2.5 * Square_Footage) + (0.8 * Proximity_to_Park)`\n",
    "    * \"Those numbers—`2.5` and `0.8`—are the **local coefficients**. They represent the unique 'rules' of the housing market at that precise location and time.\"\n",
    "\n",
    "**Finally, explain the \"repeat\" step:**\n",
    "\n",
    "\"Now, the most important part: a second expert does this entire 4-step process for the house next door, creating a slightly different formula. And a third expert does it for a house sold a month later, and so on. **GTWR repeats this process for every single data point**, ultimately producing thousands of unique, local formulas. The result isn't one set of coefficients; it's a rich map showing how the 'rules' of the housing market change across geography and evolve over time.\"\n",
    "\n",
    "---\n",
    "\n",
    "### Method 2: The \"Custom Recipe\" Analogy (A Simpler Alternative)\n",
    "\n",
    "\"Imagine you have a single, standard recipe for baking a cake. That's a traditional model. It works, but it's not perfect for everyone.\n",
    "\n",
    "GTWR is like a chef who creates a **unique, custom recipe for every single person.**\n",
    "\n",
    "1.  The chef focuses on one person at a time.\n",
    "2.  They look at the favorite ingredients of that person's **closest neighbors** (spatial proximity) and what those neighbors have eaten **most recently** (temporal proximity).\n",
    "3.  The chef then writes a custom recipe just for that one person. The amounts in that unique recipe—like '1.2 cups of flour' and '0.8 cups of sugar'—are the **local coefficients**.\n",
    "4.  Then, the chef moves to the next person and creates a brand-new custom recipe from scratch.\n",
    "\n",
    "By doing this for everyone, the chef ends up with a collection of thousands of personalized recipes instead of just one, giving a much better understanding of what people truly want.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bd689f-83b3-4264-a164-dab460276063",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d4afee-2d55-4a5b-b911-96cd09907fc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61df131a-cf0b-438f-9f52-b3546cf49582",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
