{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65172303-1ad6-4ef3-9873-90da4ce54076",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T03:26:25.548512Z",
     "iopub.status.busy": "2025-05-05T03:26:25.548512Z",
     "iopub.status.idle": "2025-05-05T03:26:28.123246Z",
     "shell.execute_reply": "2025-05-05T03:26:28.122754Z",
     "shell.execute_reply.started": "2025-05-05T03:26:25.548512Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from pca_imputation import *\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def run_pca_imputation_workflow(data, time_col=None, value_col=None):\n",
    "    \"\"\"Execute PCA imputation workflow with parameter optimization and validation.\n",
    "\n",
    "    Args:\n",
    "        data: pandas Series with datetime index or DataFrame\n",
    "        time_col: column name for time if data is DataFrame\n",
    "        value_col: column name for value if data is DataFrame\n",
    "\n",
    "    Returns:\n",
    "        dict: Results containing imputation, parameters, and validation metrics\n",
    "    \"\"\"\n",
    "    # Validate data size and determine appropriate parameters\n",
    "    data_length = len(data)\n",
    "\n",
    "    # Calculate appropriate embedding dimensions based on data size\n",
    "    # Ensure embedding dimension doesn't exceed 1/3 of data length\n",
    "    max_embedding = max(2, data_length // 3)\n",
    "\n",
    "    # Generate embedding dimensions from 2 to max_embedding (capped at 12)\n",
    "    embedding_dims = list(range(2, min(max_embedding + 1, 13), 2))\n",
    "\n",
    "    # Components must be less than embedding dimensions\n",
    "    n_components_list = [min(dim - 1, 3) for dim in embedding_dims]\n",
    "    # n_components_list = [max(2, min(dim-1, 3)) for dim in embedding_dims]\n",
    "    n_components_list = list(set(n_components_list))  # Remove duplicates\n",
    "\n",
    "    # Step 1: Parameter Optimization with safeguards\n",
    "    try:\n",
    "        param_results = parameter_grid_search(\n",
    "            data=data,\n",
    "            embedding_dims=embedding_dims,\n",
    "            n_components_list=n_components_list,\n",
    "            mask_ratio=min(0.1, 0.5 * (1 - data.isna().mean())),  # Adaptive masking\n",
    "            random_seed=42,\n",
    "            time_col=time_col,\n",
    "            value_col=value_col,\n",
    "            use_cross_validation=False,\n",
    "        )\n",
    "\n",
    "        # Get optimal parameters\n",
    "        best_params = param_results.loc[param_results[\"rmse\"].idxmin()]\n",
    "        best_embedding_dim = int(best_params[\"embedding_dim\"])\n",
    "        best_n_components = int(best_params[\"n_components\"])\n",
    "    except Exception as e:\n",
    "        # Fallback to minimal parameters if grid search fails\n",
    "        best_embedding_dim = 2\n",
    "        best_n_components = 1\n",
    "\n",
    "    # Step 2: PCA Imputation with Optimal Parameters\n",
    "    results = impute_time_series(\n",
    "        data=data,\n",
    "        embedding_dim=best_embedding_dim,\n",
    "        n_components=best_n_components,\n",
    "        time_delay=1,\n",
    "        time_col=time_col,\n",
    "        value_col=value_col,\n",
    "    )\n",
    "\n",
    "    # Step 3: Validation with minimal additional masking\n",
    "    try:\n",
    "        validation_metrics, original_values, imputed_values, mask_indices = (\n",
    "            validate_imputation_accuracy(\n",
    "                data=data,\n",
    "                embedding_dim=best_embedding_dim,\n",
    "                n_components=best_n_components,\n",
    "                time_delay=1,\n",
    "                mask_ratio=min(0.05, 0.5 * (1 - data.isna().mean())),  # Conservative masking\n",
    "                random_seed=42,\n",
    "                time_col=time_col,\n",
    "                value_col=value_col,\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        validation_metrics = {\"rmse\": np.nan, \"mae\": np.nan, \"r2\": np.nan}\n",
    "        original_values = np.array([])\n",
    "        imputed_values = np.array([])\n",
    "        mask_indices = np.array([])\n",
    "\n",
    "    return {\n",
    "        \"imputation_results\": results,\n",
    "        \"optimal_parameters\": {\n",
    "            \"embedding_dim\": best_embedding_dim,\n",
    "            \"n_components\": best_n_components,\n",
    "        },\n",
    "        \"validation_metrics\": validation_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48a5a159-685e-4946-960d-d16f3c630e10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-05T03:26:28.124238Z",
     "iopub.status.busy": "2025-05-05T03:26:28.123743Z",
     "iopub.status.idle": "2025-05-05T03:27:01.594762Z",
     "shell.execute_reply": "2025-05-05T03:27:01.594762Z",
     "shell.execute_reply.started": "2025-05-05T03:26:28.124238Z"
    }
   },
   "outputs": [],
   "source": [
    "gps_files = glob(\"GPS_Files/*.csv\")\n",
    "# select_file = gps_files[0]\n",
    "for select_file in gps_files:\n",
    "    basename = os.path.basename(select_file)\n",
    "    df = pd.read_csv(select_file, parse_dates=[0], index_col=[0], usecols=[0, 3])\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    target_array = df[\"dU(mm)\"]\n",
    "    _trend, _slope = get_polynomial_trend(series=target_array, order=3)\n",
    "    _detrend = target_array - _trend\n",
    "    scaled_values = scaler.fit_transform(_detrend.values.reshape(-1, 1))\n",
    "    \n",
    "    # Reconstruct scaled series, maintaining original index\n",
    "    scaled_series = pd.Series(scaled_values.flatten(), index=_detrend.index, name=\"Scaled Values\")\n",
    "    filter_cond = (scaled_series > 3) | (scaled_series < -3)\n",
    "    scaled_series[filter_cond] = np.nan\n",
    "    \n",
    "    results = run_pca_imputation_workflow(scaled_series)\n",
    "    reconstructed_scaled_detrend = results[\"imputation_results\"][\"reconstructed_series\"]\n",
    "    reconstructed_detrend = scaler.inverse_transform(reconstructed_scaled_detrend.reshape(-1, 1))\n",
    "    reconstructed_detrend_series = pd.Series(data=reconstructed_detrend.flatten(), index=_detrend.index)\n",
    "    \n",
    "    reconstructed_target = _trend + reconstructed_detrend_series\n",
    "    \n",
    "    df[\"PCA_dU(mm)\"] = reconstructed_target\n",
    "    \n",
    "    df.to_csv(os.path.join(\"GPS_Files/PCA_Imputed\", basename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502ad958-a5e2-4ccb-9247-076fdae81ae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
