{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f05bdbf-878e-4d81-a60b-16132cff0a38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T08:15:52.614992Z",
     "iopub.status.busy": "2025-03-17T08:15:52.614992Z",
     "iopub.status.idle": "2025-03-17T08:15:55.267456Z",
     "shell.execute_reply": "2025-03-17T08:15:55.267456Z",
     "shell.execute_reply.started": "2025-03-17T08:15:52.614992Z"
    }
   },
   "outputs": [],
   "source": [
    "import pinyin\n",
    "import pinyin.cedict\n",
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c91a72a-cf34-4607-908c-ddc255864e6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T08:15:55.268451Z",
     "iopub.status.busy": "2025-03-17T08:15:55.268451Z",
     "iopub.status.idle": "2025-03-17T08:15:55.274800Z",
     "shell.execute_reply": "2025-03-17T08:15:55.274800Z",
     "shell.execute_reply.started": "2025-03-17T08:15:55.268451Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_measure_data(precipitation_df, station_name):\n",
    "    # Extract data for the selected station\n",
    "    df_byStation = precipitation_df.loc[:, [select_station]]\n",
    "\n",
    "    # Ensure the data has an hourly frequency, filling in missing values if necessary\n",
    "    df_byStation = df_byStation.asfreq(\"H\")\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # Compute the daily sum by resampling the hourly data to daily frequency\n",
    "    df_byStation_dailySum = df_byStation.resample(\"D\").sum()\n",
    "\n",
    "    # Extract the daily summed values as a NumPy array for further processing\n",
    "    df_byStation_dailySum_arr = df_byStation_dailySum.iloc[:, 0].values\n",
    "\n",
    "    # Extract the daily timestamps as a NumPy array\n",
    "    daily_time_arr = df_byStation_dailySum.index.strftime(\"%Y%m%d\").to_numpy()\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    # Compute the monthly sum by resampling the daily data to the start of each month (\"MS\")\n",
    "    df_byStation_monthlySum = df_byStation.resample(\"MS\").sum()\n",
    "\n",
    "    # Extract the monthly summed values as a NumPy array for further processing\n",
    "    df_byStation_monthlySum_arr = df_byStation_monthlySum.iloc[:, 0].values\n",
    "\n",
    "    # Extract the monthly timestamps as a NumPy array\n",
    "    monthly_time_arr = df_byStation_monthlySum.index.strftime(\"%Y%m%d\").to_numpy()\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    measure_byStation_dict = {\n",
    "        \"daily_values\": df_byStation_dailySum_arr,\n",
    "        \"daily_date\": daily_time_arr,\n",
    "        \"monthly_values\": df_byStation_monthlySum_arr,\n",
    "        \"monthly_date\": monthly_time_arr,\n",
    "    }\n",
    "    return measure_byStation_dict\n",
    "\n",
    "\n",
    "def get_station_metadata(df, station_name):\n",
    "    info_byStation = df.query(\"站號==@station_name\")\n",
    "\n",
    "    # Extract station metadata\n",
    "    station_name = pinyin.get(info_byStation.get(\"站名\", np.nan).values[0], format=\"strip\").upper()\n",
    "    station_type = info_byStation.get(\"站種\", np.nan).values[0]\n",
    "    station_height = info_byStation.get(\"海拔高度(m)\", np.nan).values[0]\n",
    "    x_wgs84 = info_byStation.get(\"經度\", np.nan).values[0]\n",
    "    y_wgs84 = info_byStation.get(\"緯度\", np.nan).values[0]\n",
    "    station_city = info_byStation.get(\"城市\", np.nan).values[0]\n",
    "    station_address = info_byStation.get(\"地址\", np.nan).values[0]\n",
    "    station_startdate = info_byStation.get(\"資料起始日期\", np.nan).values[0]\n",
    "    station_stopdate = info_byStation.get(\"撤站日期\", np.nan).values[0]\n",
    "    station_note = info_byStation.get(\"備註\", np.nan).values[0]\n",
    "    station_previous_code = info_byStation.get(\"原站號\", np.nan).values[0]\n",
    "    station_new_code = info_byStation.get(\"新站號\", np.nan).values[0]\n",
    "    x_twd97 = info_byStation.get(\"TWD97_x\", np.nan).values[0]\n",
    "    y_twd97 = info_byStation.get(\"TWD97_y\", np.nan).values[0]\n",
    "\n",
    "    # Store metadata in a structured dictionary\n",
    "    metadata_byStation = {\n",
    "        \"EName\": station_name,\n",
    "        \"Type\": station_type,\n",
    "        \"Elevation(m)\": station_height,\n",
    "        \"X_WGS84\": x_wgs84,\n",
    "        \"Y_WGS84\": y_wgs84,\n",
    "        \"City\": station_city,\n",
    "        \"Address\": station_address,\n",
    "        \"Start Date\": station_startdate,\n",
    "        \"Stop Date\": station_stopdate,\n",
    "        \"Notes\": station_note,\n",
    "        \"Previous Code\": station_previous_code,\n",
    "        \"New Code\": station_new_code,\n",
    "        \"X_TWD97\": x_twd97,\n",
    "        \"Y_TWD97\": y_twd97,\n",
    "    }\n",
    "\n",
    "    return metadata_byStation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d82e6d7-ed07-4708-9068-221f91296740",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T08:15:55.275796Z",
     "iopub.status.busy": "2025-03-17T08:15:55.275796Z",
     "iopub.status.idle": "2025-03-17T08:15:55.780956Z",
     "shell.execute_reply": "2025-03-17T08:15:55.780956Z",
     "shell.execute_reply.started": "2025-03-17T08:15:55.275796Z"
    }
   },
   "outputs": [],
   "source": [
    "fpath = r\"E:\\SUBSIDENCE_PROJECT_DATA\\20230915水位雨量地陷資料\\sorted_data\\降雨資料\\rainfall_data.xz\"\n",
    "df = pd.read_pickle(fpath)\n",
    "data_station_code = [ele.upper() for ele in df.columns.tolist()]\n",
    "df.columns = data_station_code\n",
    "df = df.sort_index(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c35254e7-7f5d-414d-bcad-400ee5e21d0f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T08:15:55.782950Z",
     "iopub.status.busy": "2025-03-17T08:15:55.781986Z",
     "iopub.status.idle": "2025-03-17T08:15:55.806582Z",
     "shell.execute_reply": "2025-03-17T08:15:55.806582Z",
     "shell.execute_reply.started": "2025-03-17T08:15:55.781986Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "station_info = pd.read_excel(r\"E:\\SUBSIDENCE_PROJECT_DATA\\20230915水位雨量地陷資料\\sorted_data\\降雨資料\\降雨資料.xlsx\")\n",
    "station_code = [ele.upper() for ele in station_info.iloc[:, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db45b0ca-742c-43f0-9cee-4d421341fa5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T08:15:55.807576Z",
     "iopub.status.busy": "2025-03-17T08:15:55.807576Z",
     "iopub.status.idle": "2025-03-17T08:15:55.813605Z",
     "shell.execute_reply": "2025-03-17T08:15:55.813605Z",
     "shell.execute_reply.started": "2025-03-17T08:15:55.807576Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutual_stations = sorted(set(data_station_code).intersection(set(station_code)))\n",
    "len(mutual_stations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c2a2e3-2487-42e8-885a-4f5180084644",
   "metadata": {},
   "source": [
    "#### target: convert rainfall data into HDF5 file --> monthly data, sum of rainfall in one month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3674c1e-0145-48df-bca0-6d0a563269f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T08:15:55.815597Z",
     "iopub.status.busy": "2025-03-17T08:15:55.814599Z",
     "iopub.status.idle": "2025-03-17T08:15:58.048531Z",
     "shell.execute_reply": "2025-03-17T08:15:58.048531Z",
     "shell.execute_reply.started": "2025-03-17T08:15:55.815597Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0827a470764a4c13b049c2eebba76c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize dictionaries to store processed measurement data and metadata\n",
    "rainfall_measure_data = {}\n",
    "rainfall_station_metadata = {}\n",
    "# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "# Select the first station from the list of station codes\n",
    "# select_station = data_station_code[0]\n",
    "for select_station in tqdm(data_station_code):\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    measure_dict_byStation = process_measure_data(precipitation_df=df, station_name=select_station)\n",
    "    metadata_dict_byStation = get_station_metadata(df=station_info, station_name=select_station)\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    rainfall_measure_data[select_station] = measure_dict_byStation\n",
    "    rainfall_station_metadata[select_station] = metadata_dict_byStation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "336c3e78-8dd4-45bb-ac7b-9c6bc706a0a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T08:15:58.049524Z",
     "iopub.status.busy": "2025-03-17T08:15:58.049524Z",
     "iopub.status.idle": "2025-03-17T08:15:58.836372Z",
     "shell.execute_reply": "2025-03-17T08:15:58.836372Z",
     "shell.execute_reply.started": "2025-03-17T08:15:58.049524Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No conversion path for dtype: dtype('<M8[ns]')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Write updated data and metadata back to the HDF5 file\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoday_string\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Rainfall_CRFP_monthly_v1.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m hdf5_file:\n\u001b[1;32m----> 8\u001b[0m     \u001b[43mgwatertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mh5pytools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_to_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhdf5_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrainfall_measure_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     gwatertools\u001b[38;5;241m.\u001b[39mh5pytools\u001b[38;5;241m.\u001b[39mmetadata_to_hdf5(hdf5_file, rainfall_station_metadata)\n",
      "File \u001b[1;32mD:\\VENV_PYTHON\\appgeopy\\gwatertools\\h5pytools.py:48\u001b[0m, in \u001b[0;36mdata_to_hdf5\u001b[1;34m(hdf5_group, data)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     47\u001b[0m         subgroup \u001b[38;5;241m=\u001b[39m hdf5_group\u001b[38;5;241m.\u001b[39mcreate_group(key)\n\u001b[1;32m---> 48\u001b[0m     \u001b[43mdata_to_hdf5\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# If the value is not a dictionary, create a dataset\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m hdf5_group:\n",
      "File \u001b[1;32mD:\\VENV_PYTHON\\appgeopy\\gwatertools\\h5pytools.py:53\u001b[0m, in \u001b[0;36mdata_to_hdf5\u001b[1;34m(hdf5_group, data)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m hdf5_group:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m hdf5_group[key]  \u001b[38;5;66;03m# Remove existing dataset if it exists\u001b[39;00m\n\u001b[1;32m---> 53\u001b[0m \u001b[43mhdf5_group\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Programs\\miniconda3\\Library\\envs\\fafalab\\Lib\\site-packages\\h5py\\_hl\\group.py:186\u001b[0m, in \u001b[0;36mGroup.create_dataset\u001b[1;34m(self, name, shape, dtype, data, **kwds)\u001b[0m\n\u001b[0;32m    183\u001b[0m         parent_path, name \u001b[38;5;241m=\u001b[39m name\u001b[38;5;241m.\u001b[39mrsplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    184\u001b[0m         group \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequire_group(parent_path)\n\u001b[1;32m--> 186\u001b[0m dsid \u001b[38;5;241m=\u001b[39m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_new_dset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m dset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mDataset(dsid)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dset\n",
      "File \u001b[1;32mD:\\Programs\\miniconda3\\Library\\envs\\fafalab\\Lib\\site-packages\\h5py\\_hl\\dataset.py:88\u001b[0m, in \u001b[0;36mmake_new_dset\u001b[1;34m(parent, shape, dtype, data, name, chunks, compression, shuffle, fletcher32, maxshape, compression_opts, fillvalue, scaleoffset, track_times, external, track_order, dcpl, dapl, efile_prefix, virtual_prefix, allow_unknown_filter, rdcc_nslots, rdcc_nbytes, rdcc_w0, fill_time)\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     87\u001b[0m         dtype \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39mdtype(dtype)\n\u001b[1;32m---> 88\u001b[0m     tid \u001b[38;5;241m=\u001b[39m \u001b[43mh5t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_create\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogical\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Legacy\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m((compression, shuffle, fletcher32, maxshape, scaleoffset)) \u001b[38;5;129;01mand\u001b[39;00m chunks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1669\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1693\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\h5t.pyx:1759\u001b[0m, in \u001b[0;36mh5py.h5t.py_create\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: No conversion path for dtype: dtype('<M8[ns]')"
     ]
    }
   ],
   "source": [
    "# - - - - - - - - - - - - - - - - -\n",
    "# Add new description and metadata\n",
    "# - - - - - - - - - - - - - - - - -\n",
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_Rainfall_CRFP_monthly_v1.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, rainfall_measure_data)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, rainfall_station_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffadc6b3-49df-4111-94fe-18fad4ee025a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c580ba57-e238-4a12-84d1-b06c5141c027",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bfc7be4-3c7a-4b60-b48a-4ecdd1aee053",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-17T07:01:43.228383Z",
     "iopub.status.busy": "2025-03-17T07:01:43.228383Z",
     "iopub.status.idle": "2025-03-17T07:01:43.263312Z",
     "shell.execute_reply": "2025-03-17T07:01:43.263312Z",
     "shell.execute_reply.started": "2025-03-17T07:01:43.228383Z"
    },
    "scrolled": true
   },
   "source": [
    "unique_year = df.index.year.unique()\n",
    "for year in unique_year:\n",
    "    temp = df.loc[str(year), :]\n",
    "    print(year, len(temp))\n",
    "\n",
    "1992 5440\n",
    "1993 8760\n",
    "1994 8760\n",
    "1995 8760\n",
    "1996 8783\n",
    "1997 8760\n",
    "1998 8760\n",
    "1999 8760\n",
    "2000 8784\n",
    "2001 8760\n",
    "2002 8760\n",
    "2003 8760\n",
    "2004 8784\n",
    "2005 8760\n",
    "2006 8760\n",
    "2007 8760\n",
    "2008 8784\n",
    "2009 8760\n",
    "2010 8760\n",
    "2011 8760\n",
    "2012 8784\n",
    "2013 8760\n",
    "2014 8760\n",
    "2015 8760\n",
    "2016 8784\n",
    "2017 8760\n",
    "2018 8760\n",
    "2019 8760\n",
    "2020 8784\n",
    "2021 8759\n",
    "2022 8760\n",
    "2023 745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a364cfe-e2a1-4b47-873c-5133dec03b0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0827a470764a4c13b049c2eebba76c3b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_9e3413a4ad6941739ae2d3ff56b69953",
        "IPY_MODEL_6520eb48cc794cf8bf03253898a4733d",
        "IPY_MODEL_54032a48cd154d839e75a26be196c95c"
       ],
       "layout": "IPY_MODEL_9dfadcb5a557483cb71f6acc1b5606cf"
      }
     },
     "2b46f32473f442928ddd0efd2f19ab28": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3d349bbe07c14aea9cc3df06482e6dd1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "4cdcfc843bf34229821f6a9d5408a142": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "54032a48cd154d839e75a26be196c95c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_3d349bbe07c14aea9cc3df06482e6dd1",
       "style": "IPY_MODEL_2b46f32473f442928ddd0efd2f19ab28",
       "value": " 71/71 [00:02&lt;00:00, 32.93it/s]"
      }
     },
     "6520eb48cc794cf8bf03253898a4733d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_b82e2eb8ca444b3f8d740054dfc2dcee",
       "max": 71,
       "style": "IPY_MODEL_90c66786bddc403bb9f99f0c1b8fe40b",
       "value": 71
      }
     },
     "90c66786bddc403bb9f99f0c1b8fe40b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9dfadcb5a557483cb71f6acc1b5606cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9e3413a4ad6941739ae2d3ff56b69953": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_4cdcfc843bf34229821f6a9d5408a142",
       "style": "IPY_MODEL_f5223f59d9624ccc8d826d914c0bfafe",
       "value": "100%"
      }
     },
     "b82e2eb8ca444b3f8d740054dfc2dcee": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "f5223f59d9624ccc8d826d914c0bfafe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
