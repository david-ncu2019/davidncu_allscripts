{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ac1ed63-957e-4b50-851a-76edab7f242f",
   "metadata": {},
   "source": [
    "# Multivariate Time Series Refinement and Imputation Workflow\n",
    "\n",
    "## Purpose\n",
    "Robust preprocessing and imputation pipeline for multivariate time series data, specifically designed for geospatial/hydrogeological measurements across multiple layers.\n",
    "\n",
    "## Workflow Overview\n",
    "1. **Data Preparation**\n",
    "   - Loads time series data for multiple stations from HDF5 file\n",
    "   - Processes monthly measurements across different layers\n",
    "\n",
    "2. **Time Series Refinement Process**\n",
    "   - Applies polynomial trend removal\n",
    "   - Uses StandardScaler for normalization\n",
    "   - Implements outlier filtering (z-score > ±3)\n",
    "\n",
    "3. **PCA-Based Imputation**\n",
    "   - Automatically optimizes embedding dimensions\n",
    "   - Performs parameter grid search\n",
    "   - Applies Principal Component Analysis (PCA) imputation\n",
    "   - Reconstructs time series with minimal information loss\n",
    "\n",
    "4. **Key Transformations**\n",
    "   - Trend decomposition\n",
    "   - Statistical scaling\n",
    "   - Intelligent missing value reconstruction\n",
    "\n",
    "5. **Output**\n",
    "   - Generates refined time series for each station and layer\n",
    "   - Updates original HDF5 file with imputed data\n",
    "   - Creates visualization of original vs. refined time series\n",
    "\n",
    "## Key Scientific Techniques\n",
    "- Trend removal\n",
    "- Statistical normalization\n",
    "- PCA-based imputation\n",
    "- Multivariate time series analysis\n",
    "\n",
    "## Computational Workflow\n",
    "1. Per station → Per layer processing\n",
    "2. Automated parameter optimization\n",
    "3. Robust imputation strategy\n",
    "4. Comprehensive error handling\n",
    "\n",
    "## Use Case\n",
    "Geospatial measurement refinement, particularly for hydrogeological time series data with complex temporal characteristics and missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a65c88-bd3c-4eda-9c87-831d12f79bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from pca_imputation import *\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2ceea0-f54f-4401-b808-dd37ca0dad9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pca_imputation_workflow(data, time_col=None, value_col=None):\n",
    "    \"\"\"Execute PCA imputation workflow with parameter optimization and validation.\n",
    "\n",
    "    Args:\n",
    "        data: pandas Series with datetime index or DataFrame\n",
    "        time_col: column name for time if data is DataFrame\n",
    "        value_col: column name for value if data is DataFrame\n",
    "\n",
    "    Returns:\n",
    "        dict: Results containing imputation, parameters, and validation metrics\n",
    "    \"\"\"\n",
    "    # Validate data size and determine appropriate parameters\n",
    "    data_length = len(data)\n",
    "\n",
    "    # Calculate appropriate embedding dimensions based on data size\n",
    "    # Ensure embedding dimension doesn't exceed 1/3 of data length\n",
    "    max_embedding = max(2, data_length // 3)\n",
    "\n",
    "    # Generate embedding dimensions from 2 to max_embedding (capped at 12)\n",
    "    embedding_dims = list(range(2, min(max_embedding + 1, 13), 2))\n",
    "\n",
    "    # Components must be less than embedding dimensions\n",
    "    n_components_list = [min(dim - 1, 3) for dim in embedding_dims]\n",
    "    # n_components_list = [max(2, min(dim-1, 3)) for dim in embedding_dims]\n",
    "    n_components_list = list(set(n_components_list))  # Remove duplicates\n",
    "\n",
    "    # Step 1: Parameter Optimization with safeguards\n",
    "    try:\n",
    "        param_results = parameter_grid_search(\n",
    "            data=data,\n",
    "            embedding_dims=embedding_dims,\n",
    "            n_components_list=n_components_list,\n",
    "            mask_ratio=min(0.1, 0.5 * (1 - data.isna().mean())),  # Adaptive masking\n",
    "            random_seed=42,\n",
    "            time_col=time_col,\n",
    "            value_col=value_col,\n",
    "            use_cross_validation=False,\n",
    "        )\n",
    "\n",
    "        # Get optimal parameters\n",
    "        best_params = param_results.loc[param_results[\"rmse\"].idxmin()]\n",
    "        best_embedding_dim = int(best_params[\"embedding_dim\"])\n",
    "        best_n_components = int(best_params[\"n_components\"])\n",
    "    except Exception as e:\n",
    "        # Fallback to minimal parameters if grid search fails\n",
    "        best_embedding_dim = 2\n",
    "        best_n_components = 1\n",
    "\n",
    "    # Step 2: PCA Imputation with Optimal Parameters\n",
    "    results = impute_time_series(\n",
    "        data=data,\n",
    "        embedding_dim=best_embedding_dim,\n",
    "        n_components=best_n_components,\n",
    "        time_delay=1,\n",
    "        time_col=time_col,\n",
    "        value_col=value_col,\n",
    "    )\n",
    "\n",
    "    # Step 3: Validation with minimal additional masking\n",
    "    try:\n",
    "        validation_metrics, original_values, imputed_values, mask_indices = (\n",
    "            validate_imputation_accuracy(\n",
    "                data=data,\n",
    "                embedding_dim=best_embedding_dim,\n",
    "                n_components=best_n_components,\n",
    "                time_delay=1,\n",
    "                mask_ratio=min(0.05, 0.5 * (1 - data.isna().mean())),  # Conservative masking\n",
    "                random_seed=42,\n",
    "                time_col=time_col,\n",
    "                value_col=value_col,\n",
    "            )\n",
    "        )\n",
    "    except Exception as e:\n",
    "        validation_metrics = {\"rmse\": np.nan, \"mae\": np.nan, \"r2\": np.nan}\n",
    "        original_values = np.array([])\n",
    "        imputed_values = np.array([])\n",
    "        mask_indices = np.array([])\n",
    "\n",
    "    return {\n",
    "        \"imputation_results\": results,\n",
    "        \"optimal_parameters\": {\n",
    "            \"embedding_dim\": best_embedding_dim,\n",
    "            \"n_components\": best_n_components,\n",
    "        },\n",
    "        \"validation_metrics\": validation_metrics,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e764ab2-6172-4945-b212-93acc4abf7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlcw_fpath = r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\2_KrigingInterpolation\\20250314_MLCW_CRFP_monthly_v1.h5\"\n",
    "mlcw_obj = MLCW(mlcw_fpath)\n",
    "\n",
    "mlcw_measures, mlcw_metadata = mlcw_obj.get_data()\n",
    "\n",
    "available_stations = mlcw_obj.list_stations()\n",
    "available_stations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8842823-4d60-4d79-a19e-29ce2264a500",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = []\n",
    "string_decoder = lambda arr: [x.decode(\"utf-8\") for x in arr]\n",
    "fig_savefld = \"refine_timeseries/\"\n",
    "# select_station = \"TUKU\"\n",
    "for select_station in tqdm(available_stations):\n",
    "    try:\n",
    "\n",
    "        measures_byStation = mlcw_measures[select_station]\n",
    "        monthly_date_arr = pd.to_datetime(string_decoder(measures_byStation[\"monthly_date\"]))\n",
    "        monthly_values_arr = measures_byStation[\"monthly_values\"][\"compactbylayer\"]\n",
    "\n",
    "        cdisp_mlcw_df = pd.DataFrame(data={\"time\": monthly_date_arr})\n",
    "\n",
    "        n_layers = monthly_values_arr.shape[0]\n",
    "\n",
    "        for i in range(n_layers):\n",
    "            cdisp_mlcw_df[f\"Layer_{i+1}\"] = monthly_values_arr[i]\n",
    "\n",
    "        cdisp_mlcw_df = cdisp_mlcw_df.set_index(\"time\")\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        refined_output_df = pd.DataFrame(data=None, index=cdisp_mlcw_df.index)\n",
    "\n",
    "        for layer in cdisp_mlcw_df.columns:\n",
    "            target_array = cdisp_mlcw_df[layer]\n",
    "            if target_array.sum()!=0:\n",
    "                _trend, _slope = get_polynomial_trend(series=target_array, order=3)\n",
    "                _detrend = target_array - _trend\n",
    "    \n",
    "                scaled_values = scaler.fit_transform(_detrend.values.reshape(-1, 1))\n",
    "                # Reconstruct scaled series, maintaining original index\n",
    "                scaled_series = pd.Series(\n",
    "                    scaled_values.flatten(), index=_detrend.index, name=\"Scaled Values\"\n",
    "                )\n",
    "    \n",
    "                filter_cond = (scaled_series > 3) | (scaled_series < -3)\n",
    "                scaled_series[filter_cond] = np.nan\n",
    "    \n",
    "                results = run_pca_imputation_workflow(scaled_series)\n",
    "    \n",
    "                reconstructed_scaled_detrend = results[\"imputation_results\"][\"reconstructed_series\"]\n",
    "                reconstructed_detrend = scaler.inverse_transform(\n",
    "                    reconstructed_scaled_detrend.reshape(-1, 1)\n",
    "                )\n",
    "                reconstructed_detrend_series = pd.Series(\n",
    "                    data=reconstructed_detrend.flatten(), index=_detrend.index\n",
    "                )\n",
    "    \n",
    "                reconstructed_target = _trend + reconstructed_detrend_series\n",
    "                refined_output_df[layer] = refined_output_df.index.map(reconstructed_target)\n",
    "            else:\n",
    "                refined_output_df[layer] = refined_output_df.index.map(target_array)\n",
    "\n",
    "        measures_byStation[\"monthly_values\"][\"compactbylayer_PCA\"] = refined_output_df.T.values\n",
    "\n",
    "        cache.append({select_station: measures_byStation})\n",
    "\n",
    "        # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "        fig, axes = plt.subplots(nrows=n_layers, ncols=1, sharex=True, figsize=(11.7, 8.3))\n",
    "        axes = axes.flatten()\n",
    "        for idx, layer in enumerate(cdisp_mlcw_df.columns):\n",
    "            axes[idx].plot(\n",
    "                refined_output_df.loc[:, layer], color=\"blue\", linestyle=\"--\", marker=\"o\", ms=2\n",
    "            )\n",
    "            axes[idx].plot(cdisp_mlcw_df.loc[:, layer], label=layer, color=\"black\")\n",
    "            visualize.configure_axis(axes[idx], hide_spines=[\"right\", \"top\"], fontsize_base=12)\n",
    "            visualize.configure_legend(axes[idx], fontsize_base=12)\n",
    "            visualize.configure_datetime_ticks(axes[idx])\n",
    "            visualize.configure_ticks(axes[idx])\n",
    "        fig.suptitle(t=select_station, y=0.95, fontweight=\"bold\", fontsize=16)\n",
    "        fig.tight_layout()\n",
    "        fig.autofmt_xdate(ha=\"center\", rotation=90)\n",
    "        # visualize.save_figure(fig=fig, savepath=os.path.join(fig_savefld, f\"{select_station}.png\"))\n",
    "        plt.close()\n",
    "        # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    except Exception as e:\n",
    "        print(select_station, e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db73bd0-c383-4234-9a30-82e77d8f9bd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "new_mlcw_measures = merge_dicts(*cache)\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_MLCW_CRFP_monthly_v2.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, new_mlcw_measures)\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, mlcw_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62454390-2475-4572-8a75-68b0d3bffef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dd25a6-425a-45cf-bb28-558a369778dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99261063-efb6-4393-8e28-fc13a649d449",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff7d8bd-1d67-4816-888d-bb0f50dbc324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "28095a32-3cb6-4497-b124-64c5edd862a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T07:35:32.739453Z",
     "iopub.status.busy": "2025-04-15T07:35:32.739453Z",
     "iopub.status.idle": "2025-04-15T07:35:33.146879Z",
     "shell.execute_reply": "2025-04-15T07:35:33.146879Z",
     "shell.execute_reply.started": "2025-04-15T07:35:32.739453Z"
    }
   },
   "source": [
    "target_array = cdisp_mlcw_df[\"Layer_1\"]\n",
    "_trend, _slope = get_polynomial_trend(series=target_array, order=3)\n",
    "_detrend = target_array - _trend\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 6))\n",
    "axes = axes.flatten()\n",
    "target_array.plot(ax=axes[0], linestyle=\"--\", marker=\"o\")\n",
    "_trend.plot(lw=2, ax=axes[0])\n",
    "\n",
    "_detrend.plot(ax=axes[1], marker=\"o\", linestyle=\"--\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "61dc4869-e15e-4fca-8f9b-e3ca74bd52ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T07:48:33.224519Z",
     "iopub.status.busy": "2025-04-15T07:48:33.224519Z",
     "iopub.status.idle": "2025-04-15T07:48:33.314434Z",
     "shell.execute_reply": "2025-04-15T07:48:33.314434Z",
     "shell.execute_reply.started": "2025-04-15T07:48:33.224519Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "results = run_pca_imputation_workflow(scaled_series)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d2e5518-1595-4e77-be61-1120872d8040",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T07:41:08.107446Z",
     "iopub.status.busy": "2025-04-15T07:41:08.107446Z",
     "iopub.status.idle": "2025-04-15T07:41:08.198252Z",
     "shell.execute_reply": "2025-04-15T07:41:08.198252Z",
     "shell.execute_reply.started": "2025-04-15T07:41:08.107446Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "plt.plot(results[\"imputation_results\"][\"reconstructed_series\"], marker=\"o\")\n",
    "plt.plot(results[\"imputation_results\"][\"original_series\"], marker=\"s\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "166baf2f-0bac-41fc-9b54-d13ac97f8e4c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T07:35:50.307799Z",
     "iopub.status.busy": "2025-04-15T07:35:50.307799Z",
     "iopub.status.idle": "2025-04-15T07:35:50.311304Z",
     "shell.execute_reply": "2025-04-15T07:35:50.311304Z",
     "shell.execute_reply.started": "2025-04-15T07:35:50.307799Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea8a1dec-b0cd-49e1-b367-2b066be9203f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T07:35:50.779669Z",
     "iopub.status.busy": "2025-04-15T07:35:50.779669Z",
     "iopub.status.idle": "2025-04-15T07:35:50.782809Z",
     "shell.execute_reply": "2025-04-15T07:35:50.782809Z",
     "shell.execute_reply.started": "2025-04-15T07:35:50.779669Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "results[\"optimal_parameters\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "700b6432-301d-43a7-b021-6588ce413d4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T07:35:51.309599Z",
     "iopub.status.busy": "2025-04-15T07:35:51.309599Z",
     "iopub.status.idle": "2025-04-15T07:35:51.313102Z",
     "shell.execute_reply": "2025-04-15T07:35:51.313102Z",
     "shell.execute_reply.started": "2025-04-15T07:35:51.309599Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "results[\"imputation_results\"].keys()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22a30423-3d1b-4214-9d49-eefb3507527f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-15T07:49:55.997495Z",
     "iopub.status.busy": "2025-04-15T07:49:55.997495Z",
     "iopub.status.idle": "2025-04-15T07:49:56.115394Z",
     "shell.execute_reply": "2025-04-15T07:49:56.115394Z",
     "shell.execute_reply.started": "2025-04-15T07:49:55.997495Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "reconstructed_scaled_detrend = results[\"imputation_results\"][\"reconstructed_series\"]\n",
    "reconstructed_detrend = scaler.inverse_transform(reconstructed_scaled_detrend.reshape(-1,1))\n",
    "reconstructed_detrend_series = pd.Series(data=reconstructed_detrend.flatten(), index=_detrend.index)\n",
    "\n",
    "reconstructed_target = _trend + reconstructed_detrend_series\n",
    "\n",
    "reconstructed_target.plot(marker=\" \", linestyle=\"--\")\n",
    "target_array.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
