{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f564091-e44f-4ad2-86e5-19b747632218",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T08:01:15.094956Z",
     "iopub.status.busy": "2025-03-27T08:01:15.094956Z",
     "iopub.status.idle": "2025-03-27T08:01:17.671200Z",
     "shell.execute_reply": "2025-03-27T08:01:17.671200Z",
     "shell.execute_reply.started": "2025-03-27T08:01:15.094956Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "raw",
   "id": "559ba832-8e2e-46b5-8ce2-5887eeb9d483",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T15:10:59.343526Z",
     "iopub.status.busy": "2025-03-20T15:10:59.342528Z",
     "iopub.status.idle": "2025-03-20T15:11:00.418064Z",
     "shell.execute_reply": "2025-03-20T15:11:00.418064Z",
     "shell.execute_reply.started": "2025-03-20T15:10:59.343526Z"
    }
   },
   "source": [
    "fpath = r\"temp/Yunlin_Electricity_Usage_Bill.xz\"\n",
    "df = pd.read_pickle(fpath)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cc1ad7e2-d188-4e8d-82ba-5f917e057048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-20T15:11:21.202449Z",
     "iopub.status.busy": "2025-03-20T15:11:21.202449Z",
     "iopub.status.idle": "2025-03-20T15:11:21.205607Z",
     "shell.execute_reply": "2025-03-20T15:11:21.205607Z",
     "shell.execute_reply.started": "2025-03-20T15:11:21.202449Z"
    }
   },
   "source": [
    "for col in df.columns:\n",
    "    if type(col)==str:\n",
    "        print(col, end=\"\\t\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45065c6a-e12a-4ea6-af5d-4d4d1ad1ff5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T08:01:17.672168Z",
     "iopub.status.busy": "2025-03-27T08:01:17.672168Z",
     "iopub.status.idle": "2025-03-27T08:01:18.763419Z",
     "shell.execute_reply": "2025-03-27T08:01:18.763419Z",
     "shell.execute_reply.started": "2025-03-27T08:01:17.672168Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int16)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath = r\"CRFP_Ebill_2014_2021_agriculture.xz\"\n",
    "df = pd.read_pickle(fpath)\n",
    "df[\"Outliers\"].unique()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a196fd44-a8b1-4ca1-b3a0-ad1bb85328e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-27T08:00:46.843672Z",
     "iopub.status.busy": "2025-03-27T08:00:46.843672Z",
     "iopub.status.idle": "2025-03-27T08:00:46.977820Z",
     "shell.execute_reply": "2025-03-27T08:00:46.977820Z",
     "shell.execute_reply.started": "2025-03-27T08:00:46.843672Z"
    }
   },
   "source": [
    "ebill_columns = [col for col in df.columns if col.startswith(\"N\")]\n",
    "temp = df[df[\"Outliers\"]==1].sort_values(by=\"Total_Usage\", ascending=False)\n",
    "temp.loc[:, ebill_columns].iloc[0, :].plot(marker='o', linestyle='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1614fc7-c139-47c0-a5af-6e30cf54f33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from pykrige.ok import OrdinaryKriging\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "N_KNOWN_POINTS = 10\n",
    "N_UNKNOWN_POINTS = 100\n",
    "N_MONTHS = 36\n",
    "Z_SCORE_THRESHOLD = 3.0  # Threshold for outlier removal\n",
    "APPLY_LOG_TRANSFORM = False # Set to True to apply log transform, False otherwise\n",
    "CV_FOLDS = 5 # Number of folds for cross-validation based variogram selection\n",
    "VARIOGRAM_MODELS_TO_TEST = ['linear', 'power', 'gaussian', 'spherical', 'exponential'] # Models to test\n",
    "OUTPUT_INTERPOLATED_FILE = 'interpolated_groundwater_levels.csv'\n",
    "OUTPUT_EVALUATION_FILE = 'kriging_evaluation_metrics.xlsx' # Using Excel for better multi-sheet potential if needed\n",
    "\n",
    "# Suppress warnings from PyKrige (optional, can be noisy)\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning) # PyKrige specific user warnings\n",
    "\n",
    "# --- 1. Simulate Data (Replace this with loading your actual data) ---\n",
    "print(\"Step 1: Simulating data...\")\n",
    "np.random.seed(42) # for reproducibility\n",
    "\n",
    "# Known points coordinates (e.g., within a 100x100 area)\n",
    "known_coords = pd.DataFrame({\n",
    "    'X': np.random.rand(N_KNOWN_POINTS) * 100,\n",
    "    'Y': np.random.rand(N_KNOWN_POINTS) * 100\n",
    "})\n",
    "\n",
    "# Known points measurements (simulate some spatial correlation + noise)\n",
    "# Base level + trend + monthly variation + noise\n",
    "base_levels = np.random.rand(N_KNOWN_POINTS) * 10 + 50 # Base groundwater level\n",
    "spatial_effect = (known_coords['X'] * 0.05 + known_coords['Y'] * 0.03) # Simple spatial trend\n",
    "known_values_list = []\n",
    "for month in range(N_MONTHS):\n",
    "    monthly_variation = np.sin(month * 2 * np.pi / 12) * 2 # Seasonal variation\n",
    "    noise = np.random.randn(N_KNOWN_POINTS) * 0.5 # Random noise\n",
    "    month_values = base_levels + spatial_effect + monthly_variation + noise\n",
    "    # Introduce a couple of potential outliers per month for testing\n",
    "    if month % 10 == 0:\n",
    "         outlier_idx = np.random.choice(N_KNOWN_POINTS, 1, replace=False)\n",
    "         month_values[outlier_idx] *= (1 + np.random.choice([-1,1]) * 0.5) # +/- 50% outlier\n",
    "\n",
    "    known_values_list.append(month_values)\n",
    "\n",
    "known_values = pd.DataFrame(np.array(known_values_list).T, columns=[f'Month_{i+1}' for i in range(N_MONTHS)])\n",
    "\n",
    "# Combine known coords and values\n",
    "known_data = pd.concat([known_coords, known_values], axis=1)\n",
    "\n",
    "# Unknown points coordinates\n",
    "unknown_coords = pd.DataFrame({\n",
    "    'X': np.random.rand(N_UNKNOWN_POINTS) * 100,\n",
    "    'Y': np.random.rand(N_UNKNOWN_POINTS) * 100\n",
    "})\n",
    "\n",
    "print(f\"Simulated {N_KNOWN_POINTS} known points with {N_MONTHS} measurements each.\")\n",
    "print(f\"Simulated {N_UNKNOWN_POINTS} unknown points.\")\n",
    "# print(\"Known data head:\\n\", known_data.head())\n",
    "# print(\"\\nUnknown coords head:\\n\", unknown_coords.head())\n",
    "\n",
    "# --- 2. Preprocessing and Kriging Loop ---\n",
    "print(\"\\nStep 2: Starting Preprocessing and Kriging for each month...\")\n",
    "all_interpolated_values = pd.DataFrame(index=unknown_coords.index)\n",
    "all_evaluation_metrics = []\n",
    "\n",
    "x_known = known_data['X'].values\n",
    "y_known = known_data['Y'].values\n",
    "x_unknown = unknown_coords['X'].values\n",
    "y_unknown = unknown_coords['Y'].values\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for i in range(N_MONTHS):\n",
    "    month_col = f'Month_{i+1}'\n",
    "    print(f\"\\nProcessing {month_col}...\")\n",
    "\n",
    "    # Extract data for the current month\n",
    "    z_known_original = known_data[month_col].values.copy()\n",
    "    \n",
    "    # --- Preprocessing ---\n",
    "    # a) Outlier Removal (using Z-score)\n",
    "    z_scores = np.abs(stats.zscore(z_known_original, nan_policy='omit'))\n",
    "    outlier_indices = np.where(z_scores > Z_SCORE_THRESHOLD)[0]\n",
    "    z_known_processed = z_known_original.copy()\n",
    "    if len(outlier_indices) > 0:\n",
    "        print(f\"  Detected {len(outlier_indices)} outliers. Replacing with NaN.\")\n",
    "        z_known_processed[outlier_indices] = np.nan\n",
    "    else:\n",
    "        print(\"  No outliers detected.\")\n",
    "        \n",
    "    # Handle potential NaNs introduced or already present before transformation/kriging\n",
    "    valid_indices = ~np.isnan(z_known_processed)\n",
    "    if np.sum(valid_indices) < 3: # Need at least 3 points for variogram\n",
    "        print(f\"  Skipping {month_col}: Not enough valid data points ({np.sum(valid_indices)}) after outlier removal.\")\n",
    "        all_interpolated_values[month_col] = np.nan\n",
    "        all_evaluation_metrics.append({\n",
    "            'Month': month_col, 'Best_Model': 'Skipped', 'CV_RMSE': np.nan,\n",
    "            'CV_MAE': np.nan, 'Num_Valid_Points': np.sum(valid_indices),\n",
    "            'Transformation': 'None', 'Outliers_Removed': len(outlier_indices)\n",
    "        })\n",
    "        continue\n",
    "        \n",
    "    x_known_valid = x_known[valid_indices]\n",
    "    y_known_valid = y_known[valid_indices]\n",
    "    z_known_valid = z_known_processed[valid_indices]\n",
    "    \n",
    "    # b) Data Transformation (Optional: Log Transform)\n",
    "    transformation_applied = \"None\"\n",
    "    if APPLY_LOG_TRANSFORM:\n",
    "        # Check for non-positive values before log transform\n",
    "        if np.any(z_known_valid <= 0):\n",
    "            print(\"  Warning: Data contains non-positive values. Cannot apply log transform. Skipping transformation.\")\n",
    "        else:\n",
    "            print(\"  Applying Log Transformation.\")\n",
    "            z_known_valid = np.log(z_known_valid)\n",
    "            transformation_applied = \"Log\"\n",
    "            \n",
    "    # --- Variogram Model Selection via Cross-Validation ---\n",
    "    print(\"  Performing Cross-Validation to select best variogram model...\")\n",
    "    best_model = None\n",
    "    best_rmse = np.inf\n",
    "    cv_predictions = None # Store predictions from the best model's CV\n",
    "\n",
    "    kf = KFold(n_splits=min(CV_FOLDS, len(x_known_valid)), shuffle=True, random_state=i) # Ensure folds <= samples\n",
    "\n",
    "    for model in VARIOGRAM_MODELS_TO_TEST:\n",
    "        print(f\"    Testing model: {model}\")\n",
    "        model_cv_predictions = np.full_like(z_known_valid, fill_value=np.nan, dtype=float)\n",
    "        \n",
    "        try:\n",
    "            for train_idx, val_idx in kf.split(x_known_valid):\n",
    "                # Check if validation set is empty or too small (can happen with few points)\n",
    "                if len(val_idx) == 0: continue\n",
    "                \n",
    "                # Need enough points for variogram estimation in the training set\n",
    "                if len(train_idx) < 3: continue \n",
    "                \n",
    "                ok_cv = OrdinaryKriging(\n",
    "                    x_known_valid[train_idx],\n",
    "                    y_known_valid[train_idx],\n",
    "                    z_known_valid[train_idx],\n",
    "                    variogram_model=model,\n",
    "                    verbose=False,\n",
    "                    enable_plotting=False,\n",
    "                    # nlags=6 # Can adjust nlags if needed\n",
    "                )\n",
    "                \n",
    "                # Predict at validation points\n",
    "                pred_z, pred_ss = ok_cv.execute(\n",
    "                    'points',\n",
    "                    x_known_valid[val_idx],\n",
    "                    y_known_valid[val_idx]\n",
    "                )\n",
    "                model_cv_predictions[val_idx] = pred_z\n",
    "                \n",
    "            # Calculate RMSE for this model based on CV predictions\n",
    "            valid_cv_preds = ~np.isnan(model_cv_predictions)\n",
    "            if np.sum(valid_cv_preds) > 0:\n",
    "                current_rmse = np.sqrt(mean_squared_error(z_known_valid[valid_cv_preds], model_cv_predictions[valid_cv_preds]))\n",
    "                print(f\"      CV RMSE: {current_rmse:.4f}\")\n",
    "                if current_rmse < best_rmse:\n",
    "                    best_rmse = current_rmse\n",
    "                    best_model = model\n",
    "                    cv_predictions = model_cv_predictions # Store these predictions for final eval metrics\n",
    "            else:\n",
    "                 print(f\"      CV failed for model {model} (no valid predictions).\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"      Error during Kriging/CV for model {model}: {e}\")\n",
    "            continue # Try next model\n",
    "\n",
    "    if best_model is None:\n",
    "        print(f\"  Skipping {month_col}: Could not find a suitable variogram model via cross-validation.\")\n",
    "        all_interpolated_values[month_col] = np.nan\n",
    "        all_evaluation_metrics.append({\n",
    "            'Month': month_col, 'Best_Model': 'Failed', 'CV_RMSE': np.nan,\n",
    "            'CV_MAE': np.nan, 'Num_Valid_Points': len(z_known_valid),\n",
    "            'Transformation': transformation_applied, 'Outliers_Removed': len(outlier_indices)\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    print(f\"  Best variogram model selected: {best_model} (CV RMSE: {best_rmse:.4f})\")\n",
    "\n",
    "    # --- Kriging Interpolation using the Best Model ---\n",
    "    print(f\"  Performing Kriging interpolation using {best_model} model...\")\n",
    "    try:\n",
    "        ok_final = OrdinaryKriging(\n",
    "            x_known_valid,\n",
    "            y_known_valid,\n",
    "            z_known_valid,\n",
    "            variogram_model=best_model,\n",
    "            verbose=False,\n",
    "            enable_plotting=False,\n",
    "            # nlags=6\n",
    "        )\n",
    "\n",
    "        # Execute on unknown points\n",
    "        z_pred, z_var = ok_final.execute('points', x_unknown, y_unknown)\n",
    "\n",
    "        # --- Back Transformation (if applied) ---\n",
    "        if transformation_applied == \"Log\":\n",
    "            print(\"  Applying inverse transformation (Exp).\")\n",
    "            z_pred = np.exp(z_pred)\n",
    "            # Note: Variance also needs back-transformation, but it's more complex.\n",
    "            # We usually report metrics on the original scale, so back-transform predictions first.\n",
    "\n",
    "        # Store interpolated values\n",
    "        all_interpolated_values[month_col] = z_pred\n",
    "        print(f\"  Interpolation for {month_col} complete.\")\n",
    "\n",
    "        # --- Evaluation ---\n",
    "        # Use the stored cross-validation predictions from the *best* model\n",
    "        valid_cv_indices = ~np.isnan(cv_predictions)\n",
    "        if np.sum(valid_cv_indices) > 0:\n",
    "             # Back-transform CV predictions if needed *before* calculating metrics against original scale data\n",
    "            cv_preds_eval_scale = cv_predictions[valid_cv_indices]\n",
    "            z_known_eval_scale = z_known_processed[valid_indices][valid_cv_indices] # Use processed (outliers NaNed) data on original scale\n",
    "            \n",
    "            if transformation_applied == \"Log\":\n",
    "                 cv_preds_eval_scale = np.exp(cv_preds_eval_scale)\n",
    "                 # z_known_eval_scale is already on the original scale (before log)\n",
    "                 \n",
    "            cv_mae = mean_absolute_error(z_known_eval_scale, cv_preds_eval_scale)\n",
    "            # We already have best_rmse calculated on the transformed scale if applicable.\n",
    "            # Let's recalculate RMSE on the original scale for consistency in reporting.\n",
    "            cv_rmse_original_scale = np.sqrt(mean_squared_error(z_known_eval_scale, cv_preds_eval_scale))\n",
    "            \n",
    "            print(f\"  CV Metrics (Original Scale): RMSE={cv_rmse_original_scale:.4f}, MAE={cv_mae:.4f}\")\n",
    "\n",
    "            metrics = {\n",
    "                'Month': month_col,\n",
    "                'Best_Model': best_model,\n",
    "                'CV_RMSE': cv_rmse_original_scale, # Use original scale RMSE\n",
    "                'CV_MAE': cv_mae,\n",
    "                'Num_Valid_Points': len(z_known_valid),\n",
    "                'Transformation': transformation_applied,\n",
    "                'Outliers_Removed': len(outlier_indices)\n",
    "            }\n",
    "        else:\n",
    "             print(\"  Could not calculate CV metrics (no valid CV predictions).\")\n",
    "             metrics = {\n",
    "                 'Month': month_col, 'Best_Model': best_model, 'CV_RMSE': np.nan,\n",
    "                 'CV_MAE': np.nan, 'Num_Valid_Points': len(z_known_valid),\n",
    "                 'Transformation': transformation_applied, 'Outliers_Removed': len(outlier_indices)\n",
    "             }\n",
    "\n",
    "        all_evaluation_metrics.append(metrics)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  Error during final Kriging interpolation for {month_col}: {e}\")\n",
    "        all_interpolated_values[month_col] = np.nan\n",
    "        # Add error entry to metrics\n",
    "        all_evaluation_metrics.append({\n",
    "            'Month': month_col, 'Best_Model': best_model if best_model else 'Error', 'CV_RMSE': np.nan,\n",
    "            'CV_MAE': np.nan, 'Num_Valid_Points': len(z_known_valid),\n",
    "            'Transformation': transformation_applied, 'Outliers_Removed': len(outlier_indices),\n",
    "             'Error': str(e)\n",
    "        })\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nProcessing finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- 3. Save Results ---\n",
    "print(\"\\nStep 3: Saving results...\")\n",
    "\n",
    "# Combine unknown coordinates with interpolated values\n",
    "final_interpolated_data = pd.concat([unknown_coords, all_interpolated_values], axis=1)\n",
    "\n",
    "# Save interpolated values\n",
    "try:\n",
    "    final_interpolated_data.to_csv(OUTPUT_INTERPOLATED_FILE, index=False)\n",
    "    print(f\"Interpolated values saved to '{OUTPUT_INTERPOLATED_FILE}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving interpolated values: {e}\")\n",
    "\n",
    "# Save evaluation metrics\n",
    "try:\n",
    "    metrics_df = pd.DataFrame(all_evaluation_metrics)\n",
    "    metrics_df.to_excel(OUTPUT_EVALUATION_FILE, index=False, sheet_name='Monthly_Metrics')\n",
    "    print(f\"Evaluation metrics saved to '{OUTPUT_EVALUATION_FILE}'\")\n",
    "except Exception as e:\n",
    "    print(f\"Error saving evaluation metrics: {e}\")\n",
    "\n",
    "print(\"\\nProgram finished.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
