{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46d1a99-f7d5-45c8-a921-44cb1ac3382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b05b8f-51b6-46cd-92e3-038896835117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nscore(\n",
    "    df, idx_column=\"STATION\", transformation_columns=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies normal score transformation to specified columns in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input data frame.\n",
    "      idx_column (str): Name of the unique identifier column (default: \"STATION\").\n",
    "      transformation_columns (list of str): List of column names to transform.\n",
    "          If None, all columns except the idx_column are used.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: A new DataFrame containing the original identifier and the new\n",
    "          transformed columns, each prefixed with 'Trans_'.\n",
    "\n",
    "    Structural Rules for the Input DataFrame:\n",
    "      1. Must include a unique identifier column (default name: \"STATION\") that uniquely\n",
    "         identifies each record.\n",
    "      2. Must contain one or more numeric columns that are to be transformed.\n",
    "      3. Any additional columns (meta-data) are preserved as is.\n",
    "      4. If transformation_columns is not provided, all columns except the idx_column will\n",
    "         be considered for transformation.\n",
    "    \"\"\"\n",
    "    # Validate that the required identifier column exists\n",
    "    if idx_column not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Input DataFrame must contain the '{idx_column}' column as a unique identifier.\"\n",
    "        )\n",
    "\n",
    "    # Determine columns to transform if not explicitly provided\n",
    "    if transformation_columns is None:\n",
    "        transformation_columns = [\n",
    "            col for col in df.columns if col != idx_column\n",
    "        ]\n",
    "\n",
    "    # Create a copy to work on; use idx_column as index for mapping transformation results\n",
    "    output_df = df.loc[\n",
    "        :, ~df.columns.isin(transformation_columns)\n",
    "    ].copy()\n",
    "    output_df.set_index(idx_column, inplace=True)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Significant change: Refactored loop to process each transformation column generically\n",
    "    # --------------------------------------------------------------------------\n",
    "    for col in tqdm(transformation_columns):\n",
    "        try:\n",
    "            # Create a temporary DataFrame for the current column\n",
    "            temp = df[[idx_column, col]].dropna(subset=[col]).copy()\n",
    "\n",
    "            # Apply normal score transformation\n",
    "            # The new transformed column is prefixed with 'Trans_'\n",
    "            transformed_col = \"Trans_\" + col\n",
    "            temp[transformed_col], tvDisp, tnsDisp = geostats.nscore(\n",
    "                temp, col\n",
    "            )\n",
    "\n",
    "            # Filter out outliers outside the acceptable range [-3, 3]\n",
    "            filter_cond = (temp[transformed_col] >= -3) & (\n",
    "                temp[transformed_col] <= 3\n",
    "            )\n",
    "            temp = temp[filter_cond]\n",
    "            temp.set_index(idx_column, inplace=True)\n",
    "\n",
    "            # Map the transformed values back to the output DataFrame using the identifier\n",
    "            output_df[transformed_col] = output_df.index.map(\n",
    "                temp[transformed_col]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{col}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # Reset index so that the identifier becomes a column again\n",
    "    output_df.reset_index(inplace=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2f6d9b-3554-47ef-ad7b-7fef84c7f423",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def apply_logtransform(\n",
    "    df, idx_column=\"STATION\", transformation_columns=None, c=1\n",
    "):\n",
    "    \"\"\"\n",
    "    Apply a log transform to selected DataFrame columns.\n",
    "\n",
    "    Adds a constant 'c' to avoid log(0) and creates new columns\n",
    "    prefixed with 'Trans_' for each transformed column. By default,\n",
    "    all columns except the identifier (idx_column) are transformed.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Input data.\n",
    "        idx_column (str): Unique identifier column name (default \"STATION\").\n",
    "        transformation_columns (list): Columns to transform. If None, all\n",
    "                                       columns except idx_column are used.\n",
    "        c (float): Constant added to avoid log(0) (default 1).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with the original identifier and new transformed columns.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the identifier column is missing.\n",
    "    \"\"\"\n",
    "    # Check that the identifier column exists.\n",
    "    if idx_column not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Input DataFrame must contain '{idx_column}'.\"\n",
    "        )\n",
    "\n",
    "    # If no columns specified, use all except the identifier.\n",
    "    if transformation_columns is None:\n",
    "        transformation_columns = [\n",
    "            col for col in df.columns if col != idx_column\n",
    "        ]\n",
    "\n",
    "    # Create output DataFrame with identifier as index.\n",
    "    output_df = df.loc[\n",
    "        :, ~df.columns.isin(transformation_columns)\n",
    "    ].copy()\n",
    "    output_df.set_index(idx_column, inplace=True)\n",
    "\n",
    "    # Process each column to be transformed.\n",
    "    for col in transformation_columns:\n",
    "        try:\n",
    "            # Extract and clean data for the column.\n",
    "            temp = df[[idx_column, col]].dropna(subset=[col]).copy()\n",
    "            transformed_col = \"Trans_\" + col\n",
    "            # Apply the log transformation.\n",
    "            temp[transformed_col] = np.log(df[col] + c)\n",
    "            temp.set_index(idx_column, inplace=True)\n",
    "            # Map the transformed values back to the output DataFrame.\n",
    "            output_df[transformed_col] = output_df.index.map(\n",
    "                temp[transformed_col]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{col}': {e}\")\n",
    "            continue\n",
    "\n",
    "    output_df.reset_index(inplace=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216f1891-8eb4-4eac-bd3b-363862ba894b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import (\n",
    "    QuantileTransformer,\n",
    ")\n",
    "\n",
    "\n",
    "def quantile_normal_transform(\n",
    "    data, n_quantiles=1000, subsample=10_000, random_state=42\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies a quantile transformation that maps the data to a normal distribution.\n",
    "\n",
    "    Parameters:\n",
    "        data (array-like): Input rainfall data.\n",
    "        n_quantiles (int): Number of quantiles to use (should be less than or equal to len(data)).\n",
    "        random_state (int): Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Transformed data that follows a standard normal distribution.\n",
    "    \"\"\"\n",
    "    data = np.array(data).reshape(-1, 1)\n",
    "    qt = QuantileTransformer(\n",
    "        n_quantiles=n_quantiles,\n",
    "        subsample=subsample,\n",
    "        output_distribution=\"normal\",\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    transformed = qt.fit_transform(data)\n",
    "    return transformed.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feee319-a59a-4d11-90fb-ddfbe6344389",
   "metadata": {},
   "source": [
    "#### Monthly Groundwater\n",
    "\n",
    "transform data using nscore transformation (geostatspy), then save the transformed dataframe into CSV file, because Python 2.7 cannot read higher level of file."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d2800fd4-064b-4338-bbe2-e40b1d0365b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "files = glob(\"1_Input_DataSets/*GWL*.xz\")\n",
    "# select_file = files[0]\n",
    "for select_file in files:\n",
    "    basename = os.path.basename(select_file)\n",
    "    df = pd.read_pickle(select_file)\n",
    "    print(df.shape)\n",
    "    trans_cols = [col for col in df.columns if col.startswith(\"N\")]\n",
    "    transformed_df = apply_nscore(\n",
    "        df=df, idx_column=\"STATION\", transformation_columns=trans_cols\n",
    "    )\n",
    "    # transformed_df.to_csv(f\"2_Transformed/NSCORE_{basename.replace(\".xz\", \".csv\")}\", index=False)\n",
    "    with open(f\"2_Transformed/NSCORE_{basename}\", \"wb\") as f:\n",
    "        pickle.dump(transformed_df.to_dict(), f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64244e77-b05b-4547-abf3-0ee49be543f9",
   "metadata": {},
   "source": [
    "#### Monthly Rainfall\n",
    "\n",
    "transform data using log transformation, because data only contains positive values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75333825-77dd-4f39-9690-f36b038a2b84",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fpath = \"1_Input_DataSets/Monthly_Rainfall_CRFP.xz\"\n",
    "basename = os.path.basename(fpath)\n",
    "df = pd.read_pickle(fpath)\n",
    "# exclude C1K250\n",
    "df = df.query(\"STATION!='C1K250'\")\n",
    "trans_cols = [col for col in df.columns if col.startswith(\"N\")]\n",
    "\n",
    "output_df = df.loc[:, ~df.columns.isin(trans_cols)].copy()\n",
    "\n",
    "for col in trans_cols:\n",
    "    new_col = \"Trans_\" + col\n",
    "    output_df[new_col] = quantile_normal_transform(\n",
    "        data=df.loc[:, col],\n",
    "        n_quantiles=int(len(df)/2),\n",
    "        subsample=len(df),\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "# output_df.to_csv(f\"2_Transformed/QuantileNormTrans_{basename.replace(\".xz\", \".csv\")}\", index=False)\n",
    "with open(f\"2_Transformed/QuantileNormTrans_{basename}\", \"wb\") as f:\n",
    "    pickle.dump(output_df.to_dict(), f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a107ef8e-aa63-4c70-abfb-c9ef05a8eb1b",
   "metadata": {},
   "source": [
    "#### Monthly Electricity"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56f0b08c-3664-41e5-8d7c-f3e8c011e3e7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fpath = \"1_Input_DataSets/Monthly_Electricity_CRFP.xz\"\n",
    "basename = os.path.basename(fpath).split(\".\")[0]\n",
    "df = pd.read_pickle(fpath)\n",
    "\n",
    "# group all wells located at the same location,\n",
    "# then calculate the average of all\n",
    "grouped_df = df.groupby(by=\"PointKey\").mean()\n",
    "grouped_df = grouped_df.reset_index(drop=False)\n",
    "\n",
    "trans_cols = [col for col in df.columns if col.startswith(\"N\")]\n",
    "\n",
    "output_df = grouped_df.loc[:, ~df.columns.isin(trans_cols)].copy()\n",
    "\n",
    "for col in tqdm(trans_cols):\n",
    "    new_col = \"Trans_\" + col\n",
    "    output_df[new_col] = quantile_normal_transform(\n",
    "        data=grouped_df.loc[:, col], n_quantiles=10_000, subsample=50_000, random_state=42\n",
    "    )\n",
    "\n",
    "number_of_chunks = 10\n",
    "\n",
    "# Split the output DataFrame into 10 equally sized chunks.\n",
    "chunks = np.array_split(output_df, number_of_chunks)\n",
    "\n",
    "# output_df.to_csv(f\"2_Transformed/QuantileNormTrans_{basename.replace(\".xz\", \".csv\")}\", index=False)\n",
    "\n",
    "# with open(f\"2_Transformed/QuantileNormTrans_{basename}\", \"wb\") as f:\n",
    "#     pickle.dump(output_df.to_dict(), f, protocol=2)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    out_path = f\"2_Transformed/QuantileNormTrans_{basename}_{str(i+1).zfill(3)}.xz\"\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(chunk.to_dict(), f, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac7001-c46f-4c3a-981c-260455b5fa91",
   "metadata": {},
   "source": [
    "#### Monthly dU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5822d0-c8cd-46df-a563-0e3f9f7073eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpath = \"1_Input_DataSets/Monthly_DISPLACEMENT_dU_CRFP.xz\"\n",
    "basename = os.path.basename(fpath).split(\".\")[0]\n",
    "df = pd.read_pickle(fpath)\n",
    "trans_cols = [col for col in df.columns if col.startswith(\"N\")]\n",
    "transformed_df = apply_nscore(\n",
    "    df=df, idx_column=\"PointKey\", transformation_columns=trans_cols\n",
    ")\n",
    "# transformed_df.to_csv(f\"2_Transformed/NSCORE_{basename.replace(\".xz\", \".csv\")}\", index=False)\n",
    "\n",
    "# with open(f\"2_Transformed/NSCORE_{basename}\", \"wb\") as f:\n",
    "#     pickle.dump(transformed_df.to_dict(), f, protocol=2)\n",
    "\n",
    "number_of_chunks = 15\n",
    "\n",
    "# Split the output DataFrame into 10 equally sized chunks.\n",
    "chunks = np.array_split(transformed_df, number_of_chunks)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    out_path = f\"2_Transformed/NSCORE_{basename}_{str(i+1).zfill(3)}.xz\"\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(chunk.to_dict(), f, protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
