{
 "cells": [
  {
   "cell_type": "raw",
   "id": "21e72dec-1fbe-4ae5-a003-7fb54599f5ee",
   "metadata": {},
   "source": [
    "#### 2025/4/5\n",
    "\n",
    "I decided to have some adjustments.\n",
    "\n",
    "- **InSAR**: InSAR-derivded cumulative displacement --> Displacements\n",
    "- **Groundwater**: Groundwater Levels (original) --> Differences between two consecutive months\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cfccf9-1f19-40f0-a6a7-b61d3c589e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T00:10:37.907832Z",
     "iopub.status.busy": "2025-04-06T00:10:37.907832Z",
     "iopub.status.idle": "2025-04-06T00:11:13.441292Z",
     "shell.execute_reply": "2025-04-06T00:11:13.441292Z",
     "shell.execute_reply.started": "2025-04-06T00:10:37.907832Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bbbd86-ebc3-4a7b-a43b-0beaa90a0964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T00:11:13.444286Z",
     "iopub.status.busy": "2025-04-06T00:11:13.443288Z",
     "iopub.status.idle": "2025-04-06T00:11:13.459305Z",
     "shell.execute_reply": "2025-04-06T00:11:13.459305Z",
     "shell.execute_reply.started": "2025-04-06T00:11:13.444286Z"
    }
   },
   "outputs": [],
   "source": [
    "# subroutine 1\n",
    "search_station_func = lambda well_code: reverse_wellcode2station.get(well_code, np.nan)\n",
    "\n",
    "# subroutine 2\n",
    "decode_func = lambda ele: ele.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# subroutine 3\n",
    "def transform_to_df(data_dict, date_key=\"date\", value_key=\"values\"):\n",
    "    date_arr = pd.to_datetime([decode_func(ele) for ele in data_dict[date_key]])\n",
    "    value_arr = data_dict[value_key]\n",
    "    output_timeseries = pd.Series(data=value_arr, index=date_arr)\n",
    "    return output_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db57957-19a1-4521-8c77-4e05419c4233",
   "metadata": {},
   "source": [
    "#### Monthly Groundwater"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9518d52-5a67-4b69-a3f6-364cc7cc281b",
   "metadata": {},
   "source": [
    "previous version"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b878720e-6a60-4a40-b87c-7bdf8ef27f4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T07:00:27.892627Z",
     "iopub.status.busy": "2025-03-28T07:00:27.892627Z",
     "iopub.status.idle": "2025-03-28T07:00:29.557559Z",
     "shell.execute_reply": "2025-03-28T07:00:29.557559Z",
     "shell.execute_reply.started": "2025-03-28T07:00:27.892627Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Groundwater Data Processing and Timeseries Generation Workflow\n",
    "# --------------------------------------------------------------------\n",
    "# Step 1: Load groundwater data from an HDF5 file.\n",
    "#   - Define the file path.\n",
    "#   - Initialize the HDF5 object to extract measurement and info dictionaries.\n",
    "#   - List all station identifiers available in the data.\n",
    "# --------------------------------------------------------------------\n",
    "fpath = r\"20250228_Monthly_GWL_CRFP_v1.h5\"\n",
    "gwater_obj = MLCW(fpath)  # Initialize groundwater HDF5 object\n",
    "measure_data, info_data = gwater_obj.get_data()  # Get measurement and metadata\n",
    "available_stations = gwater_obj.list_stations()  # List available station IDs\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 2: Load layer classification files and build a reverse lookup.\n",
    "#   - Define the folder containing classification Excel files.\n",
    "#   - List all files matching the pattern \"GW_Layer*.xlsx\".\n",
    "#   - Build a reverse mapping from well codes to their corresponding station.\n",
    "# --------------------------------------------------------------------\n",
    "layer_classification_folder = (\n",
    "    r\"D:\\1000_SCRIPTS\\001_PreQE_Scripts\\GroundwaterLevels_DataProcessing\\GWL_Well_Info\\PersonalProcess\"\n",
    ")\n",
    "layer_classification_files = glob(\n",
    "    os.path.join(layer_classification_folder, \"GW_Layer*.xlsx\")\n",
    ")\n",
    "reverse_wellcode2station = {\n",
    "    well_code: station\n",
    "    for station in available_stations\n",
    "    for well_code in info_data[station]\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 3: Process each layer classification file.\n",
    "#   For each file:\n",
    "#   - Extract layer information and define a save path.\n",
    "#   - Load classification data (with well codes as strings).\n",
    "#   - Determine the list of stations for the current layer by applying\n",
    "#     a search function on the well codes.\n",
    "# --------------------------------------------------------------------\n",
    "for select_file in tqdm(layer_classification_files, desc=\"Processing file\"):\n",
    "    \n",
    "    # Derive the base filename (without path and extension)\n",
    "    file_basename = os.path.basename(select_file).split(\".\")[0]\n",
    "    # The current layer is assumed to be the last token in the basename\n",
    "    current_layer = file_basename.split(\"_\")[-1]\n",
    "    # Set the save path for the output timeseries DataFrame based on the layer\n",
    "    output_table_savepath = f\"1_Input_DataSets\\\\Monthly_GWL_CRFP_{current_layer}.xz\"\n",
    "    \n",
    "    # Load the classification Excel file ensuring that \"WELL_CODE\" is read as a string.\n",
    "    classify_df = pd.read_excel(select_file, dtype={\"WELL_CODE\": str})\n",
    "    # Extract the well codes from the classification file.\n",
    "    wellcode_byLayer = classify_df[\"WELL_CODE\"]\n",
    "    # Apply a search function to map well codes to station names, drop missing values,\n",
    "    # and convert the results into a list.\n",
    "    stations_byLayer = (\n",
    "        wellcode_byLayer.apply(search_station_func).dropna().tolist()\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 4: Process each station in the current layer to generate timeseries.\n",
    "    #   For every station:\n",
    "    #   - Create a blank output DataFrame with a predefined datetime index.\n",
    "    #   - Retrieve the measurement data for the station.\n",
    "    #   - Identify the common well code between the station's data and the classification.\n",
    "    #   - Convert the measurement dictionary into a timeseries DataFrame/Series.\n",
    "    #   - Map the resulting timeseries onto the output DataFrame's index.\n",
    "    #   - Extract spatial coordinates (X_TWD97 and Y_TWD97) from the classification data.\n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    # Create a blank DataFrame with a datetime index (2000-01-01 to 2025-01-01)\n",
    "    # to store the processed timeseries data for this layer.\n",
    "    output_table = pd.DataFrame(\n",
    "        data=None,\n",
    "        index=pd.date_range(start=\"2000-1-1\", end=\"2025-1-1\"),\n",
    "    )\n",
    "    \n",
    "    # Initialize lists to store coordinate values for each station.\n",
    "    X_TWD97_list = []\n",
    "    Y_TWD97_list = []\n",
    "    \n",
    "    # Loop over each station with a progress bar for station-level processing.\n",
    "    for select_station in tqdm(stations_byLayer, desc=\"Processing stations\", leave=False):\n",
    "        \n",
    "        # Retrieve measurement data for the current station.\n",
    "        measure_byStation = measure_data[select_station]\n",
    "        \n",
    "        # Identify the well code(s) that are common between the station's data and the\n",
    "        # classification well codes. Expect exactly one match.\n",
    "        target_wellcode = set(measure_byStation.keys()).intersection(\n",
    "            set(wellcode_byLayer)\n",
    "        )\n",
    "        if len(target_wellcode) == 1:\n",
    "            target_wellcode = list(target_wellcode)[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Multiple well codes at {select_station}\")\n",
    "        \n",
    "        # Get the measurement dictionary for the matched well code.\n",
    "        data_dict = measure_byStation[target_wellcode][\"measure\"]\n",
    "        # Transform the measurement dictionary into a timeseries DataFrame/Series.\n",
    "        output_timeseries = transform_to_df(data_dict)\n",
    "        # Map the timeseries data to the output_table's datetime index and store it\n",
    "        # in a new column named with the station and well code.\n",
    "        output_table[f\"{select_station}_{target_wellcode}\"] = (\n",
    "            output_table.index.map(output_timeseries)\n",
    "        )\n",
    "        \n",
    "        # ----------------- Extract Spatial Coordinates -----------------\n",
    "        # Query the classification DataFrame for the X and Y coordinates corresponding\n",
    "        # to the target well code and append them to the respective lists.\n",
    "        X_TWD97_list.append(\n",
    "            classify_df.query(\"WELL_CODE==@target_wellcode\")\n",
    "            .get(\"X_TWD97\").iloc[0]\n",
    "        )\n",
    "        Y_TWD97_list.append(\n",
    "            classify_df.query(\"WELL_CODE==@target_wellcode\")\n",
    "            .get(\"Y_TWD97\").iloc[0]\n",
    "        )\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 5: Post-Processing and Saving the Timeseries Data\n",
    "    #   - Trim the output table to include only dates with valid data.\n",
    "    #   - Transpose and clean the DataFrame for easier analysis.\n",
    "    #   - Insert the extracted coordinates into the DataFrame.\n",
    "    #   - Save the final DataFrame to a pickle file.\n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    # Limit the output_table to the date range from 2014 to 2021.\n",
    "    output_table = output_table.loc[\"2014\":\"2021\", :]\n",
    "    \n",
    "    # Transpose the DataFrame, drop rows with all NaNs, and rename columns\n",
    "    # using a date format (prefixed with \"N\"). Reset the index to have a column \"STATION\".\n",
    "    trans_output_table = output_table.dropna(how=\"all\").T\n",
    "    trans_output_table.columns = [\n",
    "        \"N\" + col.strftime(\"%Y%m%d\") for col in trans_output_table.columns\n",
    "    ]\n",
    "    trans_output_table = trans_output_table.reset_index().rename(\n",
    "        {\"index\": \"STATION\"}, axis=1\n",
    "    )\n",
    "    \n",
    "    # Insert the extracted X and Y coordinates into the DataFrame.\n",
    "    for col, value_arr in zip([\"Y_TWD97\", \"X_TWD97\"], [Y_TWD97_list, X_TWD97_list]):\n",
    "        trans_output_table.insert(loc=1, column=col, value=value_arr)\n",
    "    \n",
    "    # Save the processed timeseries DataFrame as a pickle file, using the\n",
    "    # current layer identifier in the filename.\n",
    "    trans_output_table.to_pickle(output_table_savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fa9414-fefb-4e4e-9e1d-60a9fe8b5d49",
   "metadata": {},
   "source": [
    "current version (2025/5/4)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a872dd90-b7dc-481b-b1f4-54bece8409c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T01:57:35.141462Z",
     "iopub.status.busy": "2025-04-05T01:57:35.141462Z",
     "iopub.status.idle": "2025-04-05T01:57:37.160961Z",
     "shell.execute_reply": "2025-04-05T01:57:37.160961Z",
     "shell.execute_reply.started": "2025-04-05T01:57:35.141462Z"
    }
   },
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Groundwater Data Processing and Timeseries Generation Workflow\n",
    "# --------------------------------------------------------------------\n",
    "# Step 1: Load groundwater data from an HDF5 file.\n",
    "#   - Define the file path.\n",
    "#   - Initialize the HDF5 object to extract measurement and info dictionaries.\n",
    "#   - List all station identifiers available in the data.\n",
    "# --------------------------------------------------------------------\n",
    "fpath = r\"20250228_Monthly_GWL_CRFP_v1.h5\"\n",
    "gwater_obj = MLCW(fpath)  # Initialize groundwater HDF5 object\n",
    "measure_data, info_data = gwater_obj.get_data()  # Get measurement and metadata\n",
    "available_stations = gwater_obj.list_stations()  # List available station IDs\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 2: Load layer classification files and build a reverse lookup.\n",
    "#   - Define the folder containing classification Excel files.\n",
    "#   - List all files matching the pattern \"GW_Layer*.xlsx\".\n",
    "#   - Build a reverse mapping from well codes to their corresponding station.\n",
    "# --------------------------------------------------------------------\n",
    "layer_classification_folder = r\"D:\\1000_SCRIPTS\\001_PreQE_Scripts\\GroundwaterLevels_DataProcessing\\GWL_Well_Info\\PersonalProcess\"\n",
    "layer_classification_files = glob(\n",
    "    os.path.join(layer_classification_folder, \"GW_Layer*.xlsx\")\n",
    ")\n",
    "reverse_wellcode2station = {\n",
    "    well_code: station\n",
    "    for station in available_stations\n",
    "    for well_code in info_data[station]\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 3: Process each layer classification file.\n",
    "#   For each file:\n",
    "#   - Extract layer information and define a save path.\n",
    "#   - Load classification data (with well codes as strings).\n",
    "#   - Determine the list of stations for the current layer by applying\n",
    "#     a search function on the well codes.\n",
    "# --------------------------------------------------------------------\n",
    "for select_file in tqdm(layer_classification_files[:], desc=\"Processing file\"):\n",
    "\n",
    "    # Derive the base filename (without path and extension)\n",
    "    file_basename = os.path.basename(select_file).split(\".\")[0]\n",
    "    # The current layer is assumed to be the last token in the basename\n",
    "    current_layer = file_basename.split(\"_\")[-1]\n",
    "    # Set the save path for the output timeseries DataFrame based on the layer\n",
    "    output_table_savepath = f\"1_Input_DataSets\\\\Monthly_GWL_CRFP_{current_layer}.xz\"\n",
    "\n",
    "    # Load the classification Excel file ensuring that \"WELL_CODE\" is read as a string.\n",
    "    classify_df = pd.read_excel(select_file, dtype={\"WELL_CODE\": str})\n",
    "    # Extract the well codes from the classification file.\n",
    "    wellcode_byLayer = classify_df[\"WELL_CODE\"]\n",
    "    # Apply a search function to map well codes to station names, drop missing values,\n",
    "    # and convert the results into a list.\n",
    "    stations_byLayer = wellcode_byLayer.apply(search_station_func).dropna().tolist()\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 4: Process each station in the current layer to generate timeseries.\n",
    "    #   For every station:\n",
    "    #   - Create a blank output DataFrame with a predefined datetime index.\n",
    "    #   - Retrieve the measurement data for the station.\n",
    "    #   - Identify the common well code between the station's data and the classification.\n",
    "    #   - Convert the measurement dictionary into a timeseries DataFrame/Series.\n",
    "    #   - Map the resulting timeseries onto the output DataFrame's index.\n",
    "    #   - Extract spatial coordinates (X_TWD97 and Y_TWD97) from the classification data.\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    # Create a blank DataFrame with a datetime index (2000-01-01 to 2025-01-01)\n",
    "    # to store the processed timeseries data for this layer.\n",
    "    output_table = pd.DataFrame(\n",
    "        data=None,\n",
    "        index=pd.date_range(start=\"2000-1-1\", end=\"2025-1-1\"),\n",
    "    )\n",
    "\n",
    "    # Initialize lists to store coordinate values for each station.\n",
    "    X_TWD97_list = []\n",
    "    Y_TWD97_list = []\n",
    "\n",
    "    # Loop over each station with a progress bar for station-level processing.\n",
    "    for select_station in tqdm(stations_byLayer, desc=\"Processing stations\", leave=False):\n",
    "\n",
    "        # Retrieve measurement data for the current station.\n",
    "        measure_byStation = measure_data[select_station]\n",
    "\n",
    "        # Identify the well code(s) that are common between the station's data and the\n",
    "        # classification well codes. Expect exactly one match.\n",
    "        target_wellcode = set(measure_byStation.keys()).intersection(\n",
    "            set(wellcode_byLayer)\n",
    "        )\n",
    "        if len(target_wellcode) == 1:\n",
    "            target_wellcode = list(target_wellcode)[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Multiple well codes at {select_station}\")\n",
    "\n",
    "        # Get the measurement dictionary for the matched well code.\n",
    "        data_dict = measure_byStation[target_wellcode][\"measure\"]\n",
    "        # Transform the measurement dictionary into a timeseries DataFrame/Series.\n",
    "        output_timeseries = transform_to_df(data_dict)\n",
    "        # Map the timeseries data to the output_table's datetime index and store it\n",
    "        # in a new column named with the station and well code.\n",
    "        output_table[f\"{select_station}_{target_wellcode}\"] = output_table.index.map(\n",
    "            output_timeseries\n",
    "        )\n",
    "\n",
    "        # ----------------- Extract Spatial Coordinates -----------------\n",
    "        # Query the classification DataFrame for the X and Y coordinates corresponding\n",
    "        # to the target well code and append them to the respective lists.\n",
    "        X_TWD97_list.append(\n",
    "            classify_df.query(\"WELL_CODE==@target_wellcode\").get(\"X_TWD97\").iloc[0]\n",
    "        )\n",
    "        Y_TWD97_list.append(\n",
    "            classify_df.query(\"WELL_CODE==@target_wellcode\").get(\"Y_TWD97\").iloc[0]\n",
    "        )\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 5: Post-Processing and Saving the Timeseries Data\n",
    "    #   - Trim the output table to include only dates with valid data.\n",
    "    #   - Transpose and clean the DataFrame for easier analysis.\n",
    "    #   - Insert the extracted coordinates into the DataFrame.\n",
    "    #   - Save the final DataFrame to a pickle file.\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    # Limit the output_table to the date range from 2014 to 2021.\n",
    "    output_table = output_table.loc[\"2013\":\"2021\", :]\n",
    "\n",
    "    # Transpose the DataFrame, drop rows with all NaNs, and rename columns\n",
    "    # using a date format (prefixed with \"N\"). Reset the index to have a column \"STATION\".\n",
    "    trans_output_table = output_table.dropna(how=\"all\").T\n",
    "    trans_output_table.columns = [\n",
    "        \"N\" + col.strftime(\"%Y%m%d\") for col in trans_output_table.columns\n",
    "    ]\n",
    "\n",
    "    #####################################################\n",
    "    #\n",
    "    # [UPDATED: 2025-04-05] : Calculate the differences\n",
    "    # between two consecutive months\n",
    "    #\n",
    "    #####################################################\n",
    "    trans_output_table = trans_output_table.diff(axis=1).dropna(how=\"all\", axis=1)\n",
    "\n",
    "    trans_output_table = trans_output_table.reset_index().rename(\n",
    "        {\"index\": \"STATION\"}, axis=1\n",
    "    )\n",
    "\n",
    "    # Insert the extracted X and Y coordinates into the DataFrame.\n",
    "    for col, value_arr in zip([\"Y_TWD97\", \"X_TWD97\"], [Y_TWD97_list, X_TWD97_list]):\n",
    "        trans_output_table.insert(loc=1, column=col, value=value_arr)\n",
    "\n",
    "    # Save the processed timeseries DataFrame as a pickle file, using the\n",
    "    # current layer identifier in the filename.\n",
    "    trans_output_table.to_pickle(output_table_savepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac505608-91bd-469b-a3e9-034e52729976",
   "metadata": {},
   "source": [
    "#### Monthly Rainfall\n",
    "\n",
    "The monthly rainfall data was prepared in file `20250317_PrepareRainfall.ipynb`\n",
    "\n",
    "Summarized Workflow\n",
    "```\n",
    "The `process_measure_data` function processes precipitation data for a specific station through the following workflow:\n",
    "\n",
    "1. It extracts data for the selected station and ensures hourly frequency with missing values filled.\n",
    "2. It calculates daily precipitation totals by summing hourly values and extracts both the summed values and corresponding dates.\n",
    "3. Similarly, it computes monthly precipitation totals by summing to monthly frequency and extracts the monthly values with their dates.\n",
    "4. Finally, it organizes all processed data (daily and monthly values with their corresponding dates) into a dictionary that serves as the function's output.\n",
    "```"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9de42839-b706-4cb9-bbb6-f8d6139c8ab2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T03:18:54.357400Z",
     "iopub.status.busy": "2025-04-05T03:18:54.357400Z",
     "iopub.status.idle": "2025-04-05T03:18:54.783262Z",
     "shell.execute_reply": "2025-04-05T03:18:54.783262Z",
     "shell.execute_reply.started": "2025-04-05T03:18:54.357400Z"
    },
    "scrolled": true
   },
   "source": [
    "\"\"\"\n",
    "Last Updated: 2025-04-05\n",
    "\"\"\"\n",
    "\n",
    "# Load raw rainfall data from HDF5 file\n",
    "# The data was preprocessed in a previous script (20250317_PrepareRainfall)\n",
    "fpath = r\"20250317_Rainfall_CRFP_monthly_v1.h5\"\n",
    "rainall_obj = MLCW(fpath)  # Initialize HDF5 data access object for rainfall measurements\n",
    "\n",
    "# Extract both measurement data and associated metadata\n",
    "# measure_data: Dictionary with station IDs as keys and measurement data as values\n",
    "# info_data: Dictionary with station IDs as keys and metadata (coordinates, etc.) as values\n",
    "measure_data, info_data = rainall_obj.get_data()\n",
    "\n",
    "# Retrieve list of all available monitoring stations in the dataset\n",
    "available_stations = rainall_obj.list_stations()\n",
    "\n",
    "# Initialize empty dataframe with complete monthly date range as index\n",
    "# This serves as a template for organizing all station data with a uniform temporal structure\n",
    "output_table = pd.DataFrame(\n",
    "    data=None,\n",
    "    index=pd.date_range(start=\"2000-1-1\", end=\"2025-1-1\", freq=\"MS\"),  # Monthly frequency\n",
    ")\n",
    "\n",
    "# Initialize lists to store spatial coordinate information for each station\n",
    "x_twd97_list = []  # Easting coordinates in TWD97 projection system\n",
    "y_twd97_list = []  # Northing coordinates in TWD97 projection system\n",
    "\n",
    "# Iterate through each station to extract time series and coordinate information\n",
    "# tqdm provides a progress bar for monitoring the processing status\n",
    "for select_station in tqdm(available_stations):\n",
    "    \n",
    "    # Convert raw measurement data for current station into a pandas DataFrame\n",
    "    # transform_to_df is a helper function that maps date-value pairs into a DataFrame format\n",
    "    rainfall_timeseries = transform_to_df(\n",
    "        data_dict=measure_data[select_station],\n",
    "        date_key=\"monthly_date\",      # Key containing timestamp information\n",
    "        value_key=\"monthly_values\",   # Key containing rainfall measurements\n",
    "    )\n",
    "    \n",
    "    # Extract and store spatial coordinates for the current station\n",
    "    x_twd97_list.append(info_data[select_station][\"X_TWD97\"])  # Easting coordinate\n",
    "    y_twd97_list.append(info_data[select_station][\"Y_TWD97\"])  # Northing coordinate\n",
    "    \n",
    "    # Map the station's rainfall data to the common date range template\n",
    "    # This ensures uniform temporal coverage across all stations\n",
    "    output_table[select_station] = output_table.index.map(rainfall_timeseries)\n",
    "\n",
    "# Subset the data to the specific study period (2014-2021)\n",
    "# This timeframe represents the period with most complete and reliable measurements\n",
    "output_table = output_table.loc[\"2014\":\"2021\", :]\n",
    "\n",
    "# Transpose the dataframe to have stations as rows and dates as columns\n",
    "# This format is more suitable for spatial analysis and station-based filtering\n",
    "trans_output_table = output_table.dropna(how=\"all\").T  # Remove dates with no data across all stations\n",
    "\n",
    "# Rename date columns to a standardized format (N + YYYYMMDD)\n",
    "# The 'N' prefix indicates these are rainfall values (as opposed to other variables)\n",
    "trans_output_table.columns = [\n",
    "    \"N\" + col.strftime(\"%Y%m%d\") for col in trans_output_table.columns\n",
    "]\n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# [UPDATED: 2025-04-05] : Quality Control - Station Filtering\n",
    "# Remove stations with cumulative rainfall < 100 mm over the entire 7-year period\n",
    "# This eliminates potentially malfunctioning or inconsistent monitoring stations\n",
    "#\n",
    "#############################################################################\n",
    "# Calculate total accumulated rainfall for each station over the entire period\n",
    "sum_alltime = trans_output_table.sum(axis=1)\n",
    "\n",
    "# Filter out stations with suspiciously low cumulative rainfall\n",
    "# Threshold of 100mm over 7 years indicates possible sensor malfunction\n",
    "trans_output_table = trans_output_table[sum_alltime > 100]\n",
    "\n",
    "# Previous direct insertion method (commented out for reference)\n",
    "# # Insert the extracted X and Y coordinates into the DataFrame.\n",
    "# for col, value_arr in zip([\"Y_TWD97\", \"X_TWD97\"], [y_twd97_list, x_twd97_list]):\n",
    "#     trans_output_table.insert(loc=1, column=col, value=value_arr)\n",
    "\n",
    "# Create a dedicated dataframe for station coordinates for more efficient mapping\n",
    "# This approach maintains spatial information even after filtering operations\n",
    "coordinate_df = pd.DataFrame({\n",
    "    \"STATION\": available_stations,  # Station identifiers\n",
    "    \"X_TWD97\": x_twd97_list,        # Easting coordinates \n",
    "    \"Y_TWD97\": y_twd97_list         # Northing coordinates\n",
    "})\n",
    "coordinate_df.set_index(\"STATION\", inplace=True)\n",
    "\n",
    "# Map coordinate information to the filtered stations dataframe\n",
    "# Using map ensures coordinates are properly aligned with corresponding stations\n",
    "trans_output_table[\"X_TWD97\"] = trans_output_table.index.map(coordinate_df[\"X_TWD97\"])\n",
    "trans_output_table[\"Y_TWD97\"] = trans_output_table.index.map(coordinate_df[\"Y_TWD97\"])\n",
    "\n",
    "# Define column order for the final dataset\n",
    "# First station identifier and coordinates, then chronological measurement columns\n",
    "info_cols = [\"STATION\", \"X_TWD97\", \"Y_TWD97\"]\n",
    "\n",
    "# Reset index to convert station ID from index to regular column\n",
    "# Rename the column to ensure consistent naming convention\n",
    "trans_output_table = trans_output_table.reset_index().rename(\n",
    "    {\"index\": \"STATION\"}, axis=1\n",
    ")\n",
    "\n",
    "# Build complete column list with information columns followed by temporal data columns\n",
    "info_cols.extend([col for col in trans_output_table.columns if col.startswith(\"N\")])\n",
    "\n",
    "# Reorganize dataframe to follow the specified column order\n",
    "trans_output_table = trans_output_table.loc[:, info_cols]\n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# [UPDATED: 2025-04-05] : The record in November 2019 is totally zero (all stations)\n",
    "# , which is very weird, and subsequently effects the data transformation and \n",
    "# interpolation, so I assume this is due to error of data recordings\n",
    "# --> assume the records of November 2019 = records of October 2019\n",
    "#\n",
    "#############################################################################\n",
    "trans_output_table[\"N20191101\"] = trans_output_table[\"N20191001\"]\n",
    "\n",
    "# Export the final processed dataset as a compressed pickle file\n",
    "# The .xz compression format offers optimal balance between size and access speed\n",
    "trans_output_table.to_pickle(\"1_Input_DataSets/Monthly_Rainfall_CRFP.xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6ca69-2fb2-45bf-9e4f-24e7f2e6f38b",
   "metadata": {},
   "source": [
    "#### Monthly Electricity Consumption"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e98dfd9-be4d-4bba-ac7d-ea3f13ab10a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-05T03:20:17.579023Z",
     "iopub.status.busy": "2025-04-05T03:20:17.579023Z",
     "iopub.status.idle": "2025-04-05T03:21:29.093925Z",
     "shell.execute_reply": "2025-04-05T03:21:29.093925Z",
     "shell.execute_reply.started": "2025-04-05T03:20:17.579023Z"
    }
   },
   "source": [
    "fpath = r\"CRFP_Ebill_2014_2021_agriculture.xz\"\n",
    "df = pd.read_pickle(fpath)\n",
    "\n",
    "select_df = df.query(\"Outliers==0\")\n",
    "\n",
    "final_columns = [\"WELL_NO\", \"TWD97_X\", \"TWD97_Y\", \"PointKey\"] + [\n",
    "    col for col in select_df.columns if col.startswith(\"N\")\n",
    "]\n",
    "\n",
    "output_df = select_df.loc[:, final_columns].reset_index(drop=True)\n",
    "output_df = output_df.rename({\"TWD97_X\":\"X_TWD97\", \"TWD97_Y\":\"Y_TWD97\"}, axis=1)\n",
    "\n",
    "output_df.to_pickle(\"1_Input_DataSets/Monthly_Electricity_CRFP.xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a8b71-ec69-4e95-8652-6b3a8dc9efe6",
   "metadata": {},
   "source": [
    "#### Monthly Vertical Cumulative Displacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f76c7996-6c93-4d1c-b132-298baeb83b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-06T00:19:54.641705Z",
     "iopub.status.busy": "2025-04-06T00:19:54.641705Z",
     "iopub.status.idle": "2025-04-06T00:23:07.255181Z",
     "shell.execute_reply": "2025-04-06T00:23:07.255181Z",
     "shell.execute_reply.started": "2025-04-06T00:19:54.641705Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fpath = r\"MonthlyResample_dU_timeseries.pkl\"\n",
    "df = pd.read_pickle(fpath, compression='zip')\n",
    "\n",
    "info_df = pd.read_pickle(r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\1_PrepareDatasets\\3_InSAR\\ASC_DESC_inc_azim.xz\")\n",
    "xtwd97_list = info_df['geometry'].apply(lambda ele: ele.x).values\n",
    "ytwd97_list = info_df['geometry'].apply(lambda ele: ele.y).values\n",
    "\n",
    "for coord, col in zip([ytwd97_list, xtwd97_list], [\"Y_TWD97\", \"X_TWD97\"]):\n",
    "    df.insert(loc=0, column=col, value=coord)\n",
    "\n",
    "df = df.reset_index(drop=False)\n",
    "df = df.rename({\"index\":\"PointKey\"}, axis=1)\n",
    "\n",
    "df.columns = [pd.to_datetime(col[1:]) if col.startswith(\"D\") else col for col in df.columns]\n",
    "\n",
    "df = df.set_index([\"PointKey\", \"X_TWD97\", \"Y_TWD97\"])\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns)\n",
    "\n",
    "output_df = df.loc[:, \"2014\":\"2021\"].copy()\n",
    "\n",
    "#############################################################################\n",
    "#\n",
    "# [UPDATED: 2025-04-06] : convert cumdisp to diff between two consecutive months\n",
    "#\n",
    "#############################################################################\n",
    "output_df = output_df.diff(axis=1)\n",
    "output_df.iloc[:, 0] = df.iloc[:, 0]\n",
    "\n",
    "#############################################################################\n",
    "output_df.columns = [\"N\"+col.strftime(\"%Y%m%d\") for col in output_df.columns]\n",
    "output_df = output_df.reset_index(drop=False)\n",
    "output_df.to_pickle(r\"1_Input_DataSets/Monthly_DISPLACEMENT_dU_CRFP.xz\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
