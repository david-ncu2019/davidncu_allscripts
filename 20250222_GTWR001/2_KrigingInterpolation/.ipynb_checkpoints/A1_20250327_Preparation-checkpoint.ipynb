{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0cfccf9-1f19-40f0-a6a7-b61d3c589e2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:09:40.936556Z",
     "iopub.status.busy": "2025-03-30T04:09:40.936556Z",
     "iopub.status.idle": "2025-03-30T04:09:43.372863Z",
     "shell.execute_reply": "2025-03-30T04:09:43.372863Z",
     "shell.execute_reply.started": "2025-03-30T04:09:40.936556Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61bbbd86-ebc3-4a7b-a43b-0beaa90a0964",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:09:43.375854Z",
     "iopub.status.busy": "2025-03-30T04:09:43.375854Z",
     "iopub.status.idle": "2025-03-30T04:09:43.379466Z",
     "shell.execute_reply": "2025-03-30T04:09:43.379466Z",
     "shell.execute_reply.started": "2025-03-30T04:09:43.375854Z"
    }
   },
   "outputs": [],
   "source": [
    "# subroutine 1\n",
    "search_station_func = lambda well_code: reverse_wellcode2station.get(\n",
    "    well_code, np.nan\n",
    ")\n",
    "\n",
    "# subroutine 2\n",
    "decode_func = lambda ele: ele.decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# subroutine 3\n",
    "def transform_to_df(data_dict, date_key=\"date\", value_key=\"values\"):\n",
    "    date_arr = pd.to_datetime(\n",
    "        [decode_func(ele) for ele in data_dict[date_key]]\n",
    "    )\n",
    "    value_arr = data_dict[value_key]\n",
    "    output_timeseries = pd.Series(data=value_arr, index=date_arr)\n",
    "    return output_timeseries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db57957-19a1-4521-8c77-4e05419c4233",
   "metadata": {},
   "source": [
    "#### Monthly Groundwater"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b878720e-6a60-4a40-b87c-7bdf8ef27f4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T07:00:27.892627Z",
     "iopub.status.busy": "2025-03-28T07:00:27.892627Z",
     "iopub.status.idle": "2025-03-28T07:00:29.557559Z",
     "shell.execute_reply": "2025-03-28T07:00:29.557559Z",
     "shell.execute_reply.started": "2025-03-28T07:00:27.892627Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Groundwater Data Processing and Timeseries Generation Workflow\n",
    "# --------------------------------------------------------------------\n",
    "# Step 1: Load groundwater data from an HDF5 file.\n",
    "#   - Define the file path.\n",
    "#   - Initialize the HDF5 object to extract measurement and info dictionaries.\n",
    "#   - List all station identifiers available in the data.\n",
    "# --------------------------------------------------------------------\n",
    "fpath = r\"20250228_Monthly_GWL_CRFP_v1.h5\"\n",
    "gwater_obj = MLCW(fpath)  # Initialize groundwater HDF5 object\n",
    "measure_data, info_data = gwater_obj.get_data()  # Get measurement and metadata\n",
    "available_stations = gwater_obj.list_stations()  # List available station IDs\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 2: Load layer classification files and build a reverse lookup.\n",
    "#   - Define the folder containing classification Excel files.\n",
    "#   - List all files matching the pattern \"GW_Layer*.xlsx\".\n",
    "#   - Build a reverse mapping from well codes to their corresponding station.\n",
    "# --------------------------------------------------------------------\n",
    "layer_classification_folder = (\n",
    "    r\"D:\\1000_SCRIPTS\\001_PreQE_Scripts\\GroundwaterLevels_DataProcessing\\GWL_Well_Info\\PersonalProcess\"\n",
    ")\n",
    "layer_classification_files = glob(\n",
    "    os.path.join(layer_classification_folder, \"GW_Layer*.xlsx\")\n",
    ")\n",
    "reverse_wellcode2station = {\n",
    "    well_code: station\n",
    "    for station in available_stations\n",
    "    for well_code in info_data[station]\n",
    "}\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Step 3: Process each layer classification file.\n",
    "#   For each file:\n",
    "#   - Extract layer information and define a save path.\n",
    "#   - Load classification data (with well codes as strings).\n",
    "#   - Determine the list of stations for the current layer by applying\n",
    "#     a search function on the well codes.\n",
    "# --------------------------------------------------------------------\n",
    "for select_file in tqdm(layer_classification_files, desc=\"Processing file\"):\n",
    "    \n",
    "    # Derive the base filename (without path and extension)\n",
    "    file_basename = os.path.basename(select_file).split(\".\")[0]\n",
    "    # The current layer is assumed to be the last token in the basename\n",
    "    current_layer = file_basename.split(\"_\")[-1]\n",
    "    # Set the save path for the output timeseries DataFrame based on the layer\n",
    "    output_table_savepath = f\"1_Input_DataSets\\\\Monthly_GWL_CRFP_{current_layer}.xz\"\n",
    "    \n",
    "    # Load the classification Excel file ensuring that \"WELL_CODE\" is read as a string.\n",
    "    classify_df = pd.read_excel(select_file, dtype={\"WELL_CODE\": str})\n",
    "    # Extract the well codes from the classification file.\n",
    "    wellcode_byLayer = classify_df[\"WELL_CODE\"]\n",
    "    # Apply a search function to map well codes to station names, drop missing values,\n",
    "    # and convert the results into a list.\n",
    "    stations_byLayer = (\n",
    "        wellcode_byLayer.apply(search_station_func).dropna().tolist()\n",
    "    )\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 4: Process each station in the current layer to generate timeseries.\n",
    "    #   For every station:\n",
    "    #   - Create a blank output DataFrame with a predefined datetime index.\n",
    "    #   - Retrieve the measurement data for the station.\n",
    "    #   - Identify the common well code between the station's data and the classification.\n",
    "    #   - Convert the measurement dictionary into a timeseries DataFrame/Series.\n",
    "    #   - Map the resulting timeseries onto the output DataFrame's index.\n",
    "    #   - Extract spatial coordinates (X_TWD97 and Y_TWD97) from the classification data.\n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    # Create a blank DataFrame with a datetime index (2000-01-01 to 2025-01-01)\n",
    "    # to store the processed timeseries data for this layer.\n",
    "    output_table = pd.DataFrame(\n",
    "        data=None,\n",
    "        index=pd.date_range(start=\"2000-1-1\", end=\"2025-1-1\"),\n",
    "    )\n",
    "    \n",
    "    # Initialize lists to store coordinate values for each station.\n",
    "    X_TWD97_list = []\n",
    "    Y_TWD97_list = []\n",
    "    \n",
    "    # Loop over each station with a progress bar for station-level processing.\n",
    "    for select_station in tqdm(stations_byLayer, desc=\"Processing stations\", leave=False):\n",
    "        \n",
    "        # Retrieve measurement data for the current station.\n",
    "        measure_byStation = measure_data[select_station]\n",
    "        \n",
    "        # Identify the well code(s) that are common between the station's data and the\n",
    "        # classification well codes. Expect exactly one match.\n",
    "        target_wellcode = set(measure_byStation.keys()).intersection(\n",
    "            set(wellcode_byLayer)\n",
    "        )\n",
    "        if len(target_wellcode) == 1:\n",
    "            target_wellcode = list(target_wellcode)[0]\n",
    "        else:\n",
    "            raise ValueError(f\"Multiple well codes at {select_station}\")\n",
    "        \n",
    "        # Get the measurement dictionary for the matched well code.\n",
    "        data_dict = measure_byStation[target_wellcode][\"measure\"]\n",
    "        # Transform the measurement dictionary into a timeseries DataFrame/Series.\n",
    "        output_timeseries = transform_to_df(data_dict)\n",
    "        # Map the timeseries data to the output_table's datetime index and store it\n",
    "        # in a new column named with the station and well code.\n",
    "        output_table[f\"{select_station}_{target_wellcode}\"] = (\n",
    "            output_table.index.map(output_timeseries)\n",
    "        )\n",
    "        \n",
    "        # ----------------- Extract Spatial Coordinates -----------------\n",
    "        # Query the classification DataFrame for the X and Y coordinates corresponding\n",
    "        # to the target well code and append them to the respective lists.\n",
    "        X_TWD97_list.append(\n",
    "            classify_df.query(\"WELL_CODE==@target_wellcode\")\n",
    "            .get(\"X_TWD97\").iloc[0]\n",
    "        )\n",
    "        Y_TWD97_list.append(\n",
    "            classify_df.query(\"WELL_CODE==@target_wellcode\")\n",
    "            .get(\"Y_TWD97\").iloc[0]\n",
    "        )\n",
    "    \n",
    "    # --------------------------------------------------------------------\n",
    "    # Step 5: Post-Processing and Saving the Timeseries Data\n",
    "    #   - Trim the output table to include only dates with valid data.\n",
    "    #   - Transpose and clean the DataFrame for easier analysis.\n",
    "    #   - Insert the extracted coordinates into the DataFrame.\n",
    "    #   - Save the final DataFrame to a pickle file.\n",
    "    # --------------------------------------------------------------------\n",
    "    \n",
    "    # Limit the output_table to the date range from 2014 to 2021.\n",
    "    output_table = output_table.loc[\"2014\":\"2021\", :]\n",
    "    \n",
    "    # Transpose the DataFrame, drop rows with all NaNs, and rename columns\n",
    "    # using a date format (prefixed with \"N\"). Reset the index to have a column \"STATION\".\n",
    "    trans_output_table = output_table.dropna(how=\"all\").T\n",
    "    trans_output_table.columns = [\n",
    "        \"N\" + col.strftime(\"%Y%m%d\") for col in trans_output_table.columns\n",
    "    ]\n",
    "    trans_output_table = trans_output_table.reset_index().rename(\n",
    "        {\"index\": \"STATION\"}, axis=1\n",
    "    )\n",
    "    \n",
    "    # Insert the extracted X and Y coordinates into the DataFrame.\n",
    "    for col, value_arr in zip([\"Y_TWD97\", \"X_TWD97\"], [Y_TWD97_list, X_TWD97_list]):\n",
    "        trans_output_table.insert(loc=1, column=col, value=value_arr)\n",
    "    \n",
    "    # Save the processed timeseries DataFrame as a pickle file, using the\n",
    "    # current layer identifier in the filename.\n",
    "    trans_output_table.to_pickle(output_table_savepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac505608-91bd-469b-a3e9-034e52729976",
   "metadata": {},
   "source": [
    "#### Monthly Rainfall"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d733a2c2-9858-45ac-9885-b3cf21cffd01",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T13:11:25.216754Z",
     "iopub.status.busy": "2025-03-28T13:11:25.215758Z",
     "iopub.status.idle": "2025-03-28T13:11:25.655697Z",
     "shell.execute_reply": "2025-03-28T13:11:25.655697Z",
     "shell.execute_reply.started": "2025-03-28T13:11:25.215758Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "source": [
    "fpath = r\"20250317_Rainfall_CRFP_monthly_v1.h5\"\n",
    "rainall_obj = MLCW(fpath)  # Initialize groundwater HDF5 object\n",
    "measure_data, info_data = (\n",
    "    rainall_obj.get_data()\n",
    ")  # Get measurement and metadata\n",
    "available_stations = (\n",
    "    rainall_obj.list_stations()\n",
    ")  # List available station IDs\n",
    "\n",
    "output_table = pd.DataFrame(\n",
    "    data=None,\n",
    "    index=pd.date_range(start=\"2000-1-1\", end=\"2025-1-1\"),\n",
    ")\n",
    "\n",
    "x_twd97_list = []\n",
    "y_twd97_list = []\n",
    "\n",
    "# select_station = available_stations[0]\n",
    "for select_station in tqdm(available_stations):\n",
    "    \n",
    "    rainfall_timeseries =  transform_to_df(\n",
    "        data_dict=measure_data[select_station],\n",
    "        date_key=\"monthly_date\",\n",
    "        value_key=\"monthly_values\",\n",
    "    )\n",
    "    \n",
    "    x_twd97_list.append(info_data[select_station][\"X_TWD97\"])\n",
    "    y_twd97_list.append(info_data[select_station][\"Y_TWD97\"])\n",
    "\n",
    "    output_table[select_station] = output_table.index.map(rainfall_timeseries)\n",
    "\n",
    "output_table = output_table.loc[\"2014\":\"2021\", :]\n",
    "trans_output_table = output_table.dropna(how=\"all\").T\n",
    "trans_output_table.columns = [\n",
    "    \"N\" + col.strftime(\"%Y%m%d\") for col in trans_output_table.columns\n",
    "]\n",
    "trans_output_table = trans_output_table.reset_index().rename(\n",
    "    {\"index\": \"STATION\"}, axis=1\n",
    ")\n",
    "\n",
    "# Insert the extracted X and Y coordinates into the DataFrame.\n",
    "for col, value_arr in zip([\"Y_TWD97\", \"X_TWD97\"], [y_twd97_list, x_twd97_list]):\n",
    "    trans_output_table.insert(loc=1, column=col, value=value_arr)\n",
    "\n",
    "trans_output_table.to_pickle(\"1_Input_DataSets/Monthly_Rainfall_CRFP.xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d6ca69-2fb2-45bf-9e4f-24e7f2e6f38b",
   "metadata": {},
   "source": [
    "#### Monthly Electricity Consumption"
   ]
  },
  {
   "cell_type": "raw",
   "id": "577b1ab7-232c-48e3-90fb-48e2b6cc2083",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-28T13:58:53.286232Z",
     "iopub.status.busy": "2025-03-28T13:58:53.286232Z",
     "iopub.status.idle": "2025-03-28T13:58:54.831829Z",
     "shell.execute_reply": "2025-03-28T13:58:54.831829Z",
     "shell.execute_reply.started": "2025-03-28T13:58:53.286232Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fpath = r\"CRFP_Ebill_2014_2021_agriculture.xz\"\n",
    "df = pd.read_pickle(fpath)\n",
    "\n",
    "select_df = df.query(\"Outliers==0\")\n",
    "\n",
    "final_columns = [\"WELL_NO\", \"TWD97_X\", \"TWD97_Y\", \"PointKey\"] + [\n",
    "    col for col in select_df.columns if col.startswith(\"N\")\n",
    "]\n",
    "\n",
    "output_df = select_df.loc[:, final_columns].reset_index(drop=True)\n",
    "output_df = output_df.rename({\"TWD97_X\":\"X_TWD97\", \"TWD97_Y\":\"Y_TWD97\"}, axis=1)\n",
    "\n",
    "output_df.to_pickle(\"1_Input_DataSets/Monthly_Electricity_CRFP.xz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9a8b71-ec69-4e95-8652-6b3a8dc9efe6",
   "metadata": {},
   "source": [
    "#### Monthly Vertical Cumulative Displacements"
   ]
  },
  {
   "cell_type": "raw",
   "id": "05f07ab9-6db4-49b4-bd7e-ab68b8af66be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:09:43.383454Z",
     "iopub.status.busy": "2025-03-30T04:09:43.383454Z",
     "iopub.status.idle": "2025-03-30T04:09:59.498003Z",
     "shell.execute_reply": "2025-03-30T04:09:59.497393Z",
     "shell.execute_reply.started": "2025-03-30T04:09:43.383454Z"
    },
    "scrolled": true
   },
   "source": [
    "fpath = r\"MonthlyResample_dU_timeseries.pkl\"\n",
    "df = pd.read_pickle(fpath, compression='zip')\n",
    "\n",
    "info_df = pd.read_pickle(r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\1_PrepareDatasets\\3_InSAR\\ASC_DESC_inc_azim.xz\")\n",
    "xtwd97_list = info_df['geometry'].apply(lambda ele: ele.x).values\n",
    "ytwd97_list = info_df['geometry'].apply(lambda ele: ele.y).values\n",
    "\n",
    "for coord, col in zip([ytwd97_list, xtwd97_list], [\"Y_TWD97\", \"X_TWD97\"]):\n",
    "    df.insert(loc=0, column=col, value=coord)\n",
    "\n",
    "df = df.reset_index(drop=False)\n",
    "df = df.rename({\"index\":\"PointKey\"}, axis=1)\n",
    "\n",
    "df.columns = [pd.to_datetime(col[1:]) if col.startswith(\"D\") else col for col in df.columns]\n",
    "\n",
    "df = df.set_index([\"PointKey\", \"X_TWD97\", \"Y_TWD97\"])\n",
    "\n",
    "df.columns = pd.to_datetime(df.columns)\n",
    "\n",
    "output_df = df.loc[:, \"2014\":\"2021\"].copy()\n",
    "output_df.columns = [\"N\"+col.strftime(\"%Y%m%d\") for col in output_df.columns]\n",
    "output_df = output_df.reset_index(drop=False)\n",
    "output_df.to_pickle(r\"1_Input_DataSets/Monthly_cumpdisp_dU_CRFP.xz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dc3594f-87ed-4f8b-a633-65d19242cca0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T04:09:59.500000Z",
     "iopub.status.busy": "2025-03-30T04:09:59.500000Z",
     "iopub.status.idle": "2025-03-30T04:12:48.142829Z",
     "shell.execute_reply": "2025-03-30T04:12:48.142829Z",
     "shell.execute_reply.started": "2025-03-30T04:09:59.500000Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441bfa8-339d-43b4-a39a-68cf4523e3ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
