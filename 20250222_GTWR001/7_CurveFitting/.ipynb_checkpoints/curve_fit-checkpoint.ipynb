{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce5f3a35-b584-4f05-9298-6e68c3fb9246",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T06:42:54.014890Z",
     "iopub.status.busy": "2025-06-25T06:42:54.014395Z",
     "iopub.status.idle": "2025-06-25T06:42:56.403621Z",
     "shell.execute_reply": "2025-06-25T06:42:56.403125Z",
     "shell.execute_reply.started": "2025-06-25T06:42:54.014395Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71b9542a-4772-454b-8347-4d2c881f2805",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-10T09:34:16.910361Z",
     "iopub.status.busy": "2025-06-10T09:34:16.909865Z",
     "iopub.status.idle": "2025-06-10T09:34:19.348201Z",
     "shell.execute_reply": "2025-06-10T09:34:19.347705Z",
     "shell.execute_reply.started": "2025-06-10T09:34:16.910361Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "# In scikit-gstat, the spherical model is available in the models submodule\n",
    "from skgstat.models import spherical\n",
    "\n",
    "# --- Step 1: Create Sample Experimental Variogram Data ---\n",
    "# Let's create some dummy data that looks like an experimental variogram.\n",
    "# x_data represents the distance lags (the center of our bins).\n",
    "# y_data represents the calculated semi-variance for each lag.\n",
    "x_data = np.array([15, 30, 45, 60, 75, 90, 105, 120, 135, 150, 165, 180])\n",
    "y_data = np.array([3.2, 4.8, 6.5, 8.1, 9.6, 10.5, 11.2, 11.8, 11.9, 12.1, 12.0, 12.3])\n",
    "\n",
    "# --- Step 2: Define Bounds and Initial Guess ---\n",
    "# As explained, providing bounds and an initial guess makes the fit more robust.\n",
    "# Let's define them manually here to illustrate the process.\n",
    "# Bounds are in the format: ([min_range, min_sill, min_nugget], [max_range, max_sill, max_nugget])\n",
    "bounds = ([0, 0, 0], [np.max(x_data), np.max(y_data), np.max(y_data)])\n",
    "\n",
    "# Initial guess for the parameters [range, sill, nugget]\n",
    "p0 = [np.median(x_data), np.median(y_data), 0]\n",
    "\n",
    "# --- Step 3: Run the Fitting Algorithm ---\n",
    "# We use curve_fit to find the best parameters for the spherical model.\n",
    "# The function needs:\n",
    "# 1. The model function to fit (spherical)\n",
    "# 2. The x data (distance lags)\n",
    "# 3. The y data (semi-variance)\n",
    "# 4. The initial guess (p0)\n",
    "# 5. The bounds\n",
    "# The 'cof' variable will hold the optimized coefficients: [range, sill, nugget]\n",
    "cof, cov = curve_fit(spherical, x_data, y_data, p0=p0, bounds=bounds)\n",
    "\n",
    "# --- Step 4: Display and Visualize the Results ---\n",
    "# Print the parameters found by the fitting process.\n",
    "print(f\"Fitted Range: {cof[0]:.2f}\")\n",
    "print(f\"Fitted Sill: {cof[1]:.2f}\")\n",
    "print(f\"Fitted Nugget: {cof[2]:.2f}\")\n",
    "\n",
    "\n",
    "# To plot the fitted model, we create a smooth line\n",
    "# Generate 100 x-points from 0 to the max distance for a smooth curve\n",
    "xi = np.linspace(0, x_data[-1], 100)\n",
    "# Calculate the y-points for the smooth curve using the fitted parameters\n",
    "yi = spherical(xi, *cof)\n",
    "\n",
    "# Plot the original experimental points and the fitted model\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_data, y_data, \"o\", color=\"red\", label=\"Experimental Points\")\n",
    "plt.plot(xi, yi, \"-\", color=\"blue\", label=\"Fitted Spherical Model\")\n",
    "plt.title(\"Fitting a Spherical Model to Experimental Data\")\n",
    "plt.xlabel(\"Distance Lag (h)\")\n",
    "plt.ylabel(\"Semi-variance (γ)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c1170d0-0492-40e2-9645-08917f8897f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-24T08:10:36.210651Z",
     "iopub.status.busy": "2025-06-24T08:10:36.210156Z",
     "iopub.status.idle": "2025-06-24T08:11:00.886596Z",
     "shell.execute_reply": "2025-06-24T08:11:00.886596Z",
     "shell.execute_reply.started": "2025-06-24T08:10:36.210651Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "import gstools as gs\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.optimize import (\n",
    "    curve_fit,\n",
    ")\n",
    "\n",
    "# skgstat's spherical model for fitting\n",
    "from skgstat.models import (\n",
    "    spherical,\n",
    ")\n",
    "\n",
    "# --- Part 1: Generate Data with the 'gstools' package ---\n",
    "\n",
    "# 1a: Define a model with known parameters in gstools\n",
    "ground_truth_model = gs.Spherical(dim=2, var=1.0, len_scale=20)\n",
    "\n",
    "# 1b: Create a synthetic random field and sample it\n",
    "srf = gs.SRF(ground_truth_model, seed=42)\n",
    "x = np.random.rand(500) * 100\n",
    "y = np.random.rand(500) * 100\n",
    "field_values = srf((x, y))\n",
    "\n",
    "# 1c: Calculate the experimental variogram using the CORRECT argument name: 'field'\n",
    "bin_center, exp_var = gs.vario_estimate(\n",
    "    pos=(x, y),\n",
    "    field=field_values,  # This argument is now correct.\n",
    "    bin_edges=np.linspace(0, 50, 20),\n",
    ")\n",
    "\n",
    "\n",
    "# --- Part 2: Fit the data using scikit-gstat's spherical model ---\n",
    "\n",
    "x_data = bin_center\n",
    "y_data = exp_var\n",
    "\n",
    "bounds = ([0, 0, 0], [np.max(x_data), np.max(y_data), np.max(y_data)])\n",
    "p0 = [np.median(x_data), np.median(y_data), 0]\n",
    "\n",
    "valid_points = ~np.isnan(y_data)\n",
    "cof, cov = curve_fit(\n",
    "    spherical, x_data[valid_points], y_data[valid_points], p0=p0, bounds=bounds\n",
    ")\n",
    "\n",
    "# --- Part 3: Visualize and Compare the Results ---\n",
    "\n",
    "print(\"--- Ground Truth Parameters (from gstools) ---\")\n",
    "print(f\"Original Range: 20.00\")\n",
    "print(f\"Original Sill: 1.00\")\n",
    "print(\"\\n--- Fitted Parameters (using scikit-gstat's method) ---\")\n",
    "print(f\"Fitted Range: {cof[0]:.2f}\")\n",
    "print(f\"Fitted Sill: {cof[1]:.2f}\")\n",
    "print(f\"Fitted Nugget: {cof[2]:.2f}\")\n",
    "\n",
    "# Plot the results\n",
    "xi = np.linspace(0, 50, 100)\n",
    "fig, ax = plt.subplots(figsize=(12, 7))\n",
    "ax.plot(\n",
    "    x_data,\n",
    "    y_data,\n",
    "    \"o\",\n",
    "    color=\"black\",\n",
    "    markersize=8,\n",
    "    label=\"Experimental Points (from gstools)\",\n",
    ")\n",
    "ground_truth_model.plot(ax=ax, x_max=50, label=\"Ground Truth Model (gstools)\")\n",
    "ax.plot(\n",
    "    xi,\n",
    "    spherical(xi, *cof),\n",
    "    \"--\",\n",
    "    color=\"red\",\n",
    "    linewidth=3,\n",
    "    label=\"Fitted Model (skgstat method)\",\n",
    ")\n",
    "ax.set_title(\"Validation: Fitting skgstat Model to gstools Data\")\n",
    "ax.set_xlabel(\"Distance Lag (h)\")\n",
    "ax.set_ylabel(\"Semi-variance (γ)\")\n",
    "ax.legend()\n",
    "ax.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "54923976-8248-4db8-a6d9-48c0edf97def",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T05:35:44.315655Z",
     "iopub.status.busy": "2025-06-25T05:35:44.315159Z",
     "iopub.status.idle": "2025-06-25T05:35:56.420506Z",
     "shell.execute_reply": "2025-06-25T05:35:56.420011Z",
     "shell.execute_reply.started": "2025-06-25T05:35:44.315655Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from scipy.optimize import curve_fit\n",
    "from skgstat.models import exponential\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "\n",
    "\n",
    "def fit_polynomial_trend(time_values, residual_values, poly_order=1, prediction_times=None):\n",
    "    \"\"\"Fit polynomial regression to residuals and predict future trends.\"\"\"\n",
    "    # Fit polynomial of specified order\n",
    "    coefficients = np.polyfit(time_values, residual_values, poly_order)\n",
    "    \n",
    "    # Use provided prediction times or default to training times\n",
    "    prediction_input = prediction_times if prediction_times is not None else time_values\n",
    "    \n",
    "    # Generate predictions using numpy's polynomial evaluation\n",
    "    trend_prediction = np.polyval(coefficients, prediction_input)\n",
    "    \n",
    "    # Create coefficient dictionary with descriptive names\n",
    "    coeff_names = ['intercept', 'slope', 'quadratic', 'cubic', 'quartic', 'quintic']\n",
    "    coeffs_dict = {\n",
    "        coeff_names[i]: coefficients[-(i+1)] \n",
    "        for i in range(min(len(coefficients), len(coeff_names)))\n",
    "    }\n",
    "    \n",
    "    return trend_prediction, coeffs_dict\n",
    "\n",
    "\n",
    "# Data loading and processing\n",
    "base_directory = r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\4_GTWR\\1_TestRun_101\\figure_spatial_coeffs\"\n",
    "\n",
    "# Find all coefficient folders (excluding LAYER folders)\n",
    "coefficient_folders = [\n",
    "    folder for folder in os.listdir(base_directory)\n",
    "    if \"LAYER\" not in folder and os.path.isdir(os.path.join(base_directory, folder))\n",
    "]\n",
    "\n",
    "# Process first folder only (remove [:1] to process all)\n",
    "for folder_name in tqdm(coefficient_folders[:]):\n",
    "    excel_files = glob(os.path.join(base_directory, folder_name, \"*.xlsx\"))\n",
    "    \n",
    "    # Process first Excel file only (remove [:1] to process all)\n",
    "    for excel_file_path in excel_files[:]:\n",
    "        # Load and prepare data\n",
    "        data_frame = pd.read_excel(excel_file_path, parse_dates=[0])\n",
    "        layer_name = os.path.basename(excel_file_path).split(\".\")[0]\n",
    "        \n",
    "        # Create prediction time range from data start to end of 2024\n",
    "        prediction_timerange = pd.date_range(\n",
    "            start=data_frame[\"time\"].iloc[0], \n",
    "            end=datetime(2024, 12, 31), \n",
    "            freq=\"1MS\"\n",
    "        )\n",
    "        \n",
    "        # Convert time to cumulative days for modeling\n",
    "        time_days = data_frame[\"time\"].diff().apply(lambda x: x.days).fillna(0).cumsum().values\n",
    "        prediction_days = pd.Series(prediction_timerange).diff().apply(lambda x: x.days).fillna(0).cumsum().values\n",
    "        \n",
    "        # Extract coefficient values for modeling\n",
    "        coefficient_values = data_frame[\"average_coeffs\"].values\n",
    "        data_frame = data_frame.set_index(\"time\")\n",
    "        \n",
    "        # Model fitting with exponential variogram\n",
    "        model_function = exponential\n",
    "        \n",
    "        # Set up fitting parameters and bounds\n",
    "        parameter_bounds = (\n",
    "            [np.min(time_days), np.min(coefficient_values), np.min(coefficient_values)],\n",
    "            [np.max(time_days), np.max(coefficient_values), np.max(coefficient_values)]\n",
    "        )\n",
    "        initial_guess = [np.median(time_days), np.median(coefficient_values), coefficient_values[0]]\n",
    "        \n",
    "        # Fit model on valid (non-NaN) data points\n",
    "        valid_data_mask = ~np.isnan(coefficient_values)\n",
    "        fitted_parameters, parameter_covariance = curve_fit(\n",
    "            model_function,\n",
    "            time_days[valid_data_mask],\n",
    "            coefficient_values[valid_data_mask],\n",
    "            p0=initial_guess,\n",
    "            bounds=parameter_bounds\n",
    "        )\n",
    "        \n",
    "        # Generate predictions and fit residual trend\n",
    "        model_predictions = model_function(time_days, *fitted_parameters)\n",
    "        residuals = coefficient_values - model_predictions\n",
    "        \n",
    "        # Generate exponential predictions for entire timerange\n",
    "        exponential_predictions = model_function(prediction_days, *fitted_parameters)\n",
    "        \n",
    "        # Fit polynomial trend to residuals (for future extrapolation only)\n",
    "        residual_trend_future, trend_parameters = fit_polynomial_trend(\n",
    "            time_values=time_days, \n",
    "            residual_values=residuals,\n",
    "            poly_order=1,  # Change this to 2, 3, etc. for higher order polynomials\n",
    "            prediction_times=prediction_days\n",
    "        )\n",
    "        \n",
    "        # Start with exponential predictions\n",
    "        final_predictions = exponential_predictions.copy()\n",
    "        \n",
    "        # Organize predictions with time index\n",
    "        prediction_dataframe = pd.DataFrame(\n",
    "            data={\"prediction\": final_predictions}, \n",
    "            index=prediction_timerange\n",
    "        )\n",
    "        \n",
    "        # Find last valid date and add residual trend only to future predictions\n",
    "        last_valid_date = data_frame.last_valid_index()\n",
    "        future_mask = prediction_dataframe.index > last_valid_date\n",
    "        \n",
    "        # Add residual trend only to future timestamps\n",
    "        prediction_dataframe.loc[future_mask, \"prediction\"] += residual_trend_future[future_mask]\n",
    "        \n",
    "        # Split predictions into historical and future periods\n",
    "        historical_predictions = prediction_dataframe.loc[:last_valid_date, :]\n",
    "        future_predictions = prediction_dataframe.loc[last_valid_date:, :]\n",
    "        \n",
    "        # Calculate model performance metrics\n",
    "        rmse_score = root_mean_squared_error(coefficient_values, model_predictions)\n",
    "        mae_score = mean_absolute_error(coefficient_values, model_predictions)\n",
    "        r_squared_score = r2_score(coefficient_values, model_predictions)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(11.7, 8.3))\n",
    "        \n",
    "        # Plot original coefficient data\n",
    "        ax.plot(\n",
    "            data_frame.index,\n",
    "            coefficient_values,\n",
    "            \"o\",\n",
    "            linestyle=\"--\",\n",
    "            color=\"black\",\n",
    "            markersize=8,\n",
    "            label=f\"Coefficients - {layer_name.capitalize()}\"\n",
    "        )\n",
    "        \n",
    "        # Plot fitted model (historical period)\n",
    "        ax.plot(\n",
    "            historical_predictions,\n",
    "            \"--\",\n",
    "            color=\"red\",\n",
    "            linewidth=3,\n",
    "            label=f\"Fitted {model_function.__name__.capitalize()} Model\"\n",
    "        )\n",
    "        \n",
    "        # Plot future predictions\n",
    "        ax.plot(\n",
    "            future_predictions,\n",
    "            \"--\",\n",
    "            color=\"blue\",\n",
    "            linewidth=3,\n",
    "            label=\"Future Predictions\"\n",
    "        )\n",
    "        \n",
    "        # Configure plot appearance\n",
    "        visualize.configure_axis(\n",
    "            ax=ax,\n",
    "            ylabel=\"Coefficients\",\n",
    "            tick_direction=\"out\",\n",
    "            hide_spines=[\"top\", \"right\"]\n",
    "        )\n",
    "        visualize.configure_legend(ax=ax)\n",
    "        \n",
    "        # Save figure (uncomment to enable saving)\n",
    "        output_filename = f\"{folder_name}_{layer_name.capitalize().replace(' ', '_')}.png\"\n",
    "        output_path = os.path.join(\"curvefit_coeffs\", output_filename)\n",
    "        visualize.save_figure(fig, savepath=output_path)\n",
    "        \n",
    "        # plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d5c1b-4735-4a5d-9b8b-bbbdc48a90de",
   "metadata": {},
   "source": [
    "#### Coefficients"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb548e0d-1b4c-44d2-a738-904967043d6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T05:39:14.433179Z",
     "iopub.status.busy": "2025-06-25T05:39:14.432683Z",
     "iopub.status.idle": "2025-06-25T05:39:26.744864Z",
     "shell.execute_reply": "2025-06-25T05:39:26.744368Z",
     "shell.execute_reply.started": "2025-06-25T05:39:14.433179Z"
    },
    "scrolled": true
   },
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from scipy.optimize import curve_fit\n",
    "from skgstat.models import exponential\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Data loading and processing\n",
    "base_directory = r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\4_GTWR\\1_TestRun_101\\figure_spatial_coeffs\"\n",
    "\n",
    "# Find all coefficient folders (excluding LAYER folders)\n",
    "coefficient_folders = [\n",
    "    folder for folder in os.listdir(base_directory)\n",
    "    if \"LAYER\" not in folder and os.path.isdir(os.path.join(base_directory, folder))\n",
    "]\n",
    "\n",
    "# Process first folder only (remove [:1] to process all)\n",
    "for folder_name in tqdm(coefficient_folders[:]):\n",
    "    excel_files = glob(os.path.join(base_directory, folder_name, \"*.xlsx\"))\n",
    "    \n",
    "    # Process first Excel file only (remove [:1] to process all)\n",
    "    for excel_file_path in excel_files[:]:\n",
    "        # Load and prepare data\n",
    "        data_frame = pd.read_excel(excel_file_path, parse_dates=[0])\n",
    "        layer_name = os.path.basename(excel_file_path).split(\".\")[0]\n",
    "        \n",
    "        # Create prediction time range from data start to end of 2024\n",
    "        prediction_timerange = pd.date_range(\n",
    "            start=data_frame[\"time\"].iloc[0], \n",
    "            end=datetime(2024, 12, 31), \n",
    "            freq=\"1MS\"\n",
    "        )\n",
    "        \n",
    "        # Convert time to cumulative days for modeling\n",
    "        time_days = data_frame[\"time\"].diff().apply(lambda x: x.days).fillna(0).cumsum().values\n",
    "        prediction_days = pd.Series(prediction_timerange).diff().apply(lambda x: x.days).fillna(0).cumsum().values\n",
    "        \n",
    "        # Extract coefficient values for modeling\n",
    "        coefficient_values = data_frame[\"average_coeffs\"].values\n",
    "        data_frame = data_frame.set_index(\"time\")\n",
    "        \n",
    "        # Model fitting with exponential variogram\n",
    "        model_function = exponential\n",
    "        \n",
    "        # Set up fitting parameters and bounds\n",
    "        parameter_bounds = (\n",
    "            [np.min(time_days), np.min(coefficient_values), np.min(coefficient_values)],\n",
    "            [np.max(time_days), np.max(coefficient_values), np.max(coefficient_values)]\n",
    "        )\n",
    "        initial_guess = [np.median(time_days), np.median(coefficient_values), coefficient_values[0]]\n",
    "        \n",
    "        # Fit model on valid (non-NaN) data points\n",
    "        valid_data_mask = ~np.isnan(coefficient_values)\n",
    "        fitted_parameters, parameter_covariance = curve_fit(\n",
    "            model_function,\n",
    "            time_days[valid_data_mask],\n",
    "            coefficient_values[valid_data_mask],\n",
    "            p0=initial_guess,\n",
    "            bounds=parameter_bounds\n",
    "        )\n",
    "        \n",
    "        # Generate pure exponential predictions for entire timerange\n",
    "        exponential_predictions = model_function(prediction_days, *fitted_parameters)\n",
    "        \n",
    "        # Organize predictions with time index\n",
    "        prediction_dataframe = pd.DataFrame(\n",
    "            data={\"prediction\": exponential_predictions}, \n",
    "            index=prediction_timerange\n",
    "        )\n",
    "        \n",
    "        # Split predictions into historical and future periods\n",
    "        last_valid_date = data_frame.last_valid_index()\n",
    "        historical_predictions = prediction_dataframe.loc[:last_valid_date, :]\n",
    "        future_predictions = prediction_dataframe.loc[last_valid_date:, :]\n",
    "        \n",
    "        # Calculate model performance metrics on historical data\n",
    "        historical_model_predictions = model_function(time_days, *fitted_parameters)\n",
    "        rmse_score = root_mean_squared_error(coefficient_values, historical_model_predictions)\n",
    "        mae_score = mean_absolute_error(coefficient_values, historical_model_predictions)\n",
    "        r_squared_score = r2_score(coefficient_values, historical_model_predictions)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(11.7, 8.3))\n",
    "        \n",
    "        # Plot original coefficient data\n",
    "        ax.plot(\n",
    "            data_frame.index,\n",
    "            coefficient_values,\n",
    "            \"o\",\n",
    "            linestyle=\"--\",\n",
    "            color=\"black\",\n",
    "            markersize=8,\n",
    "            label=f\"Coefficients - {layer_name.capitalize()}\"\n",
    "        )\n",
    "        \n",
    "        # Plot fitted exponential model (historical period)\n",
    "        ax.plot(\n",
    "            historical_predictions,\n",
    "            \"--\",\n",
    "            color=\"red\",\n",
    "            linewidth=3,\n",
    "            label=f\"Fitted {model_function.__name__.capitalize()} Model\"\n",
    "        )\n",
    "        \n",
    "        # Plot future exponential predictions\n",
    "        ax.plot(\n",
    "            future_predictions,\n",
    "            \"--\",\n",
    "            color=\"blue\",\n",
    "            linewidth=3,\n",
    "            label=\"Future Exponential Predictions\"\n",
    "        )\n",
    "        \n",
    "        # Configure plot appearance\n",
    "        visualize.configure_axis(\n",
    "            ax=ax,\n",
    "            ylabel=\"Coefficients\",\n",
    "            tick_direction=\"out\",\n",
    "            hide_spines=[\"top\", \"right\"]\n",
    "        )\n",
    "        visualize.configure_legend(ax=ax)\n",
    "        \n",
    "        # Save figure (uncomment to enable saving)\n",
    "        output_filename = f\"{folder_name}_{layer_name.capitalize().replace(' ', '_')}.png\"\n",
    "        output_path = os.path.join(\"curvefit_coeffs\", output_filename)\n",
    "        visualize.save_figure(fig, savepath=output_path)\n",
    "        \n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e94f7a0-ba76-4d86-90d2-9c7c8161ed92",
   "metadata": {},
   "source": [
    "#### Intercepts"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e3203f2b-d08e-471f-81b7-6cd3bd03fcac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-25T05:48:45.003007Z",
     "iopub.status.busy": "2025-06-25T05:48:45.002511Z",
     "iopub.status.idle": "2025-06-25T05:48:45.202399Z",
     "shell.execute_reply": "2025-06-25T05:48:45.202399Z",
     "shell.execute_reply.started": "2025-06-25T05:48:45.003007Z"
    }
   },
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from scipy.optimize import curve_fit\n",
    "from skgstat.models import exponential\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, root_mean_squared_error\n",
    "\n",
    "# Data loading and processing\n",
    "base_directory = r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\4_GTWR\\1_TestRun_101\\figure_spatial_coeffs\"\n",
    "\n",
    "# Find all coefficient folders (excluding LAYER folders)\n",
    "coefficient_folders = [\n",
    "    folder for folder in os.listdir(base_directory)\n",
    "    if \"LAYER\" not in folder and os.path.isdir(os.path.join(base_directory, folder))\n",
    "]\n",
    "\n",
    "# Process first folder only (remove [:1] to process all)\n",
    "for folder_name in tqdm(coefficient_folders[:1]):\n",
    "    excel_files = glob(os.path.join(base_directory, folder_name, \"*.xlsx\"))\n",
    "    \n",
    "    # Process first Excel file only (remove [:1] to process all)\n",
    "    for excel_file_path in excel_files[:1]:\n",
    "        # Load and prepare data\n",
    "        data_frame = pd.read_excel(excel_file_path, parse_dates=[0])\n",
    "        layer_name = os.path.basename(excel_file_path).split(\".\")[0]\n",
    "        \n",
    "        # Create prediction time range from data start to end of 2024\n",
    "        prediction_timerange = pd.date_range(\n",
    "            start=data_frame[\"time\"].iloc[0], \n",
    "            end=datetime(2024, 12, 31), \n",
    "            freq=\"1MS\"\n",
    "        )\n",
    "        \n",
    "        # Convert time to cumulative days for modeling\n",
    "        time_days = data_frame[\"time\"].diff().apply(lambda x: x.days).fillna(0).cumsum().values\n",
    "        prediction_days = pd.Series(prediction_timerange).diff().apply(lambda x: x.days).fillna(0).cumsum().values\n",
    "        \n",
    "        # Extract coefficient values for modeling\n",
    "        coefficient_values = data_frame[\"average_intercepts\"].values\n",
    "        data_frame = data_frame.set_index(\"time\")\n",
    "        \n",
    "        # Model fitting with exponential variogram\n",
    "        model_function = exponential\n",
    "        \n",
    "        # Set up fitting parameters and bounds\n",
    "        parameter_bounds = (\n",
    "            [np.min(time_days), np.min(coefficient_values), np.min(coefficient_values)],\n",
    "            [np.max(time_days), np.max(coefficient_values), np.max(coefficient_values)]\n",
    "        )\n",
    "        initial_guess = [np.median(time_days), np.median(coefficient_values), coefficient_values[0]]\n",
    "        \n",
    "        # Fit model on valid (non-NaN) data points\n",
    "        valid_data_mask = ~np.isnan(coefficient_values)\n",
    "        fitted_parameters, parameter_covariance = curve_fit(\n",
    "            model_function,\n",
    "            time_days[valid_data_mask],\n",
    "            coefficient_values[valid_data_mask],\n",
    "            p0=initial_guess,\n",
    "            bounds=parameter_bounds\n",
    "        )\n",
    "        \n",
    "        # Generate pure exponential predictions for entire timerange\n",
    "        exponential_predictions = model_function(prediction_days, *fitted_parameters)\n",
    "        \n",
    "        # Organize predictions with time index\n",
    "        prediction_dataframe = pd.DataFrame(\n",
    "            data={\"prediction\": exponential_predictions}, \n",
    "            index=prediction_timerange\n",
    "        )\n",
    "        \n",
    "        # Split predictions into historical and future periods\n",
    "        last_valid_date = data_frame.last_valid_index()\n",
    "        historical_predictions = prediction_dataframe.loc[:last_valid_date, :]\n",
    "        future_predictions = prediction_dataframe.loc[last_valid_date:, :]\n",
    "        \n",
    "        # Calculate model performance metrics on historical data\n",
    "        historical_model_predictions = model_function(time_days, *fitted_parameters)\n",
    "        rmse_score = root_mean_squared_error(coefficient_values, historical_model_predictions)\n",
    "        mae_score = mean_absolute_error(coefficient_values, historical_model_predictions)\n",
    "        r_squared_score = r2_score(coefficient_values, historical_model_predictions)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, ax = plt.subplots(figsize=(11.7, 8.3))\n",
    "        \n",
    "        # Plot original coefficient data\n",
    "        ax.plot(\n",
    "            data_frame.index,\n",
    "            coefficient_values,\n",
    "            \"o\",\n",
    "            linestyle=\"--\",\n",
    "            color=\"black\",\n",
    "            markersize=8,\n",
    "            label=f\"Intercept - {layer_name.capitalize()}\"\n",
    "        )\n",
    "        \n",
    "        # Plot fitted exponential model (historical period)\n",
    "        ax.plot(\n",
    "            historical_predictions,\n",
    "            \"--\",\n",
    "            color=\"red\",\n",
    "            linewidth=3,\n",
    "            label=f\"Fitted {model_function.__name__.capitalize()} Model\"\n",
    "        )\n",
    "        \n",
    "        # Plot future exponential predictions\n",
    "        ax.plot(\n",
    "            future_predictions,\n",
    "            \"--\",\n",
    "            color=\"blue\",\n",
    "            linewidth=3,\n",
    "            label=\"Future Exponential Predictions\"\n",
    "        )\n",
    "        \n",
    "        # Configure plot appearance\n",
    "        visualize.configure_axis(\n",
    "            ax=ax,\n",
    "            ylabel=\"Intercept\",\n",
    "            tick_direction=\"out\",\n",
    "            hide_spines=[\"top\", \"right\"]\n",
    "        )\n",
    "        visualize.configure_legend(ax=ax)\n",
    "        \n",
    "        # Save figure (uncomment to enable saving)\n",
    "        # output_filename = f\"{folder_name}_{layer_name.capitalize().replace(' ', '_')}.png\"\n",
    "        # output_path = os.path.join(\"curvefit_intercepts\", output_filename)\n",
    "        # visualize.save_figure(fig, savepath=output_path)\n",
    "        \n",
    "        # plt.close()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621d6f77-223e-4102-bfdf-394101a74a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "022b93492e294f39befa1e3cc6394505": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_c40e53c8f5d74a028c7461855f402f31",
       "style": "IPY_MODEL_1a9faf8e48ff49e484b0cc2f86584952",
       "value": "100%"
      }
     },
     "1a9faf8e48ff49e484b0cc2f86584952": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1bc0914741d14ba686ae8a1722ce7c2b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_022b93492e294f39befa1e3cc6394505",
        "IPY_MODEL_c3a425fa397f4bba838a3ff56aa25025",
        "IPY_MODEL_1de310cd3c7f414386ebeb33322394e1"
       ],
       "layout": "IPY_MODEL_9035fb80edda445dbfa474a58652c2b8"
      }
     },
     "1de310cd3c7f414386ebeb33322394e1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_b866e651820b4eaeb1ae7d8c227b3285",
       "style": "IPY_MODEL_1e1861fd31074ab2bc36cd2565b9d21e",
       "value": " 1/1 [00:00&lt;00:00,  1.06it/s]"
      }
     },
     "1e1861fd31074ab2bc36cd2565b9d21e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "61ac046f2f2b46908a5b506262d624c4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "9035fb80edda445dbfa474a58652c2b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "b866e651820b4eaeb1ae7d8c227b3285": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c3a425fa397f4bba838a3ff56aa25025": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_d9b3dd45eafe43d886de5017aaf42c6a",
       "max": 1,
       "style": "IPY_MODEL_61ac046f2f2b46908a5b506262d624c4",
       "value": 1
      }
     },
     "c40e53c8f5d74a028c7461855f402f31": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d9b3dd45eafe43d886de5017aaf42c6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
