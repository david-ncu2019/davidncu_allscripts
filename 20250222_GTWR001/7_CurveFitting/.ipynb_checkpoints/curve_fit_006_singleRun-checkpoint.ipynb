{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f08e89e-208e-43bf-9028-129ff208c666",
   "metadata": {},
   "source": [
    "### Configuration & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1790b0e1-e7b9-4247-a195-e827345d1f2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:00:31.757950Z",
     "iopub.status.busy": "2025-07-08T09:00:31.757454Z",
     "iopub.status.idle": "2025-07-08T09:00:33.679947Z",
     "shell.execute_reply": "2025-07-08T09:00:33.679947Z",
     "shell.execute_reply.started": "2025-07-08T09:00:31.757950Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Configuration and libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "# In[1]:\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.optimize import curve_fit\n",
    "from skgstat.models import exponential\n",
    "\n",
    "# --- Plotting Style ---\n",
    "plt.style.use(\"seaborn-v0_8-talk\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 5)\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"A centralized class for all user-configurable parameters.\"\"\"\n",
    "\n",
    "    # --- 1. SET YOUR FILE PATHS HERE ---\n",
    "    SHAPEFILE_PATH = Path(\n",
    "        r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\2_KrigingInterpolation\\points_fld\\mlcw_twd97.shp\"\n",
    "    )\n",
    "    MODEL_FOLDER = Path(\n",
    "        r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\5_GTWR_Prediction\"\n",
    "    )\n",
    "\n",
    "    # --- 2. DEFINE YOUR ANALYSIS SETTINGS ---\n",
    "    KERNEL_NAME: str = \"bisquare\"\n",
    "    START_DATE = pd.Timestamp(year=2016, month=5, day=1)\n",
    "\n",
    "    # --- 3. CHOOSE WHICH POINT TO ANALYZE ---\n",
    "    # Change these indices to analyze a different file or a different point\n",
    "    SELECT_FILE_INDEX: int = 0\n",
    "    SELECT_POINT_INDEX: int = 7\n",
    "\n",
    "\n",
    "print(\"✅ Configuration and libraries loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f101d741-647e-447a-a793-dd6a843dad6e",
   "metadata": {},
   "source": [
    "### Core Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f40e038e-2146-4fbf-bd2a-10198bc41857",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:00:33.681434Z",
     "iopub.status.busy": "2025-07-08T09:00:33.680939Z",
     "iopub.status.idle": "2025-07-08T09:00:33.689866Z",
     "shell.execute_reply": "2025-07-08T09:00:33.689866Z",
     "shell.execute_reply.started": "2025-07-08T09:00:33.681434Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# In[2]:\n",
    "def load_and_prepare_data(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Loads CSV data, creates a 'PointKey', and sets it as the index.\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    pointkey_arr = [f\"X{int(x*1000)}Y{int(y*1000)}\" for x, y in zip(df[\"X_TWD97\"], df[\"Y_TWD97\"])]\n",
    "    df.insert(loc=0, column=\"PointKey\", value=pointkey_arr)\n",
    "    df.set_index(\"PointKey\", inplace=True)\n",
    "    return df\n",
    "\n",
    "def get_time_series_for_point(df: pd.DataFrame, point_key: str, start_date: pd.Timestamp) -> Tuple[pd.Series, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts the time series for a single point, prepares it for analysis,\n",
    "    and calculates the cumulative time in days.\n",
    "    \"\"\"\n",
    "    df_point = df.loc[point_key].copy()\n",
    "    \n",
    "    time_arr = [start_date + relativedelta(months=int(t)) for t in df_point[\"time_stamp\"]]\n",
    "    df_point[\"datetime\"] = time_arr\n",
    "    df_point = df_point.set_index(\"datetime\").sort_index()\n",
    "    \n",
    "    coefficient_series = df_point[\"CUMDISP\"].asfreq(\"MS\")\n",
    "    \n",
    "    # **This calculation now exactly matches your original logic.**\n",
    "    time_days = pd.Series(coefficient_series.index).diff().apply(lambda x: x.days).fillna(0).cumsum().values\n",
    "    \n",
    "    return coefficient_series, time_days\n",
    "\n",
    "def fit_exponential_model(series: pd.Series, time_days: np.ndarray) -> Tuple[np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Fits an exponential variogram model using the original logic for bounds and p0.\n",
    "    \"\"\"\n",
    "    valid_mask = ~np.isnan(series)\n",
    "    y_data = series[valid_mask]\n",
    "    x_data = time_days[valid_mask]\n",
    "\n",
    "    if len(y_data) < 3:\n",
    "        raise ValueError(\"Not enough valid data points to fit the model.\")\n",
    "\n",
    "    # **Set up bounds and initial guess exactly as in the original script.**\n",
    "    parameter_bounds = (\n",
    "        [np.min(x_data), np.min(y_data), np.min(y_data)],\n",
    "        [np.max(x_data), np.max(y_data), np.max(y_data)],\n",
    "    )\n",
    "    initial_guess = [np.median(x_data), np.median(y_data), y_data.iloc[0]]\n",
    "\n",
    "    params, _ = curve_fit(\n",
    "        exponential,\n",
    "        x_data,\n",
    "        y_data,\n",
    "        p0=initial_guess,\n",
    "        bounds=parameter_bounds,\n",
    "    )\n",
    "    \n",
    "    predictions = pd.Series(data=exponential(time_days, *params), index=series.index)\n",
    "    return params, predictions\n",
    "\n",
    "def plot_data(original_series: pd.Series, predicted_series: pd.Series, title: str):\n",
    "    \"\"\"Generates a plot of the original vs. fitted data.\"\"\"\n",
    "    plt.figure()\n",
    "    original_series.plot(marker='o', linestyle='-', label='Original Data', markerfacecolor='none', color='gray', alpha=0.8)\n",
    "    predicted_series.plot(label='Fitted Exponential Model', color='red', linestyle='--')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Cumulative Displacement (CUMDISP)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"✅ Helper functions defined (with corrected fitting logic).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ad935d-bc12-44e3-adbf-5890c995892e",
   "metadata": {},
   "source": [
    "#### Step 1 - Find and Select Data File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50e55dc2-ffb3-4df2-9fcb-72b12d6293be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:00:33.691354Z",
     "iopub.status.busy": "2025-07-08T09:00:33.690858Z",
     "iopub.status.idle": "2025-07-08T09:00:33.878346Z",
     "shell.execute_reply": "2025-07-08T09:00:33.877849Z",
     "shell.execute_reply.started": "2025-07-08T09:00:33.690858Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 files. Analyzing:\n",
      "-> gtwr_Layer_1_kernel-bisquare_lambda-0d006_bw-23_coefficients.csv\n"
     ]
    }
   ],
   "source": [
    "# In[3]:\n",
    "# Find all files matching the kernel name in the model folder\n",
    "output_files = sorted(\n",
    "    list(Config.MODEL_FOLDER.glob(f\"*{Config.KERNEL_NAME}*.csv\"))\n",
    ")\n",
    "if not output_files:\n",
    "    raise FileNotFoundError(\n",
    "        f\"No files with kernel '{Config.KERNEL_NAME}' found in {Config.MODEL_FOLDER}\"\n",
    "    )\n",
    "\n",
    "# Select the file based on the index set in the Config\n",
    "selected_file = output_files[Config.SELECT_FILE_INDEX]\n",
    "\n",
    "print(f\"Found {len(output_files)} files. Analyzing:\")\n",
    "print(f\"-> {selected_file.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa25794-2a06-41bb-ace8-c922e3b507cb",
   "metadata": {},
   "source": [
    "#### Step 2 - Load Data and Analyze a Single Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "167ca931-7532-4847-b3aa-5fe8a4f43d66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:00:33.878843Z",
     "iopub.status.busy": "2025-07-08T09:00:33.878843Z",
     "iopub.status.idle": "2025-07-08T09:00:34.033098Z",
     "shell.execute_reply": "2025-07-08T09:00:34.033098Z",
     "shell.execute_reply.started": "2025-07-08T09:00:33.878843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Station: 'XIGANG' (PointKey: X177634127Y2639733171)\n",
      "\n",
      "Original Data Series (first 5 rows):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "datetime\n",
       "2016-06-01   -0.073482\n",
       "2016-07-01   -0.005287\n",
       "2016-08-01   -0.003389\n",
       "2016-09-01   -0.003002\n",
       "2016-10-01   -0.006207\n",
       "Freq: MS, Name: CUMDISP, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# In[4]:\n",
    "# Load the main data and the location data (shapefile)\n",
    "df_processed = load_and_prepare_data(selected_file)\n",
    "mlcw_gdf = gpd.read_file(Config.SHAPEFILE_PATH)\n",
    "\n",
    "# Select the point to analyze\n",
    "unique_point_keys = sorted(df_processed.index.unique())\n",
    "selected_point_key = unique_point_keys[Config.SELECT_POINT_INDEX]\n",
    "station_name = mlcw_gdf.query(\"PointKey == @selected_point_key\").STATION.values[\n",
    "    0\n",
    "]\n",
    "\n",
    "print(f\"Analyzing Station: '{station_name}' (PointKey: {selected_point_key})\")\n",
    "\n",
    "# Prepare the data series for this specific point\n",
    "coefficient_series = prepare_series_for_point(\n",
    "    df_processed, selected_point_key, Config.START_DATE\n",
    ")\n",
    "\n",
    "print(\"\\nOriginal Data Series (first 5 rows):\")\n",
    "display(coefficient_series.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3cbec-0708-40d1-aa27-09735b15a0cf",
   "metadata": {},
   "source": [
    "####  Step 3 - Fit Model and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f2da03-a15e-4fa8-ae4f-23bbbf71cd2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-08T09:00:34.034090Z",
     "iopub.status.busy": "2025-07-08T09:00:34.033594Z",
     "iopub.status.idle": "2025-07-08T09:00:34.179418Z",
     "shell.execute_reply": "2025-07-08T09:00:34.178922Z",
     "shell.execute_reply.started": "2025-07-08T09:00:34.034090Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Could not fit model. Initial guess is outside of provided bounds\n"
     ]
    }
   ],
   "source": [
    "# In[5]:\n",
    "try:\n",
    "    # Fit the model using the series and the separately calculated time_days\n",
    "    fitted_params, predicted_values = fit_exponential_model(coefficient_series, time_days)\n",
    "\n",
    "    print(\"--- Model Fit Results (Original Logic) ---\")\n",
    "    print(f\"Fitted Parameters (Range, Sill, Nugget): {np.round(fitted_params, 4)}\")\n",
    "    \n",
    "    plot_title = f\"Exponential Model Fit for Station: {station_name}\"\n",
    "    plot_data(coefficient_series, predicted_values, plot_title)\n",
    "\n",
    "except ValueError as e:\n",
    "    print(f\"Error: Could not fit model. {e}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error: Fitting failed to converge. {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a11afbd-28eb-4307-80a1-558e07e4ae16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
