{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22b3115b-f822-4ac7-870c-679308435e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "InSAR Time Series Analysis with Predictive Model Integration - Class-based Implementation\n",
    "\"\"\"\n",
    "\n",
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "\n",
    "\n",
    "class InSARProcessor:\n",
    "    \"\"\"\n",
    "    Class for processing InSAR measurements with predictive model integration.\n",
    "    Maintains state variables like current_layer for easy access in Jupyter notebooks.\n",
    "    \"\"\"\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    def __init__(self, excel_file, top_folder):\n",
    "        \"\"\"\n",
    "        Initialize processor with data sources.\n",
    "\n",
    "        Args:\n",
    "            excel_file: Path to InSAR measurement Excel file\n",
    "            top_folder: Directory containing HDF5 model outputs\n",
    "        \"\"\"\n",
    "        # Load and prepare InSAR data\n",
    "        self.insar_df = pd.read_excel(excel_file)\n",
    "        self.insar_df[\"time\"] = pd.to_datetime(self.insar_df[\"time\"])\n",
    "        self.insar_df = self.insar_df.set_index(\"time\")\n",
    "\n",
    "        # Get unique identifiers\n",
    "        self.unique_pointkeys = sorted(self.insar_df[\"PointKey\"].unique())\n",
    "\n",
    "        # Discover model files\n",
    "        self.coeff_files = glob(\n",
    "            os.path.join(top_folder, \"*coeff*\", \"L*\", \"*h5\")\n",
    "        )\n",
    "        self.intercept_files = glob(\n",
    "            os.path.join(top_folder, \"*intercept*\", \"L*\", \"*h5\")\n",
    "        )\n",
    "        self.file_pairs = list(zip(self.coeff_files, self.intercept_files))\n",
    "\n",
    "        # State variables\n",
    "        self.current_layer = None\n",
    "        self.current_coeff_data = None\n",
    "        self.current_intercept_data = None\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    @staticmethod\n",
    "    def dict_to_table(data_dict, pointkey, quantity=\"future_predictions\"):\n",
    "        \"\"\"Convert HDF5 dictionary data to DataFrame.\"\"\"\n",
    "        time_arr = pd.to_datetime(\n",
    "            [x.decode(\"utf-8\") for x in data_dict[pointkey][quantity][\"index\"]]\n",
    "        )\n",
    "        values_arr = data_dict[pointkey][quantity][\"values\"]\n",
    "        return pd.DataFrame(data=values_arr, index=time_arr, columns=[quantity])\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    def load_layer(self, layer_idx):\n",
    "        \"\"\"\n",
    "        Load HDF5 data for specific layer and update state.\n",
    "\n",
    "        Args:\n",
    "            layer_idx: Index of layer to load\n",
    "        \"\"\"\n",
    "        coeff_file, intercept_file = self.file_pairs[layer_idx]\n",
    "        self.current_layer = os.path.dirname(coeff_file).split(\"\\\\\")[-1]\n",
    "\n",
    "        self.current_coeff_data, _ = gwatertools.h5pytools.open_HDF5(coeff_file)\n",
    "        self.current_intercept_data, _ = gwatertools.h5pytools.open_HDF5(\n",
    "            intercept_file\n",
    "        )\n",
    "\n",
    "        print(f\"Loaded layer: {self.current_layer}\")\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    def process_pointkey(self, pointkey, quantity=\"future_predictions\"):\n",
    "        \"\"\"\n",
    "        Process single pointkey with currently loaded layer.\n",
    "\n",
    "        Args:\n",
    "            pointkey: Point identifier to process\n",
    "            quantity: Data quantity to extract\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Merged observational and predicted data\n",
    "        \"\"\"\n",
    "        if self.current_coeff_data is None:\n",
    "            raise ValueError(\"No layer loaded. Call load_layer() first.\")\n",
    "\n",
    "        # Get observational data\n",
    "        insar_data = self.insar_df.query(\"PointKey==@pointkey\").copy()\n",
    "\n",
    "        # Extract predictions\n",
    "        pred_coeffs = self.dict_to_table(\n",
    "            self.current_coeff_data, pointkey, quantity\n",
    "        )\n",
    "        pred_intercept = self.dict_to_table(\n",
    "            self.current_intercept_data, pointkey, quantity\n",
    "        )\n",
    "\n",
    "        # Merge with observations\n",
    "        insar_data[\"pred_coeffs\"] = insar_data.index.map(pred_coeffs[quantity])\n",
    "        insar_data[\"pred_intercept\"] = insar_data.index.map(\n",
    "            pred_intercept[quantity]\n",
    "        )\n",
    "\n",
    "        # Add metadata\n",
    "        insar_data = insar_data.dropna(how=\"any\")\n",
    "        insar_data[\"layer\"] = self.current_layer\n",
    "        insar_data = insar_data.reset_index(drop=False)\n",
    "\n",
    "        return insar_data\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    def process_layer_all_points(self, layer_idx):\n",
    "        \"\"\"Process all pointkeys for specific layer.\"\"\"\n",
    "        self.load_layer(layer_idx)\n",
    "        results = []\n",
    "\n",
    "        for pointkey in self.unique_pointkeys:\n",
    "            try:\n",
    "                result = self.process_pointkey(pointkey)\n",
    "                results.append(result)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        return (\n",
    "            pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    def process_point_all_layers(self, pointkey):\n",
    "        \"\"\"Process all layers for specific pointkey.\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for i in range(len(self.file_pairs)):\n",
    "            self.load_layer(i)\n",
    "            try:\n",
    "                result = self.process_pointkey(pointkey)\n",
    "                results.append(result)\n",
    "            except KeyError:\n",
    "                continue\n",
    "\n",
    "        return (\n",
    "            pd.concat(results, ignore_index=True) if results else pd.DataFrame()\n",
    "        )\n",
    "\n",
    "    # - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    def get_info(self):\n",
    "        \"\"\"Display current processor state.\"\"\"\n",
    "        print(f\"Loaded pointkeys: {len(self.unique_pointkeys)}\")\n",
    "        print(f\"Available layers: {len(self.file_pairs)}\")\n",
    "        print(f\"Current layer: {self.current_layer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7cde4c4-0bff-43ee-a842-a2fa067e8bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layer: Layer_1\n",
      "Loaded layer: Layer_2\n",
      "Loaded layer: Layer_3\n",
      "Loaded layer: Layer_4\n"
     ]
    }
   ],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "# USAGE IN JUPYTER\n",
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
    "\n",
    "# Initialize processor\n",
    "processor = InSARProcessor(\n",
    "    excel_file=\"20250714_GTWR_InSAR_2016_to_2024.xlsx\",\n",
    "    top_folder=r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\7_CurveFitting\\curvefit_scripts\\test003\",\n",
    ")\n",
    "\n",
    "# Batch processing examples\n",
    "for layer_index in range(4):\n",
    "    output_table = processor.process_layer_all_points(layer_index)\n",
    "\n",
    "    output_table = output_table.sort_values(by=[\"STATION\", \"monthly\"])\n",
    "\n",
    "    output_table[\"pred_MLCW\"] = (\n",
    "        output_table[\"InSAR_CUMDISP\"] * output_table[\"pred_coeffs\"]\n",
    "        + output_table[\"pred_intercept\"]\n",
    "    )\n",
    "\n",
    "    current_layer = processor.current_layer\n",
    "\n",
    "    today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "    output_table.to_csv(\n",
    "        f\"{today_string}_Future_InSAR_and_Predicted_Coeffs_{current_layer}.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
