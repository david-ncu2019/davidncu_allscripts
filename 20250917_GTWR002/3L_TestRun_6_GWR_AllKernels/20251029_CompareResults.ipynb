{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf32f3aa-f0f7-4e48-b47d-ed3286075433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04cc257f-4fcd-4ecb-9e47-e55391069519",
   "metadata": {},
   "source": [
    "layer = \"Layer_4\"\n",
    "files = glob(os.path.join(f\"GWR_Output_{layer}\", \"*combined*.csv\"))\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9f3659-3925-49f3-84ba-9f1231b1e9fb",
   "metadata": {},
   "source": [
    "#### Find best kernel for each station"
   ]
  },
  {
   "cell_type": "raw",
   "id": "64ae9572-e023-47c5-a543-8d524ccba806",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "df = pd.read_csv(files[0])\n",
    "unique_station = df[\"input_STATION\"].unique()\n",
    "compare_df = pd.DataFrame(data={\"STATION\": unique_station})\n",
    "compare_df = compare_df.set_index(\"STATION\")\n",
    "\n",
    "# f = files[0]\n",
    "for f in files:\n",
    "    kernel = os.path.basename(f).split(\"_\")[3]\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    cache = {\"STATION\": [], f\"{kernel}_MAE\": []}\n",
    "\n",
    "    # select_station = \"TUKU\"\n",
    "    for select_station in unique_station:\n",
    "        df_station = df.query(\"input_STATION==@select_station\")\n",
    "        mean_abs_err = df_station[\"absolute_error\"].mean()\n",
    "\n",
    "        cache[\"STATION\"].append(select_station)\n",
    "        cache[f\"{kernel}_MAE\"].append(mean_abs_err)\n",
    "\n",
    "        temp = pd.DataFrame(cache)\n",
    "        temp = temp.set_index(\"STATION\")\n",
    "        compare_df[f\"{kernel}_MAE\"] = compare_df.index.map(\n",
    "            temp[f\"{kernel}_MAE\"]\n",
    "        )\n",
    "\n",
    "cache = {\"STATION\": [], \"Best_kernel\": []}\n",
    "\n",
    "for station in unique_station:\n",
    "    best_kernel = compare_df.loc[station].sort_values().index[0].split(\"_\")[0]\n",
    "    cache[\"STATION\"].append(station)\n",
    "    cache[\"Best_kernel\"].append(best_kernel)\n",
    "\n",
    "kernel_df = pd.DataFrame(cache)\n",
    "kernel_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356ac8b8-75ae-45da-b9be-bbd02ec42c99",
   "metadata": {},
   "source": [
    "#### Find best kernel for each timestep"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b07e7e7-d3ea-4aff-b3e4-48ccd852f39f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "df = pd.read_csv(files[0])\n",
    "unique_time = [f\"time_{str(t).zfill(3)}\" for t in df[\"Time_value\"].unique()]\n",
    "compare_df = pd.DataFrame(data={\"Time_value\": unique_time})\n",
    "compare_df = compare_df.set_index(\"Time_value\")\n",
    "\n",
    "for f in files:\n",
    "    kernel = os.path.basename(f).split(\"_\")[3]\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    cache = {\"Time_value\": [], f\"{kernel}_MAE\": []}\n",
    "    for select_time in unique_time:\n",
    "        # select_time = unique_time[0]\n",
    "        numeric_time = int(select_time.split(\"_\")[-1])\n",
    "        df_time = df.query(\"Time_value==@numeric_time\")\n",
    "        mean_abs_err = df_time[\"absolute_error\"].mean()\n",
    "\n",
    "        cache[\"Time_value\"].append(select_time)\n",
    "        cache[f\"{kernel}_MAE\"].append(mean_abs_err)\n",
    "\n",
    "    temp = pd.DataFrame(cache)\n",
    "    temp = temp.set_index(\"Time_value\")\n",
    "    compare_df[f\"{kernel}_MAE\"] = compare_df.index.map(temp[f\"{kernel}_MAE\"])\n",
    "\n",
    "compare_df.to_csv(f\"Compare_MAE_overTime_byKernels_{layer}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "de4901f4-c157-4749-91fc-ee62ef678ca9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "cache = {\"Time_value\": [], \"Best_kernel\": []}\n",
    "\n",
    "for select_time in unique_time:\n",
    "    best_kernel = (\n",
    "        compare_df.loc[select_time].sort_values().index[0].split(\"_\")[0]\n",
    "    )\n",
    "    cache[\"Time_value\"].append(select_time)\n",
    "    cache[\"Best_kernel\"].append(best_kernel)\n",
    "\n",
    "kernel_df = pd.DataFrame(cache)\n",
    "kernel_df.to_csv(f\"BestKernel_byTime_{layer}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc78c14c-2d92-4165-82ac-ba77bcb1c078",
   "metadata": {},
   "source": [
    "#### Copy best-kernel files into one folder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "22235354-98b6-4c99-82a3-64189a1ed7bd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fpath = \"BestKernel_byTime_Combined.csv\"\n",
    "df = pd.read_csv(fpath)\n",
    "\n",
    "timestep_arr = df[\"Time_value\"].unique()\n",
    "layers = [f\"Layer_{i}\" for i in range(1, 5)]\n",
    "\n",
    "for select_layer in tqdm(layers[:]):  # Process all layers, not just the first\n",
    "    savefolder = f\"GWR_AllKernel_{select_layer}\"\n",
    "    os.makedirs(savefolder, exist_ok=True)  # More concise directory creation\n",
    "\n",
    "    kernel_list = df.loc[:, select_layer]\n",
    "\n",
    "    # Precompute numeric timesteps to avoid repeated string operations\n",
    "    numeric_timesteps = [int(ts.split(\"_\")[-1]) for ts in timestep_arr]\n",
    "\n",
    "    for current_kernel, numeric_timestep in zip(\n",
    "        tqdm(kernel_list), numeric_timesteps\n",
    "    ):\n",
    "        # Construct patterns once for both file types\n",
    "        pattern = f\"*{select_layer}*{current_kernel}_time{numeric_timestep}_bw*\"\n",
    "\n",
    "        # Search for both file types in a single glob call\n",
    "        search_files = glob(\n",
    "            os.path.join(f\"GWR_Output_{select_layer}\", pattern + \"*.csv\")\n",
    "        )\n",
    "\n",
    "        # Filter and copy files\n",
    "        for source_file in search_files:\n",
    "            base_file = os.path.basename(source_file)\n",
    "            dest_file = os.path.join(savefolder, base_file)\n",
    "            shutil.copy2(source_file, dest_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe0dca-fe55-412a-8168-5a07baddc8e0",
   "metadata": {},
   "source": [
    "#### Combine all into one"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d8a5849a-3fb5-42c4-b45b-8b6388c13e2e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "folders = glob(\"GWR_AllKernel_*\")\n",
    "\n",
    "# select_fld = folders[0]\n",
    "\n",
    "for select_fld in folders[:]:\n",
    "\n",
    "    combined_df = pd.DataFrame(data=None, index=None)\n",
    "\n",
    "    files_byFolder = glob(os.path.join(select_fld, \"*results.csv\"))\n",
    "\n",
    "    # select_file = files_byFolder[0]\n",
    "\n",
    "    for select_file in files_byFolder:\n",
    "\n",
    "        kernel = os.path.basename(select_file).split(\"_\")[3]\n",
    "\n",
    "        file_df = pd.read_csv(select_file)\n",
    "\n",
    "        file_df[\"kernel\"] = kernel\n",
    "\n",
    "        combined_df = pd.concat([combined_df, file_df], ignore_index=True)\n",
    "\n",
    "    combined_df = combined_df.sort_values(\n",
    "        by=[\"Time_value\", \"input_STATION\"]\n",
    "    ).reset_index(drop=True)\n",
    "\n",
    "    combined_df.to_csv(f\"{select_fld}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a690eab-91b5-4fb8-80ae-89a8e406585b",
   "metadata": {},
   "source": [
    "#### Prepare kernels+bw for predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27afcf26-d7e6-4fb2-b118-bf4b905431c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(os.path.join(\"GWR_AllKernel_Layer*\", \"*.csv\"))\n",
    "# f = files[0]\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "\n",
    "    unique_times = df[\"Time_value\"].unique()[:]\n",
    "\n",
    "    result = [\n",
    "        {\n",
    "            \"monthly\": t,\n",
    "            \"optimal_bandwidth\": df.query(\"Time_value == @t\")[\n",
    "                \"bandwidth_used\"\n",
    "            ].unique()[0],\n",
    "            \"optimal_kernel\": df.query(\"Time_value == @t\")[\"kernel\"].unique()[\n",
    "                0\n",
    "            ],\n",
    "            \"is_adaptive\": True,\n",
    "        }\n",
    "        for t in unique_times\n",
    "    ]\n",
    "\n",
    "    output = pd.DataFrame(result)\n",
    "\n",
    "    output.to_csv(\n",
    "        os.path.basename(f).replace(\".csv\", \"_model_info.csv\"), index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58073f51-b2af-4c1d-a814-d357e02a7437",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
