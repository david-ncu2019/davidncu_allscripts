{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52522a13-666f-4812-b428-e82fff492055",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23539edb-5bb3-431e-9724-fa0061d15798",
   "metadata": {},
   "source": [
    "**2025/4/23**\n",
    "\n",
    "Chunking the grid point CSV into smaller parts,\n",
    "\n",
    "The GTWR algorithm has problem wigh large number of points in spatial dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff3442-8423-4dab-9ab3-2d3f3d9c0ca1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gridpoint_fpath = r\"D:\\1000_SCRIPTS\\003_Project002\\20250917_GTWR002\\2_KrigingInterpolation\\points_fld\\grid_pnt.shp\"\n",
    "df = gpd.read_file(gridpoint_fpath, read_geometry=False)\n",
    "chunk_size = 100\n",
    "num_chunks = (len(df) + chunk_size - 1) // chunk_size\n",
    "# Initialize empty list to store our chunks\n",
    "chunks = []\n",
    "\n",
    "savefolder = os.path.join(os.getcwd(), f\"gridpnt_Chunk{chunk_size}\")\n",
    "\n",
    "if not os.path.exists(savefolder):\n",
    "    os.makedirs(savefolder, exist_ok=True)\n",
    "\n",
    "\n",
    "# Create each chunk using row indexing\n",
    "for i in range(num_chunks):\n",
    "    # Calculate starting and ending indices for the current chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = min(\n",
    "        (i + 1) * chunk_size, len(df)\n",
    "    )  # Prevent going beyond DataFrame length\n",
    "\n",
    "    # Extract the chunk and add it to our list\n",
    "    chunk = df.iloc[start_idx:end_idx].copy()\n",
    "    chunks.append(chunk)\n",
    "\n",
    "    print(\n",
    "        f\"Created chunk {i+1}: rows {start_idx+1}-{end_idx} ({len(chunk)} rows)\"\n",
    "    )\n",
    "\n",
    "for idx, _df in enumerate(chunks):\n",
    "    _df.to_csv(\n",
    "        os.path.join(savefolder, f\"grid_pnt_chunk{str(idx+1).zfill(3)}.csv\"),\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6152fcc7-dba1-4f50-bb66-3c93370865fa",
   "metadata": {},
   "source": [
    "**2025/4/21**\n",
    "\n",
    "Prepare the CSV file which contains coordinates and time, which will be used for regression points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8e5ef4-273b-4330-bb86-63051b9af006",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_points_df = pd.read_csv(\n",
    "    r\".\\calib_input\\20251014_GTWR_InputData_MLCW_InSAR_Layer_1.csv\"\n",
    ")\n",
    "calibration_points_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c4b204-0efc-407d-bb79-3effe9b6b20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_arr = calibration_points_df[\"monthly\"].unique()\n",
    "monthly_arr[:5], monthly_arr[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997b8c61-77f5-4e4f-8177-4b8f923a897e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid_point_fld = \"grid_pnt_Chunking/\"\n",
    "grid_point_fld = savefolder\n",
    "grid_point_files = glob(os.path.join(grid_point_fld, \"*.csv\"))\n",
    "\n",
    "feather_output_fld = os.path.join(os.getcwd(), f\"regpoints_Chunk{chunk_size}\")\n",
    "\n",
    "if not os.path.exists(feather_output_fld):\n",
    "    os.makedirs(feather_output_fld, exist_ok=True)\n",
    "\n",
    "# 2025/4/23: instead of using pd.concat() to combine all into one\n",
    "# we will save CSV file one by one, corresponding to time period\n",
    "\n",
    "# 2025/4/23: even chunking like that, the GTWR program cannot extract the points quickly\n",
    "# I think it depends on the number of points for each time period\n",
    "# df = pd.read_csv(\"grid_pnt.csv\")\n",
    "\n",
    "for fname in tqdm(grid_point_files[:]):\n",
    "    df = pd.read_csv(fname)\n",
    "    base = os.path.basename(fname).split(\".\")[0]\n",
    "\n",
    "    # output_fld = os.path.join(feather_output_fld, base)\n",
    "    # os.makedirs(output_fld)\n",
    "\n",
    "    regression_points_df = pd.DataFrame(data=None, index=None)\n",
    "\n",
    "    for month in tqdm(monthly_arr, position=1, leave=False):\n",
    "        df[\"monthly\"] = [month] * len(df)\n",
    "        # df.to_csv(os.path.join(feather_output_fld, f\"regpoint_t{month}.csv\"), index=False)\n",
    "        regression_points_df = pd.concat(\n",
    "            [regression_points_df, df], ignore_index=True\n",
    "        )\n",
    "\n",
    "    regression_points_df.to_feather(\n",
    "        os.path.join(feather_output_fld, f\"{base}.feather\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "55477dfd-db7c-4310-add9-e9218159f5b6",
   "metadata": {},
   "source": [
    "# 2025/04/23 -- remove this, R program cannot read .xz file\n",
    "regression_points_df.to_pickle(\"CRFP_regression_points.xz\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ab68995-9b5f-4e1b-9ae2-e273c65e31be",
   "metadata": {},
   "source": [
    "# 2025/04/23 -- remove, we will not export a big file,\n",
    "# instead, we export CSV files corresponding to each time period\n",
    "regression_points_df.to_feather(\"CRFP_regression_points.feather\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ff8ff3f-6797-4bd4-a252-02088b50ced8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "regression_points_df = pd.DataFrame(data=None, index=None)\n",
    "\n",
    "for month in tqdm(monthly_arr):\n",
    "    temp = df.iloc[:5, :].copy()\n",
    "    temp[\"monthly\"] = [month]*5\n",
    "    regression_points_df = pd.concat([regression_points_df, temp], ignore_index=True)\n",
    "\n",
    "regression_points_df.to_csv(\"regression_points_testting.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e011bef-d09c-4801-b627-8e7a644fc636",
   "metadata": {},
   "source": [
    "**2025/4/22**\n",
    "\n",
    "Reduce regression points size"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ebc659fb-26c1-4978-8b7a-d62a6c4add64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:23:00.628188Z",
     "iopub.status.busy": "2025-04-22T01:23:00.627692Z",
     "iopub.status.idle": "2025-04-22T01:23:14.412167Z",
     "shell.execute_reply": "2025-04-22T01:23:14.412167Z",
     "shell.execute_reply.started": "2025-04-22T01:23:00.628188Z"
    }
   },
   "source": [
    "df = pd.read_csv(\"grid_pnt.csv\")\n",
    "\n",
    "calibration_points_df = pd.read_csv(r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\3_MGTWR\\2_Test_Run001\\20250416_GWR_InputData_CUMDISP_MLCW_InSAR.csv\")\n",
    "\n",
    "monthly_arr = calibration_points_df[\"monthly\"].unique()\n",
    "\n",
    "regression_points_df = pd.DataFrame(data=None, index=None)\n",
    "\n",
    "for month in tqdm(monthly_arr):\n",
    "    df[\"monthly\"] = [month]*len(df)\n",
    "    temp = df.iloc[::50, :].copy()\n",
    "    regression_points_df = pd.concat([regression_points_df, temp], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fee6b8d6-2346-4051-9dd8-5950845c2f81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-22T01:23:21.610401Z",
     "iopub.status.busy": "2025-04-22T01:23:21.610401Z",
     "iopub.status.idle": "2025-04-22T01:23:21.644455Z",
     "shell.execute_reply": "2025-04-22T01:23:21.643962Z",
     "shell.execute_reply.started": "2025-04-22T01:23:21.610401Z"
    }
   },
   "source": [
    "regression_points_df.to_feather(\"CRFP_regression_points_reduced50.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd186556-0b84-41db-9f1a-8c898b43018d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "6da1a103-aebc-44fe-ac59-5b825f4f1ba0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-23T08:52:56.138424Z",
     "iopub.status.busy": "2025-04-23T08:52:56.138424Z",
     "iopub.status.idle": "2025-04-23T08:52:56.212823Z",
     "shell.execute_reply": "2025-04-23T08:52:56.210839Z",
     "shell.execute_reply.started": "2025-04-23T08:52:56.138424Z"
    }
   },
   "source": [
    "pd.read_feather(r\"D:\\1000_SCRIPTS\\003_Project002\\20250222_GTWR001\\3_MGTWR\\4_Test_Run003\\regpoints_filefld\\grid_pnt_chunk001.feather\").query(\"monthly==0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75c3a02-3e11-45d4-a207-a7e5c246914c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
