{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e46d1a99-f7d5-45c8-a921-44cb1ac3382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b05b8f-51b6-46cd-92e3-038896835117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_nscore(df, idx_column=\"STATION\", transformation_columns=None):\n",
    "    \"\"\"\n",
    "    Applies normal score transformation to specified columns in the input DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "      df (pd.DataFrame): Input data frame.\n",
    "      idx_column (str): Name of the unique identifier column (default: \"STATION\").\n",
    "      transformation_columns (list of str): List of column names to transform.\n",
    "          If None, all columns except the idx_column are used.\n",
    "\n",
    "    Returns:\n",
    "      pd.DataFrame: A new DataFrame containing the original identifier and the new\n",
    "          transformed columns, each prefixed with 'Trans_'.\n",
    "\n",
    "    Structural Rules for the Input DataFrame:\n",
    "      1. Must include a unique identifier column (default name: \"STATION\") that uniquely\n",
    "         identifies each record.\n",
    "      2. Must contain one or more numeric columns that are to be transformed.\n",
    "      3. Any additional columns (meta-data) are preserved as is.\n",
    "      4. If transformation_columns is not provided, all columns except the idx_column will\n",
    "         be considered for transformation.\n",
    "    \"\"\"\n",
    "    # Validate that the required identifier column exists\n",
    "    if idx_column not in df.columns:\n",
    "        raise ValueError(\n",
    "            f\"Input DataFrame must contain the '{idx_column}' column as a unique identifier.\"\n",
    "        )\n",
    "\n",
    "    # Determine columns to transform if not explicitly provided\n",
    "    if transformation_columns is None:\n",
    "        transformation_columns = [\n",
    "            col for col in df.columns if col != idx_column\n",
    "        ]\n",
    "\n",
    "    # Create a copy to work on; use idx_column as index for mapping transformation results\n",
    "    output_df = df.loc[:, ~df.columns.isin(transformation_columns)].copy()\n",
    "    output_df.set_index(idx_column, inplace=True)\n",
    "\n",
    "    # --------------------------------------------------------------------------\n",
    "    # Significant change: Refactored loop to process each transformation column generically\n",
    "    # --------------------------------------------------------------------------\n",
    "    for col in tqdm(transformation_columns):\n",
    "        try:\n",
    "            # Create a temporary DataFrame for the current column\n",
    "            temp = df[[idx_column, col]].dropna(subset=[col]).copy()\n",
    "\n",
    "            # Apply normal score transformation\n",
    "            # The new transformed column is prefixed with 'Trans_'\n",
    "            transformed_col = \"Trans_\" + col\n",
    "            temp[transformed_col], tvDisp, tnsDisp = geostats.nscore(temp, col)\n",
    "\n",
    "            # Filter out outliers outside the acceptable range [-3, 3]\n",
    "            filter_cond = (temp[transformed_col] >= -3) & (\n",
    "                temp[transformed_col] <= 3\n",
    "            )\n",
    "            temp = temp[filter_cond]\n",
    "            temp.set_index(idx_column, inplace=True)\n",
    "\n",
    "            # Map the transformed values back to the output DataFrame using the identifier\n",
    "            output_df[transformed_col] = output_df.index.map(\n",
    "                temp[transformed_col]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing column '{col}': {e}\")\n",
    "            continue\n",
    "\n",
    "    # Reset index so that the identifier becomes a column again\n",
    "    output_df.reset_index(inplace=True)\n",
    "    return output_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ac7001-c46f-4c3a-981c-260455b5fa91",
   "metadata": {},
   "source": [
    "#### Monthly dU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f5822d0-c8cd-46df-a563-0e3f9f7073eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input DataFrame must contain the 'PointKey' column as a unique identifier.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_pickle(fpath)\n\u001b[0;32m      4\u001b[0m trans_cols \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m col\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m----> 5\u001b[0m transformed_df \u001b[38;5;241m=\u001b[39m \u001b[43mapply_nscore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_column\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPointKey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformation_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrans_cols\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# transformed_df.to_csv(f\"2_Transformed/NSCORE_{basename.replace(\".xz\", \".csv\")}\", index=False)\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# with open(f\"2_Transformed/NSCORE_{basename}\", \"wb\") as f:\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     pickle.dump(transformed_df.to_dict(), f, protocol=2)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 25\u001b[0m, in \u001b[0;36mapply_nscore\u001b[1;34m(df, idx_column, transformation_columns)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Validate that the required identifier column exists\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m idx_column \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m---> 25\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput DataFrame must contain the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx_column\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column as a unique identifier.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     27\u001b[0m     )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Determine columns to transform if not explicitly provided\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformation_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input DataFrame must contain the 'PointKey' column as a unique identifier."
     ]
    }
   ],
   "source": [
    "fpath = \"CORRECTED_Monthly_DISPLACEMENT_CRFP_saveqgis_Oct2025.xz\"\n",
    "basename = os.path.basename(fpath).split(\".\")[0]\n",
    "df = pd.read_pickle(fpath)\n",
    "df = df.reset_index(drop=False)\n",
    "trans_cols = [col for col in df.columns if col.startswith(\"N\")]\n",
    "transformed_df = apply_nscore(\n",
    "    df=df, idx_column=\"PointKey\", transformation_columns=trans_cols\n",
    ")\n",
    "# transformed_df.to_csv(f\"2_Transformed/NSCORE_{basename.replace(\".xz\", \".csv\")}\", index=False)\n",
    "\n",
    "# with open(f\"2_Transformed/NSCORE_{basename}\", \"wb\") as f:\n",
    "#     pickle.dump(transformed_df.to_dict(), f, protocol=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3ed0e7-2bbb-49a7-aa23-12d2b42fc465",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_chunks = 30\n",
    "\n",
    "# Split the output DataFrame into 10 equally sized chunks.\n",
    "chunks = np.array_split(transformed_df, number_of_chunks)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    out_path = f\"2_Transformed/NSCORE_{basename}_{str(i+1).zfill(3)}.xz\"\n",
    "    with open(out_path, \"wb\") as f:\n",
    "        pickle.dump(chunk.to_dict(), f, protocol=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
