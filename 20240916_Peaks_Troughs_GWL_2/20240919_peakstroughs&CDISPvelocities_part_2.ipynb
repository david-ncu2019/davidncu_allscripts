{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faf1b5c-ffe1-4576-9b1b-e973a5eebde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed\n",
    "from my_packages import *\n",
    "\n",
    "from appgeopy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bc75c-88b8-4f20-9481-694972fedae4",
   "metadata": {},
   "source": [
    "2024/09/19 - **Part 2**\n",
    "\n",
    "1. Extract the dataframe from part 1\n",
    "\n",
    "2. Calculate the linear velocies of cumulative displacements corresponding to peaks and troughs\n",
    "\n",
    "3. Save to HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af26e4-982c-4a7a-8323-808ae14a8a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a simple function to decode the datetime string\n",
    "string_decode_func = np.vectorize(lambda x: x.decode(\"utf-8\"))\n",
    "\n",
    "# ________________________________________________________________________________\n",
    "\n",
    "\n",
    "def transform_to_dataframe(dict_byWellCode, extreme_type):\n",
    "    # Validate extreme_type\n",
    "    if extreme_type not in [\"peaks\", \"troughs\"]:\n",
    "        raise ValueError(\"extreme_type must be either 'peaks' or 'troughs'\")\n",
    "\n",
    "    date_string = dict_byWellCode[extreme_type][\"date\"]\n",
    "    string2date = pd.to_datetime(string_decode_func(date_string), format=\"%Y%m%d\")\n",
    "    value_array = dict_byWellCode[extreme_type][\"value\"]\n",
    "    return pd.DataFrame({\"time\": string2date, \"value\": value_array}).assign(Type=extreme_type)\n",
    "\n",
    "\n",
    "# ________________________________________________________________________________\n",
    "\n",
    "\n",
    "def create_numeric_timerange(insar_datetime, freq=\"D\"):\n",
    "    \"\"\"\n",
    "    Creates a full time range and corresponding numeric time indices for InSAR datetimes.\n",
    "\n",
    "    Parameters:\n",
    "    - insar_datetime (pd.Series): Series of InSAR datetime values.\n",
    "    - freq (str): Frequency for generating the full time range, default is daily ('D').\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: Numeric full time range series indexed by full time range dates.\n",
    "    \"\"\"\n",
    "    # Ensure the series is sorted and has no null values\n",
    "    insar_datetime = insar_datetime.dropna().sort_values()\n",
    "\n",
    "    # Use pandas `date_range` to generate a full datetime range\n",
    "    full_timerange = pd.date_range(start=insar_datetime.min(), end=insar_datetime.max(), freq=freq)\n",
    "\n",
    "    # Create a range of numbers as indices\n",
    "    numeric_indices = pd.Series(range(len(full_timerange)), index=full_timerange)\n",
    "\n",
    "    # Return the numeric indices that correspond to the original InSAR datetimes\n",
    "    return numeric_indices[insar_datetime]\n",
    "\n",
    "\n",
    "# ________________________________________________________________________________\n",
    "def get_average_velocity(x, y):\n",
    "    \"\"\"\n",
    "    Computes the average velocity from x and y data using polynomial trend analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - x (array-like): Array of x values (time indices).\n",
    "    - y (array-like): Array of y values (displacement data).\n",
    "\n",
    "    Returns:\n",
    "    - float: The average velocity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _, coeff = analysis.get_polynomial_trend(x, y, 1)\n",
    "        return coeff[-1] * 365.25\n",
    "    except ValueError as e:\n",
    "        print(f\"Error computing velocity for y={y}: {e}\")\n",
    "        return np.nan  # Return NaN if polynomial fitting fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765d8a0-d171-4fb5-8c10-986eba014e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdisp_peakstroughs_df = pd.read_pickle(r\"20240919_CDISP_byPeaksTroughs_allStations_store.pkl\", compression=\"zip\")\n",
    "cdisp_peakstroughs_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbcdca6b-32df-4d5e-9ec1-c67cd3e454f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "available_stations = cdisp_peakstroughs_df[\"Station\"].unique()\n",
    "available_stations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3902b1-d67b-41a6-9c7c-60db8b827151",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict_cache = []\n",
    "\n",
    "# select_station = available_stations[0]\n",
    "for select_station in tqdm(available_stations):\n",
    "    df_byStation = cdisp_peakstroughs_df.query(\"Station==@select_station\")\n",
    "\n",
    "    wellcodes = df_byStation[\"WellCode\"].unique()\n",
    "\n",
    "    # _______________________________________________________________________\n",
    "    # select_wellcode = wellcodes[0]\n",
    "    for select_wellcode in wellcodes:\n",
    "\n",
    "        dfStation_byWellCode = df_byStation.query(\"WellCode==@select_wellcode\").reset_index(drop=True)\n",
    "\n",
    "        # Use joblib to parallelize the velocity computation across all rows\n",
    "        def calculate_row(row):\n",
    "            return get_average_velocity(numeric_datetime.to_numpy(), row)\n",
    "\n",
    "        peaktrough_datetime_array = (\n",
    "            dfStation_byWellCode.loc[:, \"peaktrough_pairs\"]\n",
    "            .apply(lambda arr: np.array([elem.strftime(\"%Y%m%d\") for elem in arr]))\n",
    "            .values\n",
    "        )\n",
    "        peaktrough_datetime_array = np.vstack(peaktrough_datetime_array).astype(\"S8\")\n",
    "\n",
    "        velocity_array_cache = []\n",
    "\n",
    "        for index in range(len(dfStation_byWellCode)):\n",
    "            # _______________________________________________________________________\n",
    "            cdisp_byPeaksTroughs = dfStation_byWellCode.loc[index, \"cdisp_df\"]\n",
    "            numeric_datetime = create_numeric_timerange(cdisp_byPeaksTroughs.columns)\n",
    "            df_to_calculate = cdisp_byPeaksTroughs.subtract(cdisp_byPeaksTroughs.iloc[:, 0], axis=0)\n",
    "            # _______________________________________________________________________\n",
    "            # Parallelize the apply step using joblib\n",
    "            num_cores = 8  # Use all available cores\n",
    "            result = Parallel(n_jobs=num_cores)(delayed(calculate_row)(row) for _, row in df_to_calculate.iterrows())\n",
    "            # _______________________________________________________________________\n",
    "            # Convert the result back to a Numpy Array\n",
    "            velocity_arr = np.array(result)\n",
    "            velocity_array_cache.append(velocity_arr)\n",
    "\n",
    "        output_velocity_arr = np.vstack(velocity_array_cache)\n",
    "\n",
    "        output_data_dict = {\n",
    "            select_station: {\n",
    "                select_wellcode: {\"velocities\": {\"date\": peaktrough_datetime_array, \"value\": output_velocity_arr}}\n",
    "            }\n",
    "        }\n",
    "\n",
    "        data_dict_cache.append(output_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bcd7f1-c265-44f1-bab8-822dfe3909a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load old HDF5 data\n",
    "\n",
    "gwl_hdf5_file = \"20240919_GWL_CRFP_peakstroughs.h5\"\n",
    "\n",
    "# Extract existing data and metadata\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    existing_data_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "    existing_metadata_dict = gwatertools.h5pytools.hdf5_to_metadata_dict(hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4070bb6-d74f-42d0-988f-e16de16f9afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_dict = gwatertools.h5pytools.merge_dicts(*data_dict_cache)\n",
    "new_metadata_dict = {\n",
    "    \"Update005\": \"2024/09/19 : Calculate linear velocities of all PS points for each peak-trough time range\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c43bac-baf0-4a1f-b65c-2605f876b610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dictionaries\n",
    "updated_data_dict = gwatertools.h5pytools.update_data_dict(existing_data_dict, new_data_dict)\n",
    "updated_metadata_dict = gwatertools.h5pytools.update_metadata_dict(existing_metadata_dict, new_metadata_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf948fa4-5586-47ae-bbe6-187817d256eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "with h5py.File(f\"{today_string}_GWL_peakstroughs&velocities.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, existing_metadata_dict)\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, updated_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4fd209-24cb-40fa-968e-ceffbdfd1a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28696f83-703a-45a9-be97-b9c6cb12dadf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
