{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1faf1b5c-ffe1-4576-9b1b-e973a5eebde1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:08:26.906035Z",
     "iopub.status.busy": "2024-09-19T09:08:26.906035Z",
     "iopub.status.idle": "2024-09-19T09:08:30.116219Z",
     "shell.execute_reply": "2024-09-19T09:08:30.116219Z",
     "shell.execute_reply.started": "2024-09-19T09:08:26.906035Z"
    }
   },
   "outputs": [],
   "source": [
    "from my_packages import *\n",
    "\n",
    "from appgeopy import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133bc75c-88b8-4f20-9481-694972fedae4",
   "metadata": {},
   "source": [
    "2024/09/19 - **Part 1**\n",
    "\n",
    "1. Store the tables of cumulative displacement corresponding to peak-trough pairs in each well code\n",
    "\n",
    "2. All tables will be saved as a dataframe in pickle file format\n",
    "\n",
    "3. Then this dataframe will be used by another script to calculate linear velocities and save to HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62af26e4-982c-4a7a-8323-808ae14a8a75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:08:30.117184Z",
     "iopub.status.busy": "2024-09-19T09:08:30.117184Z",
     "iopub.status.idle": "2024-09-19T09:08:30.122192Z",
     "shell.execute_reply": "2024-09-19T09:08:30.122192Z",
     "shell.execute_reply.started": "2024-09-19T09:08:30.117184Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a simple function to decode the datetime string\n",
    "string_decode_func = np.vectorize(lambda x: x.decode(\"utf-8\"))\n",
    "\n",
    "# ________________________________________________________________________________\n",
    "\n",
    "\n",
    "def transform_to_dataframe(dict_byWellCode, extreme_type):\n",
    "    # Validate extreme_type\n",
    "    if extreme_type not in [\"peaks\", \"troughs\"]:\n",
    "        raise ValueError(\"extreme_type must be either 'peaks' or 'troughs'\")\n",
    "\n",
    "    date_string = dict_byWellCode[extreme_type][\"date\"]\n",
    "    string2date = pd.to_datetime(string_decode_func(date_string), format=\"%Y%m%d\")\n",
    "    value_array = dict_byWellCode[extreme_type][\"value\"]\n",
    "    return pd.DataFrame({\"time\": string2date, \"value\": value_array}).assign(Type=extreme_type)\n",
    "\n",
    "\n",
    "# ________________________________________________________________________________\n",
    "\n",
    "\n",
    "def create_numeric_timerange(insar_datetime, freq=\"D\"):\n",
    "    \"\"\"\n",
    "    Creates a full time range and corresponding numeric time indices for InSAR datetimes.\n",
    "\n",
    "    Parameters:\n",
    "    - insar_datetime (pd.Series): Series of InSAR datetime values.\n",
    "    - freq (str): Frequency for generating the full time range, default is daily ('D').\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: Numeric full time range series indexed by full time range dates.\n",
    "    \"\"\"\n",
    "    # Ensure the series is sorted and has no null values\n",
    "    insar_datetime = insar_datetime.dropna().sort_values()\n",
    "\n",
    "    # Use pandas `date_range` to generate a full datetime range\n",
    "    full_timerange = pd.date_range(start=insar_datetime.min(), end=insar_datetime.max(), freq=freq)\n",
    "\n",
    "    # Create a range of numbers as indices\n",
    "    numeric_indices = pd.Series(range(len(full_timerange)), index=full_timerange)\n",
    "\n",
    "    # Return the numeric indices that correspond to the original InSAR datetimes\n",
    "    return numeric_indices[insar_datetime]\n",
    "\n",
    "\n",
    "# ________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf3169db-c24d-4755-8b5c-b0791faeb224",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:08:30.123188Z",
     "iopub.status.busy": "2024-09-19T09:08:30.123188Z",
     "iopub.status.idle": "2024-09-19T09:08:30.762283Z",
     "shell.execute_reply": "2024-09-19T09:08:30.762283Z",
     "shell.execute_reply.started": "2024-09-19T09:08:30.123188Z"
    }
   },
   "outputs": [],
   "source": [
    "# open HDF5 file and get data\n",
    "\n",
    "gwl_hdf5_file = \"20240919_GWL_CRFP_peakstroughs.h5\"\n",
    "\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    datasets = gwatertools.h5pytools.list_datasets(hdf5_file)\n",
    "    existing_data_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4cd367-2951-4bab-ae74-dc74128f606d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:08:30.764184Z",
     "iopub.status.busy": "2024-09-19T09:08:30.764184Z",
     "iopub.status.idle": "2024-09-19T09:08:30.773156Z",
     "shell.execute_reply": "2024-09-19T09:08:30.773156Z",
     "shell.execute_reply.started": "2024-09-19T09:08:30.764184Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatetimeIndex(['2016-05-08', '2016-05-20', '2016-06-01', '2016-06-13',\n",
       "               '2016-07-07'],\n",
       "              dtype='datetime64[ns]', freq=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get InSAR datetime array\n",
    "\n",
    "insar_datetime = pd.to_datetime(string_decode_func(existing_data_dict[\"InSAR_datetime\"]), format=\"%Y%m%d\")\n",
    "insar_datetime[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72117a0e-87e2-422e-802e-7923bc69e2d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:08:30.775156Z",
     "iopub.status.busy": "2024-09-19T09:08:30.774156Z",
     "iopub.status.idle": "2024-09-19T09:08:30.781155Z",
     "shell.execute_reply": "2024-09-19T09:08:30.781155Z",
     "shell.execute_reply.started": "2024-09-19T09:08:30.775156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANHE', 'ANNAN', 'BEIGANG', 'BOZI', 'CAICUO']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "available_stations = sorted(set([elem.split(\"/\")[0] for elem in datasets if \"date\" not in elem]))\n",
    "available_stations[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e32894aa-de82-4a61-9a3f-1b110a4a74de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:12:28.709817Z",
     "iopub.status.busy": "2024-09-19T09:12:28.709817Z",
     "iopub.status.idle": "2024-09-19T09:12:31.600817Z",
     "shell.execute_reply": "2024-09-19T09:12:31.600817Z",
     "shell.execute_reply.started": "2024-09-19T09:12:28.709817Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    }
   ],
   "source": [
    "cdisp_allWellCode = pd.DataFrame(data=None)\n",
    "\n",
    "# Helper function to process peak-trough pairs and cumulative displacement\n",
    "def process_peak_trough_pairs(cdisp_df, combined):\n",
    "    cache = {\"peaktrough_pairs\": [], \"cdisp_df\": []}\n",
    "    \n",
    "    for i in range(0, len(combined), 2):\n",
    "        j = i + 1\n",
    "        peak_time, peak_value, _ = combined.loc[i, :].values\n",
    "        trough_time, trough_value, _ = combined.loc[j, :].values\n",
    "\n",
    "        # Get cumulative displacement data between peak and trough\n",
    "        cdisp_byTime = cdisp_df.loc[:, peak_time:trough_time]\n",
    "\n",
    "        if cdisp_byTime.empty or cdisp_byTime.columns.size <= 5:\n",
    "            continue  # Skip small or empty cdisp_byTime\n",
    "\n",
    "        # Store the valid data\n",
    "        cache[\"peaktrough_pairs\"].append([peak_time, trough_time])\n",
    "        cache[\"cdisp_df\"].append(cdisp_byTime)\n",
    "    \n",
    "    return pd.DataFrame(cache)\n",
    "\n",
    "# Loop through stations\n",
    "for select_station in tqdm(available_stations, desc=\"Processing Stations\", leave=False):\n",
    "    data_dict_byStation = existing_data_dict.get(select_station)\n",
    "\n",
    "    # Skip if no CDISP data is found in the station\n",
    "    cdisp_arr = data_dict_byStation.get(\"CDISP\")\n",
    "    if cdisp_arr is None:\n",
    "        continue\n",
    "\n",
    "    # Retrieve available well codes in each station\n",
    "    wellcodes = [elem for elem in data_dict_byStation if isinstance(data_dict_byStation[elem], dict)]\n",
    "    \n",
    "    # Get cumulative displacement array corresponding to the station\n",
    "    cdisp_df = pd.DataFrame(data=cdisp_arr, columns=insar_datetime)\n",
    "    \n",
    "    # Loop through wellcodes\n",
    "    for select_wellcode in wellcodes:\n",
    "        data_dict_byWellCode = data_dict_byStation.get(select_wellcode)\n",
    "\n",
    "        # Skip if no peaks data is found for the well code\n",
    "        if \"peaks\" not in data_dict_byWellCode:\n",
    "            continue\n",
    "\n",
    "        # Retrieve peaks and troughs\n",
    "        peak_df = transform_to_dataframe(data_dict_byWellCode, \"peaks\")\n",
    "        trough_df = transform_to_dataframe(data_dict_byWellCode, \"troughs\")\n",
    "\n",
    "        # Combine peaks and troughs, sorted by time\n",
    "        combined = pd.concat([peak_df, trough_df], ignore_index=True).sort_values(by=\"time\").reset_index(drop=True)\n",
    "\n",
    "        # Process peak-trough pairs and store cumulative displacement\n",
    "        cdisp_df_byWellCode = process_peak_trough_pairs(cdisp_df, combined)\n",
    "\n",
    "        # Add station and well code metadata\n",
    "        cdisp_df_byWellCode[\"Station\"] = select_station\n",
    "        cdisp_df_byWellCode[\"WellCode\"] = select_wellcode\n",
    "\n",
    "        # Append result to the main DataFrame\n",
    "        cdisp_allWellCode = pd.concat([cdisp_allWellCode, cdisp_df_byWellCode], ignore_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b47ee04b-0d02-4e59-9016-3f271faaed44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:08:30.782154Z",
     "iopub.status.busy": "2024-09-19T09:08:30.782154Z",
     "iopub.status.idle": "2024-09-19T09:08:32.034183Z",
     "shell.execute_reply": "2024-09-19T09:08:32.034183Z",
     "shell.execute_reply.started": "2024-09-19T09:08:30.782154Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "cdisp_allWellCode = pd.DataFrame(data=None)\n",
    "\n",
    "# select station\n",
    "# select_station = available_stations[0]\n",
    "for select_station in tqdm(available_stations[:], desc=\"Processing Stations\", leave=False):\n",
    "    # ____________________________________________________________________\n",
    "\n",
    "    data_dict_byStation = existing_data_dict[select_station]\n",
    "\n",
    "    if \"CDISP\" in data_dict_byStation.keys():\n",
    "    \n",
    "        # retrieve available well codes in each station\n",
    "        wellcodes = [elem for elem in data_dict_byStation.keys() if (isinstance(data_dict_byStation[elem], dict))]\n",
    "    \n",
    "        # get the cumulative displacement array corresponding to each station\n",
    "        cdisp_arr = data_dict_byStation[\"CDISP\"]\n",
    "        cdisp_df = pd.DataFrame(data=cdisp_arr, columns=insar_datetime)\n",
    "    \n",
    "        # ____________________________________________________________________\n",
    "    \n",
    "        # select_wellcode = wellcodes[0]\n",
    "        for select_wellcode in wellcodes:\n",
    "    \n",
    "            data_dict_byWellCode = data_dict_byStation[select_wellcode]\n",
    "    \n",
    "            if \"peaks\" in data_dict_byWellCode.keys():\n",
    "    \n",
    "                peak_df = transform_to_dataframe(data_dict_byWellCode, \"peaks\")\n",
    "                trough_df = transform_to_dataframe(data_dict_byWellCode, \"troughs\")\n",
    "                combined = pd.concat([peak_df, trough_df], ignore_index=True).sort_values(by=\"time\").reset_index(drop=True)\n",
    "    \n",
    "                # ____________________________________________________________________\n",
    "    \n",
    "                cache = {\"peaktrough_pairs\": [], \"cdisp_df\": []}\n",
    "    \n",
    "                for i in range(0, len(combined), 2):\n",
    "                    j = i + 1\n",
    "                    peak_time, peak_value, _ = combined.loc[i, :].values\n",
    "                    trough_time, trough_value, _ = combined.loc[j, :].values\n",
    "    \n",
    "                    cdisp_byTime = cdisp_df.loc[:, peak_time:trough_time]\n",
    "    \n",
    "                    if (not cdisp_byTime.empty) and (cdisp_byTime.columns.size > 5):\n",
    "                        cache[\"peaktrough_pairs\"].append([peak_time, trough_time])\n",
    "                        cache[\"cdisp_df\"].append(cdisp_byTime)\n",
    "    \n",
    "                cdisp_df_byWellCode = pd.DataFrame(cache)\n",
    "                cdisp_df_byWellCode[\"Station\"] = select_station\n",
    "                cdisp_df_byWellCode[\"WellCode\"] = select_wellcode\n",
    "    \n",
    "                cdisp_allWellCode = pd.concat([cdisp_allWellCode, cdisp_df_byWellCode], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c366770-5d63-48b8-90d1-43de786702ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:12:35.677820Z",
     "iopub.status.busy": "2024-09-19T09:12:35.677820Z",
     "iopub.status.idle": "2024-09-19T09:12:47.472865Z",
     "shell.execute_reply": "2024-09-19T09:12:47.472865Z",
     "shell.execute_reply.started": "2024-09-19T09:12:35.677820Z"
    }
   },
   "outputs": [],
   "source": [
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "cdisp_allWellCode.to_pickle(f\"{today_string}_CDISP_byPeaksTroughs_allStations_store.pkl\", compression=\"zip\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3fe399b9-988a-4c14-8392-06dd02f6873c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T08:28:56.607059Z",
     "iopub.status.busy": "2024-09-19T08:28:56.607059Z",
     "iopub.status.idle": "2024-09-19T08:28:56.614057Z",
     "shell.execute_reply": "2024-09-19T08:28:56.613058Z",
     "shell.execute_reply.started": "2024-09-19T08:28:56.607059Z"
    }
   },
   "source": [
    "# Example InSAR datetime data\n",
    "insar_datetime = pd.Series(pd.to_datetime([\n",
    "    '2020-01-01', '2020-01-05', '2020-01-10'\n",
    "]))\n",
    "\n",
    "# Generate the numeric time range\n",
    "numeric_time_range = create_numeric_timerange(insar_datetime)\n",
    "\n",
    "print(numeric_time_range)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "119cabdf-5af9-4047-bae4-5bfd7f9ffd63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-19T09:07:13.574869Z",
     "iopub.status.busy": "2024-09-19T09:07:13.573871Z",
     "iopub.status.idle": "2024-09-19T09:07:13.579868Z",
     "shell.execute_reply": "2024-09-19T09:07:13.579868Z",
     "shell.execute_reply.started": "2024-09-19T09:07:13.573871Z"
    }
   },
   "source": [
    "a = {\"ABC\":[1,2,3]}\n",
    "\"ABC\" in a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3f5e3a-e30e-447e-a61a-3e19c180ddbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
