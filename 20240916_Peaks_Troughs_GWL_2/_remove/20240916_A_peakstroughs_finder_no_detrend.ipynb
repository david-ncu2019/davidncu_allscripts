{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d680374a-01b8-4eec-b0dc-ef4e61911c54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T16:43:40.830158Z",
     "iopub.status.busy": "2024-09-16T16:43:40.830158Z",
     "iopub.status.idle": "2024-09-16T16:43:44.169157Z",
     "shell.execute_reply": "2024-09-16T16:43:44.169157Z",
     "shell.execute_reply.started": "2024-09-16T16:43:40.830158Z"
    }
   },
   "outputs": [],
   "source": [
    "from my_packages import *\n",
    "\n",
    "from appgeopy import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9caeefd6-1f7c-4ead-84bb-e5e610093d81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T16:43:44.171185Z",
     "iopub.status.busy": "2024-09-16T16:43:44.171185Z",
     "iopub.status.idle": "2024-09-16T16:43:44.185157Z",
     "shell.execute_reply": "2024-09-16T16:43:44.185157Z",
     "shell.execute_reply.started": "2024-09-16T16:43:44.171185Z"
    }
   },
   "outputs": [],
   "source": [
    "# _______________________________________________________________________\n",
    "def detect_peaks_troughs(signal, time_index, prominence=True, dist=True):\n",
    "    \"\"\"\n",
    "    Detect peaks and troughs in a given signal.\n",
    "\n",
    "    Parameters:\n",
    "    - signal (array-like): The signal data where peaks and troughs are to be detected.\n",
    "    - time_index (array-like): Corresponding time index for the signal.\n",
    "    - prominence (bool): Whether to consider prominence in peak detection (default=True).\n",
    "    - dist (bool): Whether to consider distance in peak detection (default=True).\n",
    "\n",
    "    Returns:\n",
    "    - peaks, properties_peaks, peak_times: Detected peaks and their properties.\n",
    "    - troughs, properties_troughs, trough_times: Detected troughs and their properties.\n",
    "    \"\"\"\n",
    "    # Detect peaks in the signal\n",
    "    peaks, properties_peaks = scipy.signal.find_peaks(signal, prominence=prominence, distance=dist)\n",
    "    peak_times = time_index[peaks]\n",
    "\n",
    "    # Detect troughs by inverting the signal\n",
    "    troughs, properties_troughs = scipy.signal.find_peaks(-signal, prominence=prominence, distance=dist)\n",
    "    trough_times = time_index[troughs]\n",
    "\n",
    "    return peaks, properties_peaks, peak_times, troughs, properties_troughs, trough_times\n",
    "\n",
    "\n",
    "# _______________________________________________________________________\n",
    "def get_threshold(dict_prop, proportion=0.8):\n",
    "    \"\"\"\n",
    "    Calculate thresholds for prominence and distance based on a proportion of the minimum values.\n",
    "\n",
    "    Parameters:\n",
    "    - dict_prop (dict): Dictionary of properties containing 'prominences' and 'left_bases'.\n",
    "    - proportion (float): Proportion of the minimum value to use for threshold calculation (default=0.8).\n",
    "\n",
    "    Returns:\n",
    "    - list: Threshold values for prominence and distance.\n",
    "    \"\"\"\n",
    "    prominence_array = dict_prop[\"prominences\"][np.where(dict_prop[\"prominences\"] > 0)]\n",
    "    prominence_thresh = np.min(prominence_array) * proportion\n",
    "\n",
    "    distance_array = dict_prop[\"left_bases\"][np.where(dict_prop[\"left_bases\"] > 0)]\n",
    "    dist_thresh = np.min(distance_array) * proportion\n",
    "    return [prominence_thresh, dist_thresh]\n",
    "\n",
    "\n",
    "# _______________________________________________________________________\n",
    "def filter_extremes_per_year(input_df, date_col, value_col, extreme_type=\"peak\"):\n",
    "    \"\"\"\n",
    "    Filters the extremes (peaks or troughs) by year, retaining only the highest peak or lowest trough per year.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df (pd.DataFrame): DataFrame containing the time-series data (peaks or troughs).\n",
    "    - date_col (str): Column name for the date (datetime format).\n",
    "    - value_col (str): Column name for the values (peaks or troughs).\n",
    "    - extreme_type (str): Type of extreme ('peak' or 'trough'). Determines whether to retain highest or lowest value.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with the highest peak or lowest trough retained for each year.\n",
    "    \"\"\"\n",
    "    df = input_df.copy()\n",
    "    df[\"Year\"] = pd.DatetimeIndex(df[date_col]).year\n",
    "\n",
    "    # Select either the highest peak or the lowest trough by grouping by year\n",
    "    if extreme_type == \"peak\":\n",
    "        filtered_df = df.loc[df.groupby(\"Year\")[value_col].idxmax()]\n",
    "    elif extreme_type == \"trough\":\n",
    "        filtered_df = df.loc[df.groupby(\"Year\")[value_col].idxmin()]\n",
    "\n",
    "    return filtered_df.drop(columns=[\"Year\"])\n",
    "\n",
    "\n",
    "# _______________________________________________________________________\n",
    "\n",
    "\n",
    "def filter_extremes_by_range(input_df, date_col, value_col, extreme_type=\"peak\", months_range=2):\n",
    "    \"\"\"\n",
    "    Filters the extremes (peaks or troughs) within a given range of months around each year.\n",
    "    Retains only the highest peak or lowest trough within the search window, and ensures that no duplicate\n",
    "    peaks or troughs are selected in overlapping windows.\n",
    "\n",
    "    Parameters:\n",
    "    - input_df (pd.DataFrame): DataFrame containing the time-series data (peaks or troughs).\n",
    "    - date_col (str): Column name for the date (datetime format).\n",
    "    - value_col (str): Column name for the values (peaks or troughs).\n",
    "    - extreme_type (str): Type of extreme ('peak' or 'trough'). Determines whether to retain the highest or lowest value.\n",
    "    - months_range (int): Number of months before and after the current year to include in the search window.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame with the selected peaks or troughs for each window.\n",
    "    \"\"\"\n",
    "    df = input_df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "\n",
    "    # Create a column for Year and Month for easier manipulation\n",
    "    df[\"Year\"] = pd.DatetimeIndex(df[date_col]).year\n",
    "    df[\"Month\"] = pd.DatetimeIndex(df[date_col]).month\n",
    "\n",
    "    # Initialize list to hold selected extremes\n",
    "    selected_extremes = []\n",
    "\n",
    "    # Iterate over each year in the dataset\n",
    "    for year in sorted(df[\"Year\"].unique()[:-1]): # skip the last year\n",
    "        # Define the search window: year +/- x months\n",
    "        start_date = pd.Timestamp(year=year, month=1, day=1) - pd.DateOffset(months=months_range)\n",
    "        end_date = pd.Timestamp(year=year, month=12, day=31) + pd.DateOffset(months=months_range)\n",
    "\n",
    "        # Subset the data within the search window\n",
    "        window_df = df[(df[date_col] >= start_date) & (df[date_col] <= end_date)]\n",
    "\n",
    "        # If there is no data in the current window, skip this year\n",
    "        if window_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Select either the highest peak or the lowest trough\n",
    "        if extreme_type == \"peak\":\n",
    "            extreme_row = window_df.loc[window_df[value_col].idxmax()]\n",
    "        elif extreme_type == \"trough\":\n",
    "            extreme_row = window_df.loc[window_df[value_col].idxmin()]\n",
    "\n",
    "        # Append the selected extreme to the result list\n",
    "        selected_extremes.append(extreme_row)\n",
    "\n",
    "        # Remove the selected extreme from the original dataframe to avoid selecting it again in the next window\n",
    "        df = df.drop(extreme_row.name)\n",
    "\n",
    "    # Convert selected extremes to a DataFrame and return\n",
    "    selected_extremes_df = pd.DataFrame(selected_extremes).drop(columns=[\"Year\", \"Month\"])\n",
    "\n",
    "    return selected_extremes_df\n",
    "\n",
    "\n",
    "# _______________________________________________________________________\n",
    "def filter_consecutive(input_peaks, input_troughs, time_peak, value_peak, time_trough, value_trough):\n",
    "    \"\"\"\n",
    "    Filters out consecutive peaks or troughs, ensuring alternating patterns of peaks and troughs.\n",
    "\n",
    "    Parameters:\n",
    "    - input_peaks (pd.DataFrame): DataFrame containing peak data.\n",
    "    - input_troughs (pd.DataFrame): DataFrame containing trough data.\n",
    "    - time_peak (str): Column name for peak timestamps.\n",
    "    - value_peak (str): Column name for peak values.\n",
    "    - time_trough (str): Column name for trough timestamps.\n",
    "    - value_trough (str): Column name for trough values.\n",
    "\n",
    "    Returns:\n",
    "    - filtered_peaks, filtered_troughs (pd.DataFrame): Filtered DataFrames of peaks and troughs.\n",
    "    \"\"\"\n",
    "    peaks = input_peaks.copy()\n",
    "    troughs = input_troughs.copy()\n",
    "\n",
    "    # Combine peaks and troughs into one DataFrame, assign 'Type' column for identification\n",
    "    combined = pd.concat(\n",
    "        [\n",
    "            peaks[[time_peak, value_peak]].rename(columns={time_peak: \"Time\", value_peak: \"Value\"}).assign(Type=\"Peak\"),\n",
    "            troughs[[time_trough, value_trough]]\n",
    "            .rename(columns={time_trough: \"Time\", value_trough: \"Value\"})\n",
    "            .assign(Type=\"Trough\"),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    combined = combined.sort_values(by=\"Time\").reset_index(drop=True)\n",
    "\n",
    "    # Filter alternating peaks and troughs\n",
    "    filtered = []\n",
    "    last_type = None\n",
    "\n",
    "    for _, row in combined.iterrows():\n",
    "        curr_type = row[\"Type\"]\n",
    "        curr_value = row[\"Value\"]\n",
    "\n",
    "        if last_type is None or curr_type != last_type:\n",
    "            filtered.append(row)\n",
    "            last_type = curr_type\n",
    "        else:\n",
    "            if curr_type == \"Peak\" and curr_value > filtered[-1][\"Value\"]:\n",
    "                filtered[-1] = row\n",
    "            elif curr_type == \"Trough\" and curr_value < filtered[-1][\"Value\"]:\n",
    "                filtered[-1] = row\n",
    "\n",
    "    # Convert filtered list back to DataFrame, separate back into peaks and troughs\n",
    "    filtered_df = pd.DataFrame(filtered)\n",
    "\n",
    "    filtered_peaks = filtered_df[filtered_df[\"Type\"] == \"Peak\"].rename(columns={\"Time\": time_peak, \"Value\": value_peak})\n",
    "\n",
    "    filtered_troughs = filtered_df[filtered_df[\"Type\"] == \"Trough\"].rename(\n",
    "        columns={\"Time\": time_trough, \"Value\": value_trough}\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        filtered_peaks.drop(\"Type\", axis=1).reset_index(drop=True),\n",
    "        filtered_troughs.drop(\"Type\", axis=1).reset_index(drop=True),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c8de7b5-cfb5-4d71-ab21-b7ec5dc8bd54",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T16:43:44.186155Z",
     "iopub.status.busy": "2024-09-16T16:43:44.186155Z",
     "iopub.status.idle": "2024-09-16T16:43:44.847155Z",
     "shell.execute_reply": "2024-09-16T16:43:44.847155Z",
     "shell.execute_reply.started": "2024-09-16T16:43:44.186155Z"
    }
   },
   "outputs": [],
   "source": [
    "# _______________________________________________________________________\n",
    "# Load HDF5 File and Extract Dataset Information\n",
    "hdf5_fpath = r\"20240903_GWL_CRFP.h5\"\n",
    "\n",
    "with h5py.File(hdf5_fpath, \"r\") as hdf5_file:\n",
    "    # Extract existing data and available datasets\n",
    "    existing_data_dict = h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "    available_datasets = h5pytools.list_datasets(hdf5_file)\n",
    "\n",
    "    # Extract the 'date' array and convert to a datetime index\n",
    "    date_strings = [date.decode(\"utf-8\") for date in existing_data_dict[\"date\"]]\n",
    "    datetime_array = pd.to_datetime(date_strings, format=\"%Y%m%d\")\n",
    "\n",
    "# _______________________________________________________________________\n",
    "# Extract the list of stations and wellcodes for processing\n",
    "stations = sorted(set([elem.split(\"/\")[0] for elem in available_datasets if \"date\" not in elem]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00bb8757-5a99-4357-9b93-91516f779fae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-17T02:25:54.170503Z",
     "iopub.status.busy": "2024-09-17T02:25:54.170503Z",
     "iopub.status.idle": "2024-09-17T02:26:34.991502Z",
     "shell.execute_reply": "2024-09-17T02:26:34.990503Z",
     "shell.execute_reply.started": "2024-09-17T02:25:54.170503Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 21/21 [00:40<00:00,  1.94s/it]\n"
     ]
    }
   ],
   "source": [
    "error_station = []\n",
    "\n",
    "# station = \"DONGFANG\"\n",
    "for station in tqdm(stations[::5]):\n",
    "    try:\n",
    "\n",
    "        # _______________________________________________________________________\n",
    "        # Extract and process station data\n",
    "        station_data = existing_data_dict[station]\n",
    "        wellcodes = [elem for elem, val in station_data.items() if isinstance(val, dict) and len(val) == 3]\n",
    "        # wellcode = wellcodes[1]\n",
    "        for wellcode in wellcodes:\n",
    "\n",
    "            # _______________________________________________________________________\n",
    "            # Extract time-series data and process the valid data range\n",
    "            model_gwl_arr = station_data[wellcode][\"measure\"][\"model\"]\n",
    "            model_gwl_series = pd.Series(data=model_gwl_arr, index=datetime_array)\n",
    "            valid_gwl_series = model_gwl_series.loc[\n",
    "                model_gwl_series.first_valid_index() : model_gwl_series.last_valid_index()\n",
    "            ]\n",
    "\n",
    "            if valid_gwl_series.index.year.unique().size > 2:\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "                # Apply smoothing on the time series\n",
    "                smoothed_series = smoothing.simple_moving_average(num_arr=valid_gwl_series, window_size=61)\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "                # Detect peaks and troughs\n",
    "                peaks, properties_peaks, peak_times, troughs, properties_troughs, trough_times = detect_peaks_troughs(\n",
    "                    signal=smoothed_series.values, time_index=smoothed_series.index\n",
    "                )\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "                # Calculate thresholds for peaks and troughs\n",
    "                peak_prom, peak_dist = get_threshold(properties_peaks, 1.2)\n",
    "                trough_prom, trough_dist = get_threshold(properties_troughs, 0.5)\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "                # Redetect peaks and troughs based on thresholds\n",
    "                peaks, _ = scipy.signal.find_peaks(smoothed_series.values, prominence=peak_prom, distance=peak_dist)\n",
    "                peak_times = smoothed_series.index[peaks]\n",
    "                troughs, _ = scipy.signal.find_peaks(\n",
    "                    -smoothed_series.values, prominence=trough_prom, distance=trough_dist\n",
    "                )\n",
    "                trough_times = smoothed_series.index[troughs]\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "                # Convert to DataFrames\n",
    "                signal_peaks = pd.DataFrame(data={\"time\": peak_times, \"value\": valid_gwl_series[peak_times].values})\n",
    "                signal_troughs = pd.DataFrame(\n",
    "                    data={\"time\": trough_times, \"value\": valid_gwl_series[trough_times].values}\n",
    "                )\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "                # Filter extremes per year for peaks and troughs\n",
    "                # filtered_peaks = filter_extremes_per_year(\n",
    "                #     signal_peaks, date_col=\"time\", value_col=\"value\", extreme_type=\"peak\"\n",
    "                # )\n",
    "                filtered_peaks = filter_extremes_by_range(\n",
    "                    signal_peaks, date_col=\"time\", value_col=\"value\", extreme_type=\"peak\", months_range=2\n",
    "                )\n",
    "                # filtered_troughs = filter_extremes_per_year(\n",
    "                #     signal_troughs, date_col=\"time\", value_col=\"value\", extreme_type=\"trough\"\n",
    "                # )\n",
    "                filtered_troughs = filter_extremes_by_range(\n",
    "                    signal_troughs, date_col=\"time\", value_col=\"value\", extreme_type=\"trough\", months_range=2\n",
    "                )\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "                # Apply filtering for consecutive peaks and troughs\n",
    "                final_peaks, final_troughs = filter_consecutive(\n",
    "                    input_peaks=filtered_peaks,\n",
    "                    input_troughs=filtered_troughs,\n",
    "                    time_peak=\"time\",\n",
    "                    value_peak=\"value\",\n",
    "                    time_trough=\"time\",\n",
    "                    value_trough=\"value\",\n",
    "                )\n",
    "\n",
    "                # savepath = f\"temp003\\\\{station}_{wellcode}.xlsx\"\n",
    "                # if not os.path.isfile(savepath):\n",
    "                #     final_peaks.to_excel(savepath, index=False, sheet_name=\"peaks\")\n",
    "                #     data_io.save_df_to_excel(\n",
    "                #         df_to_save=final_troughs, filepath=savepath, sheet_name=\"troughs\", verbose=False\n",
    "                #     )\n",
    "                # else:\n",
    "                #     print(\"Target file has already existed!\")\n",
    "                #     continue\n",
    "\n",
    "                # _______________________________________________________________________\n",
    "\n",
    "                fig_width, fig_height = (11.7 * 3 / 2, 8.3 * 2 / 3)\n",
    "                fig = plt.figure(figsize=(fig_width, fig_height))\n",
    "\n",
    "                ax = fig.add_subplot(111)\n",
    "                # _______________________________________________________________________\n",
    "                # Plot the valid groundwater level series\n",
    "                ax.plot(valid_gwl_series, color=\"black\", zorder=1)\n",
    "\n",
    "                # Plot all detected peaks (in grey with transparency)\n",
    "                ax.plot(\n",
    "                    signal_peaks.set_index(\"time\"),\n",
    "                    marker=\"o\",\n",
    "                    linestyle=\" \",\n",
    "                    markersize=14,\n",
    "                    zorder=1,\n",
    "                    color=\"none\",\n",
    "                    markeredgecolor=\"black\",\n",
    "                    alpha=0.5,\n",
    "                )\n",
    "\n",
    "                # Plot filtered peaks (by year, in blue)\n",
    "                ax.plot(\n",
    "                    filtered_peaks.set_index(\"time\"),\n",
    "                    marker=\"s\",\n",
    "                    linestyle=\" \",\n",
    "                    markersize=12,\n",
    "                    zorder=2,\n",
    "                    color=\"blue\",\n",
    "                    alpha=0.2,\n",
    "                )\n",
    "\n",
    "                # Plot final filtered peaks (after consecutive filtering, in lime green)\n",
    "                ax.plot(\n",
    "                    final_peaks.set_index(\"time\"),\n",
    "                    marker=\"^\",\n",
    "                    linestyle=(0, (1, 2)),\n",
    "                    markersize=10,\n",
    "                    zorder=3,\n",
    "                    color=\"lime\",\n",
    "                    markeredgecolor=\"black\",\n",
    "                    alpha=1,\n",
    "                    label=\"Peaks\",\n",
    "                )\n",
    "\n",
    "                # Plot all detected troughs (in grey with transparency)\n",
    "                ax.plot(\n",
    "                    signal_troughs.set_index(\"time\"),\n",
    "                    marker=\"o\",\n",
    "                    linestyle=\" \",\n",
    "                    markersize=14,\n",
    "                    color=\"none\",\n",
    "                    markeredgecolor=\"black\",\n",
    "                    zorder=1,\n",
    "                    alpha=0.5,\n",
    "                )\n",
    "\n",
    "                # Plot filtered troughs (by year, in red)\n",
    "                ax.plot(\n",
    "                    filtered_troughs.set_index(\"time\"),\n",
    "                    marker=\"s\",\n",
    "                    linestyle=\" \",\n",
    "                    markersize=12,\n",
    "                    color=\"darkorange\",\n",
    "                    zorder=2,\n",
    "                    alpha=0.2,\n",
    "                )\n",
    "\n",
    "                # Plot final filtered troughs (after consecutive filtering, in magenta)\n",
    "                ax.plot(\n",
    "                    final_troughs.set_index(\"time\"),\n",
    "                    marker=\"^\",\n",
    "                    linestyle=(0, (1, 2)),\n",
    "                    markersize=10,\n",
    "                    color=\"magenta\",\n",
    "                    markeredgecolor=\"black\",\n",
    "                    zorder=3,\n",
    "                    alpha=1,\n",
    "                    label=\"Troughs\",\n",
    "                )\n",
    "\n",
    "                # Configure datetime ticks for the x-axis\n",
    "                visualize.configure_axis(\n",
    "                    ax=ax,\n",
    "                    xlabel=\"\",\n",
    "                    ylabel=\"Groundwater Levels (m)\",\n",
    "                    scaling_factor=1.2,\n",
    "                    title=f\"{station} - {wellcode}\",\n",
    "                )\n",
    "                visualize.configure_datetime_ticks(ax=ax, axis=\"x\")\n",
    "                visualize.configure_legend(ax=ax, scaling_factor=1, frameon=False, fontsize_base=12)\n",
    "\n",
    "                # Add grid and set layout for better readability\n",
    "                ax.grid(axis=\"x\", which=\"major\", linestyle=\"-\", linewidth=1, color=\"grey\")\n",
    "                ax.grid(axis=\"x\", which=\"minor\", linestyle=\"--\", linewidth=1, color=\"lightgrey\")\n",
    "                ax.set_axisbelow(True)\n",
    "                ax.set_xlim(datetime(2000, 1, 1), datetime(2025, 1, 1))\n",
    "\n",
    "                # Optimize layout and show the plot\n",
    "                fig.tight_layout()\n",
    "                # Apply rotation to the x-tick labels on the shared axis (ax3)\n",
    "                plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "                # Optionally, you can save the figure as well\n",
    "                fld2savefig = \"temp001_4\"\n",
    "                os.makedirs(fld2savefig, exist_ok=True)\n",
    "                output_fig_path = os.path.join(fld2savefig, f\"{station}_{wellcode}.png\")\n",
    "                visualize.save_figure(fig, output_fig_path)\n",
    "                plt.close()\n",
    "    except Exception as e:\n",
    "        error_station.append(f\"{station}_{wellcode}\")\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8e27c4-e8f2-4041-a4a1-982584ad1064",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-16T16:43:44.866157Z",
     "iopub.status.idle": "2024-09-16T16:43:44.866157Z",
     "shell.execute_reply": "2024-09-16T16:43:44.866157Z"
    }
   },
   "outputs": [],
   "source": [
    "error_station"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6a999b3b-fb57-4bd6-8183-2980229cdb8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T13:19:45.259959Z",
     "iopub.status.busy": "2024-09-16T13:19:45.259959Z",
     "iopub.status.idle": "2024-09-16T13:19:45.710958Z",
     "shell.execute_reply": "2024-09-16T13:19:45.710958Z",
     "shell.execute_reply.started": "2024-09-16T13:19:45.259959Z"
    }
   },
   "source": [
    "signal_peaks = pd.DataFrame(data={\"time\": peak_times, \"value\": valid_gwl_series[peak_times].values})\n",
    "signal_troughs = pd.DataFrame(data={\"time\": trough_times, \"value\": valid_gwl_series[trough_times].values})\n",
    "\n",
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(valid_gwl_series, color=\"black\", zorder=2)\n",
    "ax.plot(signal_peaks.set_index(\"time\"), marker=\"o\", linestyle=\" \", markersize=10, color=\"deepskyblue\", zorder=1)\n",
    "ax.plot(signal_troughs.set_index(\"time\"), marker=\"o\", linestyle=\" \", markersize=10, color=\"orange\", zorder=1)\n",
    "visualize.configure_datetime_ticks(ax=ax, axis=\"x\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9faf4ff1-036b-409b-85c8-6541de88284d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T13:22:39.288740Z",
     "iopub.status.busy": "2024-09-16T13:22:39.287740Z",
     "iopub.status.idle": "2024-09-16T13:22:39.677769Z",
     "shell.execute_reply": "2024-09-16T13:22:39.677769Z",
     "shell.execute_reply.started": "2024-09-16T13:22:39.288740Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(valid_gwl_series, color='black', zorder=2)\n",
    "ax.plot(signal_peaks.set_index(\"time\"), marker='o', linestyle=' ', markersize=10, color='deepskyblue', zorder=1)\n",
    "ax.plot(signal_troughs.set_index(\"time\"), marker='o', linestyle=' ', markersize=10, color='orange', zorder=1)\n",
    "visualize.configure_datetime_ticks(ax=ax, axis=\"x\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "28614da5-7753-4437-bb88-1e0649282da9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-16T13:22:58.286740Z",
     "iopub.status.busy": "2024-09-16T13:22:58.286740Z",
     "iopub.status.idle": "2024-09-16T13:22:58.617741Z",
     "shell.execute_reply": "2024-09-16T13:22:58.617741Z",
     "shell.execute_reply.started": "2024-09-16T13:22:58.286740Z"
    }
   },
   "source": [
    "fig = plt.figure(figsize=(20, 6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(valid_gwl_series, color=\"black\", zorder=2)\n",
    "\n",
    "ax.plot(\n",
    "    signal_peaks.set_index(\"time\"), marker=\"o\", linestyle=\" \", markersize=10, zorder=1, color=\"deepskyblue\", alpha=0.3\n",
    ")\n",
    "ax.plot(filtered_peaks.set_index(\"time\"), marker=\"s\", linestyle=\" \", markersize=10, zorder=1, color=\"blue\")\n",
    "\n",
    "ax.plot(signal_troughs.set_index(\"time\"), marker=\"o\", linestyle=\" \", markersize=10, color=\"orange\", zorder=1, alpha=0.3)\n",
    "ax.plot(filtered_troughs.set_index(\"time\"), marker=\"s\", linestyle=\" \", markersize=10, color=\"red\", zorder=1)\n",
    "\n",
    "visualize.configure_datetime_ticks(ax=ax, axis=\"x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d9d3f-9186-44d8-931c-cbada410fc48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee9857e-8afe-4242-a1f3-8958f7c38be3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
