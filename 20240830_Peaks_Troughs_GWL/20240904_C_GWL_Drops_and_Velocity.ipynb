{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa58957a-2fb7-4bd6-b864-f7c3f7231fae",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-05T16:18:34.907418Z",
     "iopub.status.busy": "2024-09-05T16:18:34.907418Z",
     "iopub.status.idle": "2024-09-05T16:18:37.429033Z",
     "shell.execute_reply": "2024-09-05T16:18:37.428033Z",
     "shell.execute_reply.started": "2024-09-05T16:18:34.907418Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<dask.config.set at 0x2856b329070>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask\n",
    "import dask.dataframe as dd\n",
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from tqdm.notebook import tqdm  # Use tqdm for notebooks\n",
    "\n",
    "dask.config.set(scheduler=\"processes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a0cfd2a-65a4-4e68-b4bf-275491a208c9",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-05T16:18:37.430033Z",
     "iopub.status.busy": "2024-09-05T16:18:37.430033Z",
     "iopub.status.idle": "2024-09-05T16:18:37.443863Z",
     "shell.execute_reply": "2024-09-05T16:18:37.443031Z",
     "shell.execute_reply.started": "2024-09-05T16:18:37.430033Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def extract_data_from_hdf5(file_path):\n",
    "    \"\"\"\n",
    "    Extracts data and metadata from an HDF5 file.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the HDF5 file.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: (data_dict, insar_datetime) extracted from the file.\n",
    "    \"\"\"\n",
    "    with h5py.File(file_path, \"r\") as hdf5_file:\n",
    "        data_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "        insar_datetime_arr = data_dict[\"InSAR_datetime\"]\n",
    "        insar_datetime = pd.to_datetime(insar_datetime_arr, format=\"%Y%m%d\")\n",
    "    return data_dict, insar_datetime\n",
    "\n",
    "\n",
    "# ________________________________________________________________________\n",
    "def create_numeric_timerange(insar_datetime):\n",
    "    \"\"\"\n",
    "    Creates a full time range and corresponding numeric time indices.\n",
    "\n",
    "    Parameters:\n",
    "    - insar_datetime (pd.Series): Series of InSAR datetime values.\n",
    "\n",
    "    Returns:\n",
    "    - pd.Series: Numeric full time range series indexed by full time range dates.\n",
    "    \"\"\"\n",
    "    # Create a continuous datetime range and corresponding numeric indices\n",
    "    full_timerange = datetime_handle.get_fulltime(insar_datetime)\n",
    "    numeric_full_timerange = datetime_handle.numeric_time_index(full_timerange)\n",
    "    numeric_full_timerange_series = pd.Series(data=numeric_full_timerange, index=full_timerange)\n",
    "    # Extract numeric indices for the InSAR datetimes\n",
    "    cdisp_numeric_timerange = numeric_full_timerange_series[insar_datetime]\n",
    "    return cdisp_numeric_timerange\n",
    "\n",
    "\n",
    "# ________________________________________________________________________\n",
    "def peaktrough_byWellCode(well_data):\n",
    "    \"\"\"\n",
    "    Creates a DataFrame from peak and trough data for a specific well.\n",
    "\n",
    "    Parameters:\n",
    "    - well_data (dict): Dictionary containing well data.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame with peak and trough times and values.\n",
    "    \"\"\"\n",
    "    # Extract peak and trough data\n",
    "    peaks = well_data.get(\"peaks\", {})\n",
    "    troughs = well_data.get(\"troughs\", {})\n",
    "\n",
    "    peak_date = peaks.get(\"date\", [])\n",
    "    peak_value = peaks.get(\"value\", [])\n",
    "    trough_date = troughs.get(\"date\", [])\n",
    "    trough_value = troughs.get(\"value\", [])\n",
    "\n",
    "    # Create DataFrame using extracted data\n",
    "    peaktrough_df = pd.DataFrame(\n",
    "        {\n",
    "            \"peak_time\": pd.to_datetime(peak_date, format=\"%Y%m%d\"),\n",
    "            \"peaks\": peak_value,\n",
    "            \"trough_time\": pd.to_datetime(trough_date, format=\"%Y%m%d\"),\n",
    "            \"troughs\": trough_value,\n",
    "        }\n",
    "    )\n",
    "    return peaktrough_df\n",
    "\n",
    "\n",
    "# ________________________________________________________________________\n",
    "def get_average_velocity(x, y):\n",
    "    \"\"\"\n",
    "    Computes the average velocity from x and y data using polynomial trend analysis.\n",
    "\n",
    "    Parameters:\n",
    "    - x (array-like): Array of x values (time indices).\n",
    "    - y (array-like): Array of y values (displacement data).\n",
    "\n",
    "    Returns:\n",
    "    - float: The average velocity.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        _, coeff = analysis.get_polynomial_trend(x, y, 1)\n",
    "        return coeff[-1] * 365.25\n",
    "    except ValueError as e:\n",
    "        print(f\"Error computing velocity for y={y}: {e}\")\n",
    "        return np.nan  # Return NaN if polynomial fitting fails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62d9d7a-da33-47dd-bd9c-6b48646ecd5f",
   "metadata": {
    "editable": true,
    "execution": {
     "iopub.execute_input": "2024-09-05T16:18:37.445906Z",
     "iopub.status.busy": "2024-09-05T16:18:37.444380Z",
     "iopub.status.idle": "2024-09-05T16:18:37.911766Z",
     "shell.execute_reply": "2024-09-05T16:18:37.911766Z",
     "shell.execute_reply.started": "2024-09-05T16:18:37.445906Z"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Main script execution\n",
    "gwl_hdf5_file = \"20240903_GWL_CRFP.h5\"\n",
    "\n",
    "# Extract data from the HDF5 file\n",
    "existing_data_dict, insar_datetime = extract_data_from_hdf5(gwl_hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ce14fe2a-821d-40e0-97e1-5b26b549de07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-05T16:23:57.418431Z",
     "iopub.status.busy": "2024-09-05T16:23:57.417452Z",
     "iopub.status.idle": "2024-09-05T16:26:52.498827Z",
     "shell.execute_reply": "2024-09-05T16:26:52.497820Z",
     "shell.execute_reply.started": "2024-09-05T16:23:57.418431Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Stations:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Wellcodes in ANHE:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# _____________________ SECTION: Initialize Processed Data _____________________________\n",
    "\n",
    "# Initialize the processed data dictionary that will store results for each station\n",
    "processed_data = {}\n",
    "\n",
    "error_stations = []\n",
    "\n",
    "# Extract the available GWL stations from the dictionary keys (excluding 'date' keys)\n",
    "available_stations = [elem for elem in existing_data_dict.keys() if \"date\" not in elem]\n",
    "\n",
    "# _____________________ SECTION: Main Loop - Processing Stations _____________________________\n",
    "\n",
    "\n",
    "# Iterate over each station to process data\n",
    "for select_station in tqdm(available_stations, desc=\"Processing Stations\", leave=False):\n",
    "\n",
    "    # Extract station data from the existing data dictionary\n",
    "    station_data = existing_data_dict[select_station]\n",
    "    # processed_data[select_station] = {}\n",
    "    processed_data_byStation = {}\n",
    "\n",
    "    # Extract wellcodes that contain dictionaries and have exactly 3 items\n",
    "    wellcode_byStation = [elem for elem, val in station_data.items() if isinstance(val, dict) and len(val) == 3]\n",
    "\n",
    "    # _____________________ SECTION: Inner Loop - Processing Wellcodes _____________________________\n",
    "\n",
    "    # Iterate over each wellcode within the station\n",
    "    for select_wellcode in tqdm(\n",
    "        wellcode_byStation,\n",
    "        desc=f\"Processing Wellcodes in {select_station}\",\n",
    "        leave=False,\n",
    "    ):\n",
    "        # Extract well data\n",
    "        well_data = station_data[select_wellcode]\n",
    "        \n",
    "        # Create DataFrame using extracted peak and trough data\n",
    "        peaktrough_df = peaktrough_byWellCode(well_data)\n",
    "\n",
    "        # Extract CDISP array and create DataFrame\n",
    "        cdisp_array = station_data.get(\"CDISP\", [])\n",
    "        cdisp_df = pd.DataFrame(data=cdisp_array, columns=insar_datetime)\n",
    "\n",
    "        # Initialize dictionary to store velocity data\n",
    "        velocity_dict = {\n",
    "            \"date\": [],\n",
    "            \"abs_difference\": [],\n",
    "            \"linear_velocity\": [],\n",
    "        }\n",
    "\n",
    "        _velocity_cache = None\n",
    "\n",
    "        # ____________________ SECTION: Processing Peak-Trough Pairs ____________________\n",
    "\n",
    "        # Calculate velocity for each peak-trough pair\n",
    "        for row in peaktrough_df.itertuples(index=False):\n",
    "            peak_time, peak, trough_time, trough = (\n",
    "                row.peak_time,\n",
    "                row.peaks,\n",
    "                row.trough_time,\n",
    "                row.troughs,\n",
    "            )\n",
    "\n",
    "            # Define time range for displacement data extraction\n",
    "            start_time, end_time = min(peak_time, trough_time), max(peak_time, trough_time)\n",
    "            temp = cdisp_df.loc[:, start_time:end_time]\n",
    "\n",
    "            if not temp.empty:\n",
    "                # Use Dask to compute velocities across the time range\n",
    "                temp_dask = dd.from_pandas(temp, npartitions=20)\n",
    "                cdisp_numeric_timerange = create_numeric_timerange(temp.columns)\n",
    "\n",
    "                meta = pd.Series(dtype=\"float64\")\n",
    "                velocities_dask = temp_dask.map_partitions(\n",
    "                    lambda df: df.apply(\n",
    "                        lambda y: get_average_velocity(cdisp_numeric_timerange.values, y),\n",
    "                        axis=1,\n",
    "                    ),\n",
    "                    meta=meta,\n",
    "                ).compute()\n",
    "\n",
    "                # Ensure the computed velocity data is not empty\n",
    "                if not velocities_dask.empty:\n",
    "                    # Store dates and differences in the dictionary\n",
    "                    velocity_dict[\"date\"].append(f\"{start_time.strftime('%Y%m%d')}_{end_time.strftime('%Y%m%d')}\")\n",
    "                    velocity_dict[\"abs_difference\"].append(abs(peak - trough))\n",
    "\n",
    "                    # Simplified velocity cache update logic\n",
    "                    velocities_np = velocities_dask.to_numpy().reshape(1, -1)\n",
    "                    _velocity_cache = (\n",
    "                        np.append(_velocity_cache, velocities_np, axis=0)\n",
    "                        if _velocity_cache is not None\n",
    "                        else velocities_np\n",
    "                    )\n",
    "\n",
    "        velocity_dict[\"linear_velocity\"] = _velocity_cache\n",
    "        processed_data_byStation[select_wellcode] = {\"peaks_troughs\": velocity_dict}\n",
    "\n",
    "    try:\n",
    "        # ___________________ SECTION: Save Processed Data ______________________\n",
    "        # Write updated data and metadata back to a new HDF5 file\n",
    "        savename = f\"{datetime.now().strftime('%Y%m%d')}_{select_station}.h5\"\n",
    "        savepath = os.path.join(\"temp\", savename)\n",
    "        with h5py.File(savepath, \"w\") as hdf5_file:\n",
    "            gwatertools.h5pytools.data_to_hdf5(hdf5_file, processed_data_byStation)\n",
    "        processed_data.update({select_station:processed_data_byStation})\n",
    "    except Exception as e:\n",
    "        error_stations.append(select_station)\n",
    "        pass\n",
    "\n",
    "# Processed data is now stored in processed_data dictionary for further use or analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a80f2-6b66-4560-b69c-afac219460fe",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-05T14:14:48.578398Z",
     "iopub.status.idle": "2024-09-05T14:14:48.578398Z",
     "shell.execute_reply": "2024-09-05T14:14:48.578398Z",
     "shell.execute_reply.started": "2024-09-05T14:14:48.578398Z"
    }
   },
   "outputs": [],
   "source": [
    "# ________________________ SECTION: Final Output Write __________________________\n",
    "\n",
    "gwl_hdf5_file = \"20240904_GWL_Drop_and_Velocity.h5\"\n",
    "\n",
    "# # Copy the original HDF5 file to a secure version\n",
    "# shutil.copy2(src=gwl_hdf5_file, dst=gwl_hdf5_file.replace(\".h5\", \"_secure.h5\"))\n",
    "\n",
    "# Extract existing data and metadata\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    existing_data_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "    existing_metadata_dict = gwatertools.h5pytools.hdf5_to_metadata_dict(\n",
    "        hdf5_file\n",
    "    )\n",
    "\n",
    "# Update dictionaries with new measurement data\n",
    "updated_data_dict = gwatertools.h5pytools.update_data_dict(\n",
    "    existing_data_dict, processed_data\n",
    ")\n",
    "\n",
    "# Write updated data and metadata back to a new HDF5 file\n",
    "output_file_name = f\"{datetime.now().strftime('%Y%m%d')}_GWL_Drop_and_Velocity.h5\"\n",
    "with h5py.File(output_file_name, \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.metadata_to_hdf5(hdf5_file, existing_metadata_dict)\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, updated_data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
