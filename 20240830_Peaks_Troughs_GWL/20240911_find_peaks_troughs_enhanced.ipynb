{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bab2499-62e3-403f-8edf-4371d6b61f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from signaltools import *\n",
    "\n",
    "\n",
    "# ______________________________________________________________________\n",
    "def analyze_time_series(\n",
    "    series, prominence=None, smoothing_window=None, polyorder=3, min_distance=None, detrend_degree=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Analyzes a time series for peaks and troughs with detrending, smoothing, and filtering.\n",
    "\n",
    "    Parameters:\n",
    "        series (pd.Series): Time-series data with a datetime index.\n",
    "        prominence (float): Prominence value for peak detection. If None, computed dynamically.\n",
    "        smoothing_window (int): Window length for Savitzky-Golay smoothing. If None, computed dynamically.\n",
    "        polyorder (int): Polynomial order for Savitzky-Golay smoothing.\n",
    "        min_distance (int): Minimum distance between alternating peaks and troughs. If None, computed dynamically.\n",
    "        detrend_degree (int): Degree of polynomial for detrending the series.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Detected peaks, troughs, smoothed and original data, and dates.\n",
    "    \"\"\"\n",
    "    original_data = series.values\n",
    "    dates = series.index\n",
    "\n",
    "    # Step 1: Detrend the series\n",
    "    trend, detrended_series = get_seasonal_and_trend_data(series, detrend_degree)\n",
    "\n",
    "    # Step 2: Smooth the detrended data\n",
    "    smoothing_window = smoothing_window or compute_smoothing_window(len(detrended_series))\n",
    "    smoothed_series = smooth_time_series(detrended_series, smoothing_window, polyorder)\n",
    "\n",
    "    # Step 3: Detect peaks and troughs\n",
    "    min_distance = min_distance or compute_dynamic_distance(series)\n",
    "    prominence = prominence or compute_dynamic_prominence(series)\n",
    "    peaks, troughs = detect_peaks_troughs(smoothed_series, prominence, distance=min_distance)\n",
    "\n",
    "    # Step 4: Refine peaks and troughs\n",
    "    refined_peaks, refined_troughs = refine_peaks_troughs(peaks, troughs, smoothed_series, min_distance)\n",
    "\n",
    "    # Final output\n",
    "    final_peaks = pd.Series(original_data[refined_peaks], index=dates[refined_peaks])\n",
    "    final_troughs = pd.Series(original_data[refined_troughs], index=dates[refined_troughs])\n",
    "\n",
    "    smoothed_series = smoothed_series + trend\n",
    "\n",
    "    return refined_peaks, refined_troughs, final_peaks, final_troughs, smoothed_series, original_data, dates\n",
    "\n",
    "\n",
    "# ______________________________________________________________________\n",
    "def plot_results(dates, original_data, smoothed_data, peaks, troughs, title):\n",
    "    \"\"\"\n",
    "    Plots time-series data with detected peaks and troughs, displaying the corresponding indexes.\n",
    "\n",
    "    Parameters:\n",
    "        dates (pd.DatetimeIndex): Dates corresponding to the data.\n",
    "        original_data (np.array): Original time-series data.\n",
    "        smoothed_data (np.array): Smoothed time-series data.\n",
    "        peaks (list): Detected peaks indices.\n",
    "        troughs (list): Detected troughs indices.\n",
    "        title (str): Plot title.\n",
    "\n",
    "    Returns:\n",
    "        matplotlib.figure.Figure: The figure object for saving or further processing.\n",
    "    \"\"\"\n",
    "    # Set the figure size and calculate the scaling factor\n",
    "    fig_width, fig_height = visualize.BASE_SIZE\n",
    "    scaling_factor = visualize.calculate_scaling_factor(fig_width, fig_height)\n",
    "\n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(fig_width * 1.5, fig_height))\n",
    "\n",
    "    # Plot the original data and smoothed data\n",
    "    ax.plot(dates, original_data, label=\"Original Data\", color=\"grey\", alpha=1, zorder=1)\n",
    "    ax.plot(dates, smoothed_data, label=\"Smoothed Data\", color=\"blue\", linewidth=1.5, alpha=0.8, zorder=1)\n",
    "\n",
    "    # Plot peaks and troughs using scatter plots\n",
    "    ax.scatter(dates[peaks], original_data[peaks], color=\"green\", marker=\"s\", label=\"Peaks\", s=70, alpha=0.5, zorder=2)\n",
    "    ax.scatter(dates[peaks], original_data[peaks], color=\"black\", marker=\"o\", s=10, linewidth=0, alpha=0.5, zorder=2)\n",
    "    ax.scatter(dates[troughs], original_data[troughs], color=\"red\", marker=\"s\", label=\"Troughs\", s=70, alpha=0.5, zorder=2)\n",
    "    ax.scatter(dates[troughs], original_data[troughs], color=\"black\", marker=\"o\", s=10, linewidth=0, alpha=0.5, zorder=2)\n",
    "\n",
    "    # Annotate peaks with their corresponding indexes above the markers\n",
    "    for i in range(len(peaks)):\n",
    "        ax.text(\n",
    "            dates[peaks[i]],\n",
    "            original_data[peaks[i]] + 0.5,\n",
    "            str(peaks[i]),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=\"vertical\",\n",
    "            fontsize=10 * scaling_factor,\n",
    "        )\n",
    "\n",
    "    # Annotate troughs with their corresponding indexes below the markers\n",
    "    for i in range(len(troughs)):\n",
    "        ax.text(\n",
    "            dates[troughs[i]],\n",
    "            original_data[troughs[i]] - 0.5,\n",
    "            str(troughs[i]),\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            rotation=\"vertical\",\n",
    "            fontsize=10 * scaling_factor,\n",
    "        )\n",
    "\n",
    "    # Configure the axis labels, title, and font scaling using visualize.py\n",
    "    visualize.configure_axis(\n",
    "        ax=ax, xlabel=\"Date\", ylabel=\"Groundwater Levels (m)\", title=title, scaling_factor=scaling_factor\n",
    "    )\n",
    "\n",
    "    # Configure the legend\n",
    "    visualize.configure_legend(ax=ax, scaling_factor=0.5, frameon=False)\n",
    "\n",
    "    # Configure ticks (datetime for x-axis)\n",
    "    visualize.configure_datetime_ticks(ax=ax, axis=\"x\", major_interval=12, minor_interval=6, date_format=\"%Y\")\n",
    "\n",
    "    # Adjust layout and set x-axis label rotation\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19cfdc8-2c3c-4be5-98c7-5a7dab135416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ______________________________________________________________________\n",
    "# Step 1: Load the HDF5 File and Extract Dataset Information\n",
    "hdf5_fpath = r\"20240903_GWL_CRFP.h5\"\n",
    "\n",
    "with h5py.File(hdf5_fpath, \"r\") as hdf5_file:\n",
    "    \"\"\"\n",
    "    Load HDF5 file to extract the time series data and available datasets.\n",
    "\n",
    "    - existing_data_dict: Dictionary containing all datasets from the HDF5 file.\n",
    "    - available_datasets: List of dataset paths in the HDF5 file.\n",
    "    - datetime_array: Array of datetime objects parsed from the \"date\" field in the dataset.\n",
    "    \"\"\"\n",
    "    existing_data_dict = h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "    available_datasets = h5pytools.list_datasets(hdf5_file)\n",
    "\n",
    "    # Extract the 'date' array and convert to a datetime index\n",
    "    datetime_array = pd.to_datetime(existing_data_dict[\"date\"], format=\"%Y%m%d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedf9675-529a-4977-9c15-062688d7e28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = pd.DataFrame(data=None, index=None)\n",
    "\n",
    "# ======================================================================\n",
    "#                        Main Program: Data Preprocessing\n",
    "# ======================================================================\n",
    "\n",
    "# ______________________________________________________________________\n",
    "# Step 2: Identify the Stations and Wellcodes in the Dataset\n",
    "\"\"\"\n",
    "Process the dataset to identify stations and wellcodes:\n",
    "- stations: Extract unique station names from dataset paths.\n",
    "- wellcode_byStation: List of wellcodes corresponding to a station, filtered by the number of dictionary items.\n",
    "\"\"\"\n",
    "stations = sorted(set([elem.split(\"/\")[0] for elem in available_datasets if \"date\" not in elem]))\n",
    "\n",
    "# Loop over each station in the dataset\n",
    "for select_station in tqdm(stations, desc=\"Processing Stations\"):\n",
    "\n",
    "    # Extract the station's data from the dictionary\n",
    "    station_data = existing_data_dict[select_station]\n",
    "\n",
    "    # Filter wellcodes that contain exactly 3 items (indicating wellcode structure)\n",
    "    wellcode_byStation = [elem for elem, val in station_data.items() if isinstance(val, dict) and len(val) == 3]\n",
    "\n",
    "    # Loop over each wellcode in the station's data\n",
    "    for select_wellcode in tqdm(wellcode_byStation, leave=False):\n",
    "\n",
    "        output_savename = f\"{select_station}_{select_wellcode}\"\n",
    "\n",
    "        # ======================================================================\n",
    "        #                        Main Program: Time Series Analysis\n",
    "        # ======================================================================\n",
    "\n",
    "        # ______________________________________________________________________\n",
    "        # Step 3: Extract Model GWL Data and Convert to Time Series\n",
    "        \"\"\"\n",
    "        Extract the 'model' field for the selected station and wellcode, and convert it into a pandas time series.\n",
    "        - model_gwl_arr: Array of groundwater level (GWL) model values.\n",
    "        - model_gwl_series: Time-series representation of the model GWL data.\n",
    "        \"\"\"\n",
    "        model_gwl_arr = station_data[select_wellcode][\"measure\"][\"model\"]\n",
    "        model_gwl_series = pd.Series(data=model_gwl_arr, index=datetime_array)\n",
    "\n",
    "        if np.unique(model_gwl_series.index.year).size > 2:\n",
    "\n",
    "            valid_idx_start = model_gwl_series.first_valid_index()\n",
    "            valid_idx_end = model_gwl_series.last_valid_index()\n",
    "\n",
    "            valid_gwl_series = model_gwl_series[valid_idx_start:valid_idx_end]\n",
    "            # ______________________________________________________________________\n",
    "            # Step 4: Perform Time Series Analysis for Peak and Trough Detection\n",
    "            \"\"\"\n",
    "            Analyze the time series by:\n",
    "            1. Detrending the series.\n",
    "            2. Smoothing the series.\n",
    "            3. Detecting peaks and troughs.\n",
    "            \"\"\"\n",
    "            (\n",
    "                refined_peaks,\n",
    "                refined_troughs,\n",
    "                final_peaks,\n",
    "                final_troughs,\n",
    "                smoothed_data,\n",
    "                original_data,\n",
    "                dates,\n",
    "            ) = analyze_time_series(\n",
    "                valid_gwl_series,\n",
    "                # smoothing_window=int(len(valid_gwl_series) * 0.01),  # Dynamic smoothing window\n",
    "                smoothing_window=61,  # Dynamic smoothing window\n",
    "                detrend_degree=3,\n",
    "                polyorder=1,\n",
    "            )\n",
    "\n",
    "            # ______________________________________________________________________\n",
    "            output_peak_table = pd.DataFrame(\n",
    "                {\"indexes\": refined_peaks, \"dates\": dates[refined_peaks], \"values\": original_data[refined_peaks]}\n",
    "            )\n",
    "\n",
    "            output_trough_table = pd.DataFrame(\n",
    "                {\"indexes\": refined_troughs, \"dates\": dates[refined_troughs], \"values\": original_data[refined_troughs]}\n",
    "            )\n",
    "\n",
    "            excel_savepath = os.path.join(\"temp4\", f\"{output_savename}.xlsx\")\n",
    "\n",
    "            output_peak_table.to_excel(excel_savepath, sheet_name=\"peaks\", index=False)\n",
    "            data_io.save_df_to_excel(\n",
    "                df_to_save=output_trough_table,\n",
    "                filepath=excel_savepath,\n",
    "                sheet_name=\"troughs\",\n",
    "                verbose=False,\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "            # ======================================================================\n",
    "            #                        Main Program: Visualization\n",
    "            # ======================================================================\n",
    "\n",
    "            # ______________________________________________________________________\n",
    "            # Step 5: Generate and Display Plot for Peaks and Troughs\n",
    "            \"\"\"\n",
    "            Visualize the original and smoothed time series along with the detected peaks and troughs.\n",
    "            - fig: Figure object containing the plot.\n",
    "            \"\"\"\n",
    "            fig = plot_results(\n",
    "                dates,\n",
    "                original_data,\n",
    "                smoothed_data,\n",
    "                refined_peaks,\n",
    "                refined_troughs,\n",
    "                f\"{select_station} {select_wellcode}\",  # Plot title based on station and wellcode\n",
    "            )\n",
    "\n",
    "            # Optional: Save the plot to a file (commented out by default)\n",
    "            savepath = f\"temp3\\\\{output_savename}.png\"\n",
    "            visualize.save_figure(fig, savepath)\n",
    "\n",
    "            # Close the plot to free memory (especially important in loops)\n",
    "            plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3865c6db-8b89-4608-a57a-5e541409355e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bd6e7d-599f-44b0-a9b7-5175db7a696a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
