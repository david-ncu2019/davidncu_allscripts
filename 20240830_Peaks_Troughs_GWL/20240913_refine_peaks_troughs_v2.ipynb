{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "534958d0-afcb-4a44-bb39-5fc72e063983",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T09:39:44.884059Z",
     "iopub.status.busy": "2024-09-13T09:39:44.883058Z",
     "iopub.status.idle": "2024-09-13T09:39:47.031178Z",
     "shell.execute_reply": "2024-09-13T09:39:47.031178Z",
     "shell.execute_reply.started": "2024-09-13T09:39:44.884059Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *\n",
    "from signaltools import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31f69b98-4aec-4fc5-918c-42f786d1852d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T09:39:47.032180Z",
     "iopub.status.busy": "2024-09-13T09:39:47.032180Z",
     "iopub.status.idle": "2024-09-13T09:39:47.045180Z",
     "shell.execute_reply": "2024-09-13T09:39:47.045180Z",
     "shell.execute_reply.started": "2024-09-13T09:39:47.032180Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_peak_trough_data(file_path, sheet_name, data_type=\"peaks\"):\n",
    "    \"\"\"\n",
    "    Read peak or trough data from an Excel sheet and add a 'type' column.\n",
    "\n",
    "    Parameters:\n",
    "    - file_path (str): Path to the Excel file.\n",
    "    - sheet_name (str): Name of the Excel sheet to read data from.\n",
    "    - data_type (str): Type of the data (\"peaks\" or \"troughs\").\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: DataFrame containing the data with an added 'type' column.\n",
    "    \"\"\"\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    df[\"type\"] = data_type\n",
    "    return df\n",
    "\n",
    "def calculate_date_diff_threshold(df, date_col=\"dates\", quantile=0.25, min_threshold=None):\n",
    "    \"\"\"\n",
    "    Calculate the threshold for date differences between consecutive data points.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the time series data.\n",
    "    - date_col (str): The column containing the dates.\n",
    "    - quantile (float): The quantile to calculate for the threshold.\n",
    "    - min_threshold (float or None): If specified, the threshold will be at least this value.\n",
    "\n",
    "    Returns:\n",
    "    - float: The threshold for date differences.\n",
    "    \"\"\"\n",
    "    date_diffs = df[date_col].diff().dropna().abs().dt.days\n",
    "    threshold = date_diffs.quantile(quantile)\n",
    "    if min_threshold is not None:\n",
    "        threshold = max(threshold, min_threshold)\n",
    "    return threshold\n",
    "\n",
    "def compare_values_within_threshold(idx_current, idx_neighbor, dates, values, threshold, data_type=\"peaks\"):\n",
    "    \"\"\"\n",
    "    Compare values at two indices if their date difference is within the threshold.\n",
    "\n",
    "    Parameters:\n",
    "    - idx_current (int): Index of the current data point.\n",
    "    - idx_neighbor (int): Index of the neighboring data point.\n",
    "    - dates (pd.Series): Series of dates.\n",
    "    - values (pd.Series): Series of values corresponding to dates.\n",
    "    - threshold (float): Threshold for allowable date difference.\n",
    "    - data_type (str): Type of data (\"peaks\" or \"troughs\").\n",
    "\n",
    "    Returns:\n",
    "    - int or None: Index of the data point to remove, or None if no removal.\n",
    "    \"\"\"\n",
    "    date_current = dates.iloc[idx_current]\n",
    "    date_neighbor = dates.iloc[idx_neighbor]\n",
    "    value_current = values.iloc[idx_current]\n",
    "    value_neighbor = values.iloc[idx_neighbor]\n",
    "    day_diff = abs((date_current - date_neighbor).days)\n",
    "\n",
    "    if day_diff < threshold:\n",
    "        if data_type == \"peaks\":\n",
    "            # For peaks, remove the lower peak\n",
    "            return idx_neighbor if value_current > value_neighbor else idx_current\n",
    "        elif data_type == \"troughs\":\n",
    "            # For troughs, remove the higher trough\n",
    "            return idx_neighbor if value_current < value_neighbor else idx_current\n",
    "    return None\n",
    "\n",
    "def identify_redundant_indices(df, threshold, data_type=\"peaks\", date_col=\"dates\", value_col=\"values\"):\n",
    "    \"\"\"\n",
    "    Identify indices of data points to remove based on proximity and value comparison.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the time series data.\n",
    "    - threshold (float): Threshold for allowable date difference.\n",
    "    - data_type (str): Type of data (\"peaks\" or \"troughs\").\n",
    "    - date_col (str): Column name for dates.\n",
    "    - value_col (str): Column name for values.\n",
    "\n",
    "    Returns:\n",
    "    - list: List of indices to remove.\n",
    "    \"\"\"\n",
    "    indices_to_remove = []\n",
    "\n",
    "    dates = df[date_col].reset_index(drop=True)\n",
    "    values = df[value_col].reset_index(drop=True)\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        idx_current = i\n",
    "        # Compare with previous and next points\n",
    "        neighbor_indices = []\n",
    "        if i > 0:\n",
    "            neighbor_indices.append(i - 1)\n",
    "        if i < len(df) - 1:\n",
    "            neighbor_indices.append(i + 1)\n",
    "        for idx_neighbor in neighbor_indices:\n",
    "            idx_to_remove = compare_values_within_threshold(\n",
    "                idx_current=idx_current,\n",
    "                idx_neighbor=idx_neighbor,\n",
    "                dates=dates,\n",
    "                values=values,\n",
    "                threshold=threshold,\n",
    "                data_type=data_type\n",
    "            )\n",
    "            if idx_to_remove is not None:\n",
    "                indices_to_remove.append(df.index[idx_to_remove])\n",
    "\n",
    "    # Remove duplicate indices\n",
    "    indices_to_remove = list(set(indices_to_remove))\n",
    "    return indices_to_remove\n",
    "\n",
    "def filter_consecutive_duplicates(df, value_col=\"values\", type_col=\"type\"):\n",
    "    \"\"\"\n",
    "    Filter DataFrame by keeping the largest peaks and smallest troughs when consecutive\n",
    "    peaks or troughs are detected.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing both peaks and troughs.\n",
    "    - value_col (str): Column name for values.\n",
    "    - type_col (str): Column name specifying the type of data (\"peaks\" or \"troughs\").\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Filtered DataFrame with desired peaks and troughs.\n",
    "    \"\"\"\n",
    "    keep_indices = []\n",
    "    i = 0\n",
    "    while i < len(df):\n",
    "        current_type = df.iloc[i][type_col]\n",
    "        current_value = df.iloc[i][value_col]\n",
    "        max_min_value = current_value\n",
    "        max_min_index = i\n",
    "        j = i\n",
    "        while j + 1 < len(df) and df.iloc[j + 1][type_col] == current_type:\n",
    "            j += 1\n",
    "            next_value = df.iloc[j][value_col]\n",
    "            if current_type == \"peaks\":\n",
    "                if next_value > max_min_value:\n",
    "                    max_min_value = next_value\n",
    "                    max_min_index = j\n",
    "            elif current_type == \"troughs\":\n",
    "                if next_value < max_min_value:\n",
    "                    max_min_value = next_value\n",
    "                    max_min_index = j\n",
    "        keep_indices.append(df.index[max_min_index])\n",
    "        i = j + 1\n",
    "    return df.loc[keep_indices].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c50dc6bb-f9a5-412d-988b-ab08ab7f311a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T09:39:47.047181Z",
     "iopub.status.busy": "2024-09-13T09:39:47.046182Z",
     "iopub.status.idle": "2024-09-13T09:40:00.276687Z",
     "shell.execute_reply": "2024-09-13T09:40:00.276687Z",
     "shell.execute_reply.started": "2024-09-13T09:39:47.047181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temp4_v2\\ANHE_10070111.xlsx\n",
      "temp4_v2\\ANHE_10070121.xlsx\n",
      "temp4_v2\\ANHE_10070131.xlsx\n",
      "temp4_v2\\ANHE_10070141.xlsx\n",
      "temp4_v2\\ANNAN_09140112.xlsx\n",
      "temp4_v2\\ANNAN_09140122.xlsx\n",
      "temp4_v2\\BEIGANG_09060112.xlsx\n",
      "temp4_v2\\BEIGANG_09060122.xlsx\n",
      "temp4_v2\\BOZI_09180111.xlsx\n",
      "temp4_v2\\BOZI_09180121.xlsx\n",
      "temp4_v2\\BOZI_09180131.xlsx\n",
      "temp4_v2\\CAICUO_09180211.xlsx\n",
      "temp4_v2\\CAICUO_09180221.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Define main parameters (user can modify these as needed)\n",
    "EXCEL_FOLDER = \"temp4_v2\"\n",
    "HDF5_FILE_PATH = \"20240903_GWL_CRFP.h5\"\n",
    "THRESHOLD_QUANTILE = 0.2\n",
    "MIN_THRESHOLD = None  # Minimum threshold for date differences, can be set to None\n",
    "\n",
    "# Load HDF5 File and Extract Data\n",
    "with h5py.File(HDF5_FILE_PATH, \"r\") as hdf5_file:\n",
    "    existing_data_dict = h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "    available_datasets = h5pytools.list_datasets(hdf5_file)\n",
    "    datetime_array = pd.to_datetime(existing_data_dict[\"date\"], format=\"%Y%m%d\")\n",
    "\n",
    "# Get list of Excel files from folder\n",
    "excel_files = glob(os.path.join(EXCEL_FOLDER, \"*.xlsx\"))\n",
    "\n",
    "# Process each Excel file\n",
    "# for file_path in tqdm(excel_files, desc=\"Processing Excel files\"):\n",
    "\n",
    "# file_path = excel_files[-1]\n",
    "for file_path in excel_files:\n",
    "\n",
    "    print(file_path)\n",
    "    \n",
    "    base_name = os.path.basename(file_path).split(\".\")[0]\n",
    "    station, wellcode = base_name.split(\"_\")\n",
    "    \n",
    "    # Get model groundwater level data\n",
    "    model_gwl_arr = existing_data_dict[station][wellcode][\"measure\"][\"model\"]\n",
    "    model_gwl_series = pd.Series(data=model_gwl_arr, index=datetime_array)\n",
    "    \n",
    "    # Read peaks and troughs data\n",
    "    peaks_df = read_peak_trough_data(file_path, sheet_name=\"peaks\", data_type=\"peaks\")\n",
    "    troughs_df = read_peak_trough_data(file_path, sheet_name=\"troughs\", data_type=\"troughs\")\n",
    "    combined_df = pd.concat([peaks_df, troughs_df]).sort_values(by=\"dates\").reset_index(drop=True)\n",
    "    \n",
    "    # Calculate thresholds for peaks and troughs\n",
    "    peak_threshold = calculate_date_diff_threshold(\n",
    "        peaks_df, date_col=\"dates\", quantile=THRESHOLD_QUANTILE, min_threshold=MIN_THRESHOLD\n",
    "    )\n",
    "    trough_threshold = calculate_date_diff_threshold(\n",
    "        troughs_df, date_col=\"dates\", quantile=THRESHOLD_QUANTILE, min_threshold=MIN_THRESHOLD\n",
    "    )\n",
    "    \n",
    "    # Identify indices to remove for peaks and troughs\n",
    "    peak_indices_to_remove = identify_redundant_indices(\n",
    "        peaks_df, threshold=peak_threshold, data_type=\"peaks\", date_col=\"dates\", value_col=\"values\"\n",
    "    )\n",
    "    trough_indices_to_remove = identify_redundant_indices(\n",
    "        troughs_df, threshold=trough_threshold, data_type=\"troughs\", date_col=\"dates\", value_col=\"values\"\n",
    "    )\n",
    "    \n",
    "    # Filter peaks and troughs DataFrames\n",
    "    filtered_peaks_df = peaks_df.drop(peak_indices_to_remove).reset_index(drop=True)\n",
    "    filtered_troughs_df = troughs_df.drop(trough_indices_to_remove).reset_index(drop=True)\n",
    "    \n",
    "    # Combine filtered peaks and troughs\n",
    "    filtered_combined_df = pd.concat([filtered_peaks_df, filtered_troughs_df]).sort_values(by=\"dates\").reset_index(drop=True)\n",
    "    \n",
    "    # Apply second filter to remove consecutive duplicates\n",
    "    final_df = filter_consecutive_duplicates(filtered_combined_df, value_col=\"values\", type_col=\"type\")\n",
    "\n",
    "    # ________________________________________________________________________________________________________________\n",
    "\n",
    "    # Set the figure size and calculate the scaling factor\n",
    "    fig_width, fig_height = visualize.BASE_SIZE\n",
    "    scaling_factor = visualize.calculate_scaling_factor(fig_width, fig_height)\n",
    "    \n",
    "    # Create the figure and axis\n",
    "    fig, ax = plt.subplots(figsize=(fig_width * 1.5, fig_height * 2 / 3))\n",
    "    \n",
    "    # Plot the original data and smoothed data\n",
    "    ax.plot(model_gwl_series, label=\"Modeled GWL\", color=\"black\", linewidth=1.5, alpha=0.8, zorder=1)\n",
    "    \n",
    "    # Plot peaks and troughs using scatter plots\n",
    "    ax.scatter(\n",
    "        combined_df.query(\"type=='peaks'\")[\"dates\"],\n",
    "        combined_df.query(\"type=='peaks'\")[\"values\"],\n",
    "        color=\"grey\",\n",
    "        marker=\"s\",\n",
    "        s=70,\n",
    "        alpha=0.3,\n",
    "        zorder=2,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        combined_df.query(\"type=='troughs'\")[\"dates\"],\n",
    "        combined_df.query(\"type=='troughs'\")[\"values\"],\n",
    "        color=\"grey\",\n",
    "        marker=\"s\",\n",
    "        s=70,\n",
    "        alpha=0.3,\n",
    "        zorder=2,\n",
    "    )\n",
    "    \n",
    "    ax.scatter(\n",
    "        final_df.query(\"type=='peaks'\")[\"dates\"],\n",
    "        final_df.query(\"type=='peaks'\")[\"values\"],\n",
    "        color=\"blue\",\n",
    "        marker=\"s\",\n",
    "        label=\"Peaks\",\n",
    "        s=70,\n",
    "        alpha=0.7,\n",
    "        zorder=3,\n",
    "    )\n",
    "    ax.scatter(\n",
    "        final_df.query(\"type=='troughs'\")[\"dates\"],\n",
    "        final_df.query(\"type=='troughs'\")[\"values\"],\n",
    "        color=\"magenta\",\n",
    "        marker=\"s\",\n",
    "        label=\"Troughs\",\n",
    "        s=70,\n",
    "        alpha=0.7,\n",
    "        zorder=3,\n",
    "    )\n",
    "    \n",
    "    # Annotate peaks with their corresponding indexes above the markers\n",
    "    for i in range(len(peaks_df)):\n",
    "        ax.text(\n",
    "            peaks_df.loc[i, \"dates\"],\n",
    "            peaks_df.loc[i, \"values\"] + 0.4,\n",
    "            str(peaks_df.loc[i, \"indexes\"]),\n",
    "            ha=\"center\",\n",
    "            va=\"bottom\",\n",
    "            rotation=\"vertical\",\n",
    "            fontsize=10 * scaling_factor,\n",
    "        )\n",
    "    \n",
    "    # Annotate peaks with their corresponding indexes above the markers\n",
    "    for i in range(len(troughs_df)):\n",
    "        ax.text(\n",
    "            troughs_df.loc[i, \"dates\"],\n",
    "            troughs_df.loc[i, \"values\"] - 0.4,\n",
    "            str(troughs_df.loc[i, \"indexes\"]),\n",
    "            ha=\"center\",\n",
    "            va=\"top\",\n",
    "            rotation=\"vertical\",\n",
    "            fontsize=10 * scaling_factor,\n",
    "        )\n",
    "    \n",
    "    \n",
    "    # Configure the axis labels, title, and font scaling using visualize.py\n",
    "    visualize.configure_axis(\n",
    "        ax=ax, xlabel=\"Date\", ylabel=\"Groundwater Levels (m)\", title=f\"{station} {wellcode}\", scaling_factor=scaling_factor\n",
    "    )\n",
    "    \n",
    "    # Configure the legend\n",
    "    visualize.configure_legend(ax=ax, scaling_factor=0.5, frameon=False)\n",
    "    \n",
    "    # Configure ticks (datetime for x-axis)\n",
    "    visualize.configure_datetime_ticks(ax=ax, axis=\"x\", major_interval=12, minor_interval=6, date_format=\"%Y\")\n",
    "    \n",
    "    # Adjust layout and set x-axis label rotation\n",
    "    fig.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.setp(ax.get_xticklabels(), rotation=90, ha=\"center\")\n",
    "    \n",
    "    # Optional: Save the plot to a file (commented out by default)\n",
    "    output_savename = f\"{station}_{wellcode}\"\n",
    "    savepath = f\"temp3_v2\\\\{output_savename}.png\"\n",
    "    visualize.save_figure(fig, savepath)\n",
    "    \n",
    "    # Close the plot to free memory (especially important in loops)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979caf4a-86e0-45d5-b8f7-05dd96e357de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
