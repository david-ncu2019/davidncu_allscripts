{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a503bc74-3c3d-4fcf-a8dc-e351ecdb0e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:19:40.097791Z",
     "iopub.status.busy": "2024-08-29T08:19:40.096793Z",
     "iopub.status.idle": "2024-08-29T08:19:42.860945Z",
     "shell.execute_reply": "2024-08-29T08:19:42.859946Z",
     "shell.execute_reply.started": "2024-08-29T08:19:40.097791Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4827a956-6b06-4460-8634-37a6ef5c5292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:19:42.862947Z",
     "iopub.status.busy": "2024-08-29T08:19:42.861945Z",
     "iopub.status.idle": "2024-08-29T08:19:42.876946Z",
     "shell.execute_reply": "2024-08-29T08:19:42.875945Z",
     "shell.execute_reply.started": "2024-08-29T08:19:42.862947Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Define Functions for Repeated Operations\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_seasonal_and_trend_data(series):\n",
    "    \"\"\"Extract trend and seasonal components from a series.\"\"\"\n",
    "    numeric_time_idx = datetime_handle.numeric_time_index(series)\n",
    "    finite_values = series[~series.isnull()].values\n",
    "\n",
    "    # Polynomial trend\n",
    "    trend, _ = analysis.get_polynomial_trend(\n",
    "        x=numeric_time_idx,\n",
    "        y=finite_values,\n",
    "        order=2,\n",
    "        x_estimate=np.arange(len(series)),\n",
    "    )\n",
    "    trend.index = series.index\n",
    "\n",
    "    # Detrend Data\n",
    "    detrended_series = series - trend\n",
    "\n",
    "    # Seasonality Analysis\n",
    "    seasonality_info = analysis.find_seasonality(\n",
    "        time_series_data=detrended_series\n",
    "    )\n",
    "    seasonality_info = seasonality_info[seasonality_info[\"Period (days)\"] > 7]\n",
    "    seasonality_info = seasonality_info.nlargest(n=50, columns=\"Amplitude\")\n",
    "\n",
    "    return trend, detrended_series, seasonality_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8811b5-f1ac-4e7b-817f-bb515d1c35a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:19:42.878946Z",
     "iopub.status.busy": "2024-08-29T08:19:42.877946Z",
     "iopub.status.idle": "2024-08-29T08:19:42.891947Z",
     "shell.execute_reply": "2024-08-29T08:19:42.890946Z",
     "shell.execute_reply.started": "2024-08-29T08:19:42.878946Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_well_data(gwl_hdf5_file, select_dataset):\n",
    "    \"\"\"\n",
    "    Process data for a single well, removing outliers, extracting trends,\n",
    "    fitting a sinusoidal model, and correcting phase shifts.\n",
    "\n",
    "    Parameters:\n",
    "    - ename (str): The name of the station or entity.\n",
    "    - wellcode (str): The code identifying the specific well.\n",
    "    - hdf5_file (str): The path to the HDF5 file containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - wellcode (str): The well code.\n",
    "    - df_fromHDF5 (pd.DataFrame): The DataFrame containing the processed data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the HDF5 file in read mode to retrieve existing data for the well\n",
    "        with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "            # Read the existing dataset associated with the well code\n",
    "            value_arr = hdf5_file[select_dataset][...]\n",
    "\n",
    "        # Create a DataFrame from the retrieved HDF5 data with datetime index\n",
    "        df_fromHDF5 = pd.DataFrame(\n",
    "            {\"time\": datetime_idx, \"daily_value\": value_arr}\n",
    "        )\n",
    "        df_fromHDF5 = df_fromHDF5.set_index(\"time\")\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        series_average = np.nanmean(df_fromHDF5)\n",
    "        series_stdev = np.nanstd(df_fromHDF5)\n",
    "\n",
    "        # Remove outliers beyond 3 standard deviations\n",
    "        condition = (df_fromHDF5 >= (series_average - 3 * series_stdev)) & (\n",
    "            df_fromHDF5 <= (series_average + 3 * series_stdev)\n",
    "        )\n",
    "\n",
    "        df_fromHDF5 = df_fromHDF5.where(condition, np.nan)\n",
    "\n",
    "        # Trim the DataFrame to the first and last valid indices\n",
    "        df_trimmed = df_fromHDF5.loc[\n",
    "            df_fromHDF5.first_valid_index() : df_fromHDF5.last_valid_index()\n",
    "        ]\n",
    "\n",
    "        series = df_trimmed.iloc[:, 0]\n",
    "\n",
    "        # Extract trend and seasonality\n",
    "        trend, detrended_series, seasonality_info = get_seasonal_and_trend_data(\n",
    "            series\n",
    "        )\n",
    "\n",
    "        # Prepare Sinusoidal Model Inputs\n",
    "        (\n",
    "            time_values,\n",
    "            observed_values,\n",
    "            amplitudes,\n",
    "            periods,\n",
    "            phase_shifts,\n",
    "            baseline,\n",
    "        ) = modeling.prepare_sinusoidal_model_inputs(\n",
    "            time_series_data=detrended_series,\n",
    "            seasonality_info=seasonality_info.query(\"Frequency != 0\"),\n",
    "        )\n",
    "\n",
    "        # Fit Sinusoidal Model and Correct Phase Shift\n",
    "        fitted_signal = modeling.fit_sinusoidal_model(\n",
    "            time_values=time_values,\n",
    "            observed_values=observed_values,\n",
    "            amplitudes=amplitudes,\n",
    "            periods=periods,\n",
    "            phase_shifts=phase_shifts,\n",
    "            baseline=baseline,\n",
    "            predict_time=np.arange(len(df_trimmed)),\n",
    "        )\n",
    "\n",
    "        # Correct phase shift in the fitted signal\n",
    "        corrected_signal_series = pd.Series(\n",
    "            analysis.correct_phase_shift(detrended_series, fitted_signal),\n",
    "            index=df_trimmed.index,\n",
    "        )\n",
    "\n",
    "        # Combine trend and corrected signal to get the modeled series\n",
    "        df_trimmed[\"model\"] = trend + corrected_signal_series\n",
    "        df_fromHDF5[\"model\"] = df_fromHDF5.index.map(df_trimmed[\"model\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for {select_dataset}: {e}\")\n",
    "        df_fromHDF5[\"model\"] = df_fromHDF5[\"daily_value\"]\n",
    "        # Handle error appropriately (e.g., logging, returning None, etc.)\n",
    "\n",
    "    return df_fromHDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d4bf1cc-b6d3-4dfa-bf3d-29453c4c0069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:19:42.895943Z",
     "iopub.status.busy": "2024-08-29T08:19:42.894943Z",
     "iopub.status.idle": "2024-08-29T08:19:42.907943Z",
     "shell.execute_reply": "2024-08-29T08:19:42.906947Z",
     "shell.execute_reply.started": "2024-08-29T08:19:42.895943Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_dict_from_list(elements, value):\n",
    "    if len(elements) == 1:\n",
    "        return {\n",
    "            elements[0]: value\n",
    "        }  # Base case: return the NumPy array for the last element\n",
    "    return {\n",
    "        elements[0]: generate_dict_from_list(elements[1:], value)\n",
    "    }  # Recursively build the dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "id": "42430273-31ff-4d3b-bc72-72afc6960a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:05:09.687213Z",
     "iopub.status.busy": "2024-08-29T07:05:09.686217Z",
     "iopub.status.idle": "2024-08-29T07:05:09.691215Z",
     "shell.execute_reply": "2024-08-29T07:05:09.690214Z",
     "shell.execute_reply.started": "2024-08-29T07:05:09.687213Z"
    }
   },
   "source": [
    "# extract original time-series from HDF5 file\n",
    "# model the original time-series\n",
    "# generate a dictionary to store the model data in the corresponding station/well\n",
    "# extract dictionaries of data and metadata from current HDF5\n",
    "# update it with new data (model data)\n",
    "# save a backup\n",
    "# write the updated dictionaries to HDF5 file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3016820c-c28c-4237-8863-d03f718afa30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:19:42.909944Z",
     "iopub.status.busy": "2024-08-29T08:19:42.908943Z",
     "iopub.status.idle": "2024-08-29T08:19:43.033946Z",
     "shell.execute_reply": "2024-08-29T08:19:43.032945Z",
     "shell.execute_reply.started": "2024-08-29T08:19:42.909944Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ANHE/10070111/measure/daily_value',\n",
       " 'ANHE/10070121/measure/daily_value',\n",
       " 'ANHE/10070131/measure/daily_value',\n",
       " 'ANHE/10070141/measure/daily_value',\n",
       " 'ANNAN/09140111/measure/daily_value']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gwl_hdf5_file = \"20240828_GWL_CRFP.h5\"\n",
    "# List available datasets in the HDF5 file for reference\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    available_datasets = gwatertools.h5pytools.list_datasets(hdf5_file)\n",
    "    datetime_idx = pd.to_datetime(hdf5_file[\"date\"][...], format=\"%Y%m%d\")\n",
    "\n",
    "available_datasets[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c00c6ecb-877c-4236-85d2-4f9a2e4a39c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T08:19:43.036945Z",
     "iopub.status.busy": "2024-08-29T08:19:43.035944Z",
     "iopub.status.idle": "2024-08-29T09:25:30.614653Z",
     "shell.execute_reply": "2024-08-29T09:25:30.613656Z",
     "shell.execute_reply.started": "2024-08-29T08:19:43.036945Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█████████████▊                                                                 | 55/316 [12:42<1:30:47, 20.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data for DOULIU/090111M1/measure/daily_value: RANSAC could not find a valid consensus set. All `max_trials` iterations were skipped because each randomly chosen sub-sample failed the passing criteria. See estimator attributes for diagnostics (n_skips*).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▊| 315/316 [1:05:46<00:12, 12.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing data for date: ufunc 'true_divide' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m df_fromHDF5 \u001b[38;5;241m=\u001b[39m process_well_data(gwl_hdf5_file, select_dataset)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# _____________________________________________\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m station_name, well_code, data_group, data_type \u001b[38;5;241m=\u001b[39m select_dataset\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# _____________________________________________\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m station_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m all_stations_measurement_data\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# _____________________________________________\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 1)"
     ]
    }
   ],
   "source": [
    "all_stations_measurement_data = {}\n",
    "all_stations_metadata = {}\n",
    "\n",
    "for select_dataset in tqdm(available_datasets[:-1]):\n",
    "\n",
    "    # Open the HDF5 file in read mode to retrieve existing data for the well\n",
    "    with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "        # Read the existing dataset associated with the well code\n",
    "        value_arr = hdf5_file[select_dataset][...]\n",
    "\n",
    "    df_fromHDF5 = process_well_data(gwl_hdf5_file, select_dataset)\n",
    "    # _____________________________________________\n",
    "    station_name, well_code, data_group, data_type = select_dataset.split(\"/\")\n",
    "    # _____________________________________________\n",
    "\n",
    "    if station_name not in all_stations_measurement_data.keys():\n",
    "        # _____________________________________________\n",
    "        all_stations_measurement_data[station_name] = {\n",
    "            well_code: {data_group: {\"model\": df_fromHDF5[\"model\"].values}}\n",
    "        }\n",
    "        # _____________________________________________\n",
    "        all_stations_metadata[station_name] = {\n",
    "            \"UpdatedTime\": datetime.now().strftime(\"%Y/%m/%d, %H:%M:%S\")\n",
    "        }\n",
    "\n",
    "        # _____________________________________________\n",
    "    else:\n",
    "        all_stations_measurement_data[station_name].update(\n",
    "            {well_code: {data_group: {\"model\": df_fromHDF5[\"model\"].values}}}\n",
    "        )\n",
    "\n",
    "    all_stations_metadata[station_name][well_code] = {\n",
    "        data_group: {\n",
    "            \"model\": {\n",
    "                \"Description\": \"Denoise and fill missing values in the GWL data using second-order polynomial trend & fourier transform analysis\",\n",
    "                \"FIRST_OBS\": df_fromHDF5[\"model\"]\n",
    "                .first_valid_index()\n",
    "                .strftime(\"%Y%m%d\"),\n",
    "                \"LAST_OBS\": df_fromHDF5[\"model\"]\n",
    "                .last_valid_index()\n",
    "                .strftime(\"%Y%m%d\"),\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ee36741-9d18-4937-8222-2c6c08556c5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T10:00:23.479246Z",
     "iopub.status.busy": "2024-08-29T10:00:23.478246Z",
     "iopub.status.idle": "2024-08-29T10:00:24.029246Z",
     "shell.execute_reply": "2024-08-29T10:00:24.028242Z",
     "shell.execute_reply.started": "2024-08-29T10:00:23.479246Z"
    }
   },
   "outputs": [],
   "source": [
    "# shutil.copy2(src=gwl_hdf5_file, dst=gwl_hdf5_file.replace(\".h5\", \"_secure.h5\"))\n",
    "\n",
    "# gwl_hdf5_file = \n",
    "# # Extract existing data and metadata\n",
    "# with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "#     existing_data_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "#     existing_metadata_dict = gwatertools.h5pytools.hdf5_to_metadata_dict(\n",
    "#         hdf5_file\n",
    "#     )\n",
    "\n",
    "# # Update dictionaries\n",
    "# updated_data_dict = gwatertools.h5pytools.update_data_dict(\n",
    "#     existing_data_dict, all_stations_measurement_data\n",
    "# )\n",
    "# updated_metadata_dict = gwatertools.h5pytools.update_metadata_dict(\n",
    "#     existing_metadata_dict, all_stations_metadata\n",
    "# )\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(\"20240828_GWL_CRFP_v2.h5\", \"w\") as hdf5_file:\n",
    "    metadata_to_hdf5(hdf5_file, updated_metadata_dict)\n",
    "    data_to_hdf5(hdf5_file, updated_data_dict)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "71945641-0541-405b-a0c1-6cbfea23e724",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:43:28.755397Z",
     "iopub.status.busy": "2024-08-28T01:43:28.754398Z",
     "iopub.status.idle": "2024-08-28T01:43:28.838491Z",
     "shell.execute_reply": "2024-08-28T01:43:28.838491Z",
     "shell.execute_reply.started": "2024-08-28T01:43:28.755397Z"
    }
   },
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Main Script\n",
    "# ------------------------------------------------------------------------------\n",
    "gwl_hdf5_file = \"test.h5\"\n",
    "\n",
    "# List available datasets in the HDF5 file for reference\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    available_datasets = gwatertools.h5pytools.list_datasets(hdf5_file)\n",
    "    available_datasets = [_ for _ in available_datasets if \"date\" not in _]\n",
    "\n",
    "# Extract unique station names from available datasets\n",
    "available_stations = sorted(\n",
    "    set(dataset.split(\"/\")[0] for dataset in available_datasets)\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping each station to its corresponding files to process\n",
    "file_to_process_dict = {\n",
    "    station: {\n",
    "        dataset.split(\"/\")[-1]\n",
    "        for dataset in available_datasets\n",
    "        if dataset.startswith(station)\n",
    "    }\n",
    "    for station in available_stations\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d1f2ccb5-182f-43ec-8fbb-7bf6aaf11ad3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:43:28.840487Z",
     "iopub.status.busy": "2024-08-28T01:43:28.840487Z",
     "iopub.status.idle": "2024-08-28T01:44:49.242735Z",
     "shell.execute_reply": "2024-08-28T01:44:49.242735Z",
     "shell.execute_reply.started": "2024-08-28T01:43:28.840487Z"
    }
   },
   "source": [
    "updates_dict = {}\n",
    "error_log = {}\n",
    "\n",
    "for ename in list(file_to_process_dict.keys())[:1]:\n",
    "    \"\"\"\n",
    "    {\n",
    "    'sensor_data':{\n",
    "                'date': new_data,\n",
    "                'well_1' : np.array(xyz, 2),\n",
    "                'well_2' : np.array(xyz, 2)\n",
    "                }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    temp_data = {}\n",
    "\n",
    "    temp_date_idx = []\n",
    "    for wellcode in file_to_process_dict[ename]:\n",
    "        try:\n",
    "            wellcode, df = process_well_data(ename, wellcode, gwl_hdf5_file)\n",
    "            temp_data[wellcode] = df.values\n",
    "            temp_date_idx.extend(df.index.strftime(\"%Y%m%d\").tolist())\n",
    "        except Exception as e:\n",
    "            error_log.setdefault(ename, {})[wellcode] = str(e)\n",
    "\n",
    "    temp_data[\"date\"] = np.array(sorted(set(temp_date_idx)), dtype=\"S10\")\n",
    "    dict_to_update = {\n",
    "        ename: {\n",
    "            \"sensor_data\": temp_data,\n",
    "            \"metadata\": {\n",
    "                \"Updated Date\": datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"),\n",
    "                \"Description\": \"Model the groundwater level data to reduce noise and fill missing values\",\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    updates_dict.update(dict_to_update)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b8a8376-9fc0-4613-8f75-bf8732045892",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:44:49.244736Z",
     "iopub.status.busy": "2024-08-28T01:44:49.244736Z",
     "iopub.status.idle": "2024-08-28T01:44:49.293737Z",
     "shell.execute_reply": "2024-08-28T01:44:49.293737Z",
     "shell.execute_reply.started": "2024-08-28T01:44:49.244736Z"
    }
   },
   "source": [
    "# SECTION 5: Apply Updates to HDF5 File\n",
    "# -------------------------------------\n",
    "# Use the predefined function to update the HDF5 file with new data and metadata\n",
    "shutil.copy2(src=gwl_hdf5_file, dst=gwl_hdf5_file.replace(\".h5\", \"_secure.h5\"))\n",
    "gwatertools.h5pytools.update_hdf5(gwl_hdf5_file, updates_dict)\n",
    "\n",
    "\n",
    "# Save error log if any errors occurred\n",
    "if error_log:\n",
    "    error_log_path = \"error_log_test.txt\"\n",
    "    with open(error_log_path, \"w\") as f:\n",
    "        for ename, errors in error_log.items():\n",
    "            for wellcode, error_msg in errors.items():\n",
    "                f.write(f\"Error for {ename}/{wellcode}: {error_msg}\\n\")\n",
    "    print(\n",
    "        f\"Errors occurred during processing. See error log at: {error_log_path}\"\n",
    "    )\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
