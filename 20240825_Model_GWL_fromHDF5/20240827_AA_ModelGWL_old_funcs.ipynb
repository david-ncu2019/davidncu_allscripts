{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a503bc74-3c3d-4fcf-a8dc-e351ecdb0e5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:43:25.666396Z",
     "iopub.status.busy": "2024-08-28T01:43:25.665396Z",
     "iopub.status.idle": "2024-08-28T01:43:28.727396Z",
     "shell.execute_reply": "2024-08-28T01:43:28.726397Z",
     "shell.execute_reply.started": "2024-08-28T01:43:25.666396Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4827a956-6b06-4460-8634-37a6ef5c5292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:43:28.729398Z",
     "iopub.status.busy": "2024-08-28T01:43:28.728401Z",
     "iopub.status.idle": "2024-08-28T01:43:28.737397Z",
     "shell.execute_reply": "2024-08-28T01:43:28.736393Z",
     "shell.execute_reply.started": "2024-08-28T01:43:28.729398Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Define Functions for Repeated Operations\n",
    "# ------------------------------------------------------------------------------\n",
    "def get_seasonal_and_trend_data(series):\n",
    "    \"\"\"Extract trend and seasonal components from a series.\"\"\"\n",
    "    numeric_time_idx = datetime_handle.numeric_time_index(series)\n",
    "    finite_values = series[~series.isnull()].values\n",
    "\n",
    "    # Polynomial trend\n",
    "    trend, _ = analysis.get_polynomial_trend(\n",
    "        x=numeric_time_idx,\n",
    "        y=finite_values,\n",
    "        order=1,\n",
    "        x_estimate=np.arange(len(series)),\n",
    "    )\n",
    "    trend.index = series.index\n",
    "\n",
    "    # Detrend Data\n",
    "    detrended_series = series - trend\n",
    "\n",
    "    # Seasonality Analysis\n",
    "    seasonality_info = analysis.find_seasonality(\n",
    "        time_series_data=detrended_series\n",
    "    )\n",
    "    seasonality_info = seasonality_info[seasonality_info[\"Period (days)\"] > 7]\n",
    "    seasonality_info = seasonality_info.nlargest(n=50, columns=\"Amplitude\")\n",
    "\n",
    "    return trend, detrended_series, seasonality_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e8811b5-f1ac-4e7b-817f-bb515d1c35a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:43:28.739397Z",
     "iopub.status.busy": "2024-08-28T01:43:28.738394Z",
     "iopub.status.idle": "2024-08-28T01:43:28.751397Z",
     "shell.execute_reply": "2024-08-28T01:43:28.750394Z",
     "shell.execute_reply.started": "2024-08-28T01:43:28.739397Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_well_data(ename, wellcode, hdf5_file):\n",
    "    \"\"\"\n",
    "    Process data for a single well, removing outliers, extracting trends,\n",
    "    fitting a sinusoidal model, and correcting phase shifts.\n",
    "\n",
    "    Parameters:\n",
    "    - ename (str): The name of the station or entity.\n",
    "    - wellcode (str): The code identifying the specific well.\n",
    "    - hdf5_file (str): The path to the HDF5 file containing the data.\n",
    "\n",
    "    Returns:\n",
    "    - wellcode (str): The well code.\n",
    "    - df_fromHDF5 (pd.DataFrame): The DataFrame containing the processed data.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Extract data from the HDF5 file for the specific location and sensor\n",
    "        df_fromHDF5 = gwatertools.h5pytools.export_data_to_dataframe(\n",
    "            file_name=hdf5_file,\n",
    "            location_name=ename,\n",
    "            sensor_name=wellcode,\n",
    "        ).set_index(\"datetime\")\n",
    "\n",
    "        # Calculate mean and standard deviation\n",
    "        series_average = np.nanmean(df_fromHDF5)\n",
    "        series_stdev = np.nanstd(df_fromHDF5)\n",
    "\n",
    "        # Remove outliers beyond 3 standard deviations\n",
    "        condition = (df_fromHDF5 >= (series_average - 3 * series_stdev)) & (\n",
    "            df_fromHDF5 <= (series_average + 3 * series_stdev)\n",
    "        )\n",
    "\n",
    "        df_fromHDF5 = df_fromHDF5.where(condition, np.nan)\n",
    "\n",
    "        # Trim the DataFrame to the first and last valid indices\n",
    "        df_trimmed = df_fromHDF5.loc[\n",
    "            df_fromHDF5.first_valid_index() : df_fromHDF5.last_valid_index()\n",
    "        ]\n",
    "\n",
    "        series = df_trimmed[\"value\"]\n",
    "\n",
    "        # Extract trend and seasonality\n",
    "        trend, detrended_series, seasonality_info = get_seasonal_and_trend_data(\n",
    "            series\n",
    "        )\n",
    "\n",
    "        # Prepare Sinusoidal Model Inputs\n",
    "        (\n",
    "            time_values,\n",
    "            observed_values,\n",
    "            amplitudes,\n",
    "            periods,\n",
    "            phase_shifts,\n",
    "            baseline,\n",
    "        ) = modeling.prepare_sinusoidal_model_inputs(\n",
    "            time_series_data=detrended_series,\n",
    "            seasonality_info=seasonality_info.query(\"Frequency != 0\"),\n",
    "        )\n",
    "\n",
    "        # Fit Sinusoidal Model and Correct Phase Shift\n",
    "        fitted_signal = modeling.fit_sinusoidal_model(\n",
    "            time_values=time_values,\n",
    "            observed_values=observed_values,\n",
    "            amplitudes=amplitudes,\n",
    "            periods=periods,\n",
    "            phase_shifts=phase_shifts,\n",
    "            baseline=baseline,\n",
    "            predict_time=np.arange(len(df_trimmed)),\n",
    "        )\n",
    "\n",
    "        # Correct phase shift in the fitted signal\n",
    "        corrected_signal_series = pd.Series(\n",
    "            analysis.correct_phase_shift(detrended_series, fitted_signal),\n",
    "            index=df_trimmed.index,\n",
    "        )\n",
    "\n",
    "        # Combine trend and corrected signal to get the modeled series\n",
    "        df_trimmed[\"model\"] = trend + corrected_signal_series\n",
    "        df_fromHDF5[\"model\"] = df_fromHDF5.index.map(df_trimmed[\"model\"])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for {ename}/{wellcode}: {e}\")\n",
    "        # Handle error appropriately (e.g., logging, returning None, etc.)\n",
    "\n",
    "    return wellcode, df_fromHDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6f08fb9-cf9a-473b-9087-49bf95fcf1e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:43:28.755397Z",
     "iopub.status.busy": "2024-08-28T01:43:28.754398Z",
     "iopub.status.idle": "2024-08-28T01:43:28.838491Z",
     "shell.execute_reply": "2024-08-28T01:43:28.838491Z",
     "shell.execute_reply.started": "2024-08-28T01:43:28.755397Z"
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# Main Script\n",
    "# ------------------------------------------------------------------------------\n",
    "gwl_hdf5_file = \"test.h5\"\n",
    "\n",
    "# List available datasets in the HDF5 file for reference\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    available_datasets = gwatertools.h5pytools.list_datasets(hdf5_file)\n",
    "    available_datasets = [_ for _ in available_datasets if \"date\" not in _]\n",
    "\n",
    "# Extract unique station names from available datasets\n",
    "available_stations = sorted(\n",
    "    set(dataset.split(\"/\")[0] for dataset in available_datasets)\n",
    ")\n",
    "\n",
    "# Create a dictionary mapping each station to its corresponding files to process\n",
    "file_to_process_dict = {\n",
    "    station: {\n",
    "        dataset.split(\"/\")[-1]\n",
    "        for dataset in available_datasets\n",
    "        if dataset.startswith(station)\n",
    "    }\n",
    "    for station in available_stations\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "349afa04-5808-4328-8b69-99547f3b4db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:43:28.840487Z",
     "iopub.status.busy": "2024-08-28T01:43:28.840487Z",
     "iopub.status.idle": "2024-08-28T01:44:49.242735Z",
     "shell.execute_reply": "2024-08-28T01:44:49.242735Z",
     "shell.execute_reply.started": "2024-08-28T01:43:28.840487Z"
    }
   },
   "outputs": [],
   "source": [
    "updates_dict = {}\n",
    "error_log = {}\n",
    "\n",
    "for ename in list(file_to_process_dict.keys())[:1]:\n",
    "    \"\"\"\n",
    "    {\n",
    "    'sensor_data':{\n",
    "                'date': new_data,\n",
    "                'well_1' : np.array(xyz, 2),\n",
    "                'well_2' : np.array(xyz, 2)\n",
    "                }\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    temp_data = {}\n",
    "\n",
    "    temp_date_idx = []\n",
    "    for wellcode in file_to_process_dict[ename]:\n",
    "        try:\n",
    "            wellcode, df = process_well_data(ename, wellcode, gwl_hdf5_file)\n",
    "            temp_data[wellcode] = df.values\n",
    "            temp_date_idx.extend(df.index.strftime(\"%Y%m%d\").tolist())\n",
    "        except Exception as e:\n",
    "            error_log.setdefault(ename, {})[wellcode] = str(e)\n",
    "\n",
    "    temp_data[\"date\"] = np.array(sorted(set(temp_date_idx)), dtype=\"S10\")\n",
    "    dict_to_update = {\n",
    "        ename: {\n",
    "            \"sensor_data\": temp_data,\n",
    "            \"metadata\": {\n",
    "                \"Updated Date\": datetime.now().strftime(\"%Y/%m/%d %H:%M:%S\"),\n",
    "                \"Description\": \"Model the groundwater level data to reduce noise and fill missing values\",\n",
    "            },\n",
    "        }\n",
    "    }\n",
    "    updates_dict.update(dict_to_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed6966f-77f1-4a70-b395-014f027a98be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-28T01:44:49.244736Z",
     "iopub.status.busy": "2024-08-28T01:44:49.244736Z",
     "iopub.status.idle": "2024-08-28T01:44:49.293737Z",
     "shell.execute_reply": "2024-08-28T01:44:49.293737Z",
     "shell.execute_reply.started": "2024-08-28T01:44:49.244736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed.\n"
     ]
    }
   ],
   "source": [
    "# SECTION 5: Apply Updates to HDF5 File\n",
    "# -------------------------------------\n",
    "# Use the predefined function to update the HDF5 file with new data and metadata\n",
    "shutil.copy2(src=gwl_hdf5_file, dst=gwl_hdf5_file.replace(\".h5\", \"_secure.h5\"))\n",
    "gwatertools.h5pytools.update_hdf5(gwl_hdf5_file, updates_dict)\n",
    "\n",
    "\n",
    "# Save error log if any errors occurred\n",
    "if error_log:\n",
    "    error_log_path = \"error_log_test.txt\"\n",
    "    with open(error_log_path, \"w\") as f:\n",
    "        for ename, errors in error_log.items():\n",
    "            for wellcode, error_msg in errors.items():\n",
    "                f.write(f\"Error for {ename}/{wellcode}: {error_msg}\\n\")\n",
    "    print(\n",
    "        f\"Errors occurred during processing. See error log at: {error_log_path}\"\n",
    "    )\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
