{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa04e9f3-7a19-41bf-b52e-a019a7c3ac4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T10:06:17.295978Z",
     "iopub.status.busy": "2024-09-23T10:06:17.294978Z",
     "iopub.status.idle": "2024-09-23T10:06:19.462007Z",
     "shell.execute_reply": "2024-09-23T10:06:19.462007Z",
     "shell.execute_reply.started": "2024-09-23T10:06:17.295978Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823d7c60-61e9-4b86-bbcb-745711319b01",
   "metadata": {},
   "source": [
    "1. search for leveling benchmarks located within xyz (m) surrounding GPS stations, xyz ~ 1 km\n",
    "2. extract `alignment periods` between `leveling` and `GPS` data sets\n",
    "3. compare **average linear velocities** between `GPS` and `leveling` data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d44dc512-8a69-42aa-bb89-6c56a3c625e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T10:06:19.463007Z",
     "iopub.status.busy": "2024-09-23T10:06:19.463007Z",
     "iopub.status.idle": "2024-09-23T10:06:19.821007Z",
     "shell.execute_reply": "2024-09-23T10:06:19.821007Z",
     "shell.execute_reply.started": "2024-09-23T10:06:19.463007Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>樁號</th>\n",
       "      <th>中英對</th>\n",
       "      <th>點名</th>\n",
       "      <th>鄉鎮市</th>\n",
       "      <th>97縱座</th>\n",
       "      <th>97橫座</th>\n",
       "      <th>備註</th>\n",
       "      <th>始有記</th>\n",
       "      <th>最終記</th>\n",
       "      <th>id</th>\n",
       "      <th>STATION</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>井BD</td>\n",
       "      <td>JBD</td>\n",
       "      <td>布袋國小</td>\n",
       "      <td>布袋鎮</td>\n",
       "      <td>2586729.0</td>\n",
       "      <td>165431.0</td>\n",
       "      <td>None</td>\n",
       "      <td>1998-12-15</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>214.0</td>\n",
       "      <td>BDES</td>\n",
       "      <td>POINT (165431.000 2586729.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CYBDWW12</td>\n",
       "      <td>CYBDWW12</td>\n",
       "      <td>布袋水位站</td>\n",
       "      <td>布袋鎮</td>\n",
       "      <td>2586679.0</td>\n",
       "      <td>165348.0</td>\n",
       "      <td>None</td>\n",
       "      <td>2010-08-15</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>227.0</td>\n",
       "      <td>BDES</td>\n",
       "      <td>POINT (165348.000 2586679.000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>內部031</td>\n",
       "      <td>NB031</td>\n",
       "      <td>西港國小</td>\n",
       "      <td>大城鄉</td>\n",
       "      <td>2639747.0</td>\n",
       "      <td>177579.0</td>\n",
       "      <td>104年遺失</td>\n",
       "      <td>2000-06-15</td>\n",
       "      <td>2014-06-15</td>\n",
       "      <td>455.0</td>\n",
       "      <td>CHSG</td>\n",
       "      <td>POINT (177579.000 2639747.000)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         樁號       中英對     點名  鄉鎮市       97縱座      97橫座      備註         始有記  \\\n",
       "0       井BD       JBD   布袋國小  布袋鎮  2586729.0  165431.0    None  1998-12-15   \n",
       "1  CYBDWW12  CYBDWW12  布袋水位站  布袋鎮  2586679.0  165348.0    None  2010-08-15   \n",
       "2     內部031     NB031   西港國小  大城鄉  2639747.0  177579.0  104年遺失  2000-06-15   \n",
       "\n",
       "          最終記     id STATION                        geometry  \n",
       "0  2019-06-15  214.0    BDES  POINT (165431.000 2586729.000)  \n",
       "1  2019-06-15  227.0    BDES  POINT (165348.000 2586679.000)  \n",
       "2  2014-06-15  455.0    CHSG  POINT (177579.000 2639747.000)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ----> gps shapefile\n",
    "gps_df = gpd.read_file(r\"E:\\006_InSAR_Project_2022\\GPSDataWorkspace\\Selected_ActiveGPS_4Calibration_20220609.shp\")\n",
    "# ----> leveling shapefile\n",
    "leveling_df = gpd.read_file(\n",
    "    r\"E:\\SUBSIDENCE_PROJECT_DATA\\地陷資料整理\\水準點\\LevelingBenchmarks_DavidNCU\\shapefiles\\leveling_benchmark_location.shp\"\n",
    ")\n",
    "# ----> buffer radius\n",
    "buffer_radius = 200\n",
    "# ----> extract leveling points surrounding GPS stations\n",
    "results = [\n",
    "    geospatial.find_point_neighbors(\n",
    "        central_point=gps_df.iloc[i, :],\n",
    "        target_points_gdf=leveling_df,\n",
    "        central_key_column=\"STATION\",\n",
    "        buffer_radius=buffer_radius,\n",
    "    )\n",
    "    for i in range(len(gps_df))\n",
    "]\n",
    "\n",
    "result_gdf = pd.concat(results, ignore_index=True)\n",
    "result_gdf.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2634b25-2a02-4dee-a282-b338a01a8710",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T10:06:19.823007Z",
     "iopub.status.busy": "2024-09-23T10:06:19.822009Z",
     "iopub.status.idle": "2024-09-23T10:06:19.826007Z",
     "shell.execute_reply": "2024-09-23T10:06:19.826007Z",
     "shell.execute_reply.started": "2024-09-23T10:06:19.822009Z"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_dictionary(df, output_date_colname=\"date\", output_value_colname=\"value\"):\n",
    "    return {output_date_colname: df.index.strftime(\"%Y%m%d\").to_numpy(), output_value_colname: df.values.flatten()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f500f8f-67d4-4963-bd4e-987b1c9422a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T10:06:19.827978Z",
     "iopub.status.busy": "2024-09-23T10:06:19.827978Z",
     "iopub.status.idle": "2024-09-23T10:06:23.185978Z",
     "shell.execute_reply": "2024-09-23T10:06:23.185978Z",
     "shell.execute_reply.started": "2024-09-23T10:06:19.827978Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5308995cf6754fbb86e32a60e9a91986",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define paths to folders containing GPS and leveling data\n",
    "gps_folder = (\n",
    "    r\"E:\\GPSData+4Calibration+PSInSAR\\AVAILABLE_4CALIBRATION\\GPS_nearby\"  # Path to the folder containing GPS data\n",
    ")\n",
    "leveling_folder = r\"E:\\SUBSIDENCE_PROJECT_DATA\\website_LandSubsidence-wra-gov-tw\\Leveling_Download_20240722\"  # Path to the folder containing leveling data\n",
    "\n",
    "# Initialize a list to store processed data for each station\n",
    "cache = []\n",
    "\n",
    "cache2 = {\n",
    "    \"STATION\": [],\n",
    "    \"GPS(mm/year)\": [],\n",
    "    \"Leveling(mm/year)\": [],\n",
    "}\n",
    "\n",
    "# Extract a list of unique station names from the result_gdf DataFrame\n",
    "available_stations = result_gdf[\"STATION\"].unique().tolist()\n",
    "\n",
    "# Iterate through each available station to process the data\n",
    "for select_station in tqdm(available_stations):\n",
    "    # Extract rows from the 'result_gdf' DataFrame that match the current selected station\n",
    "    gdf_bySTATION = result_gdf.query(\"STATION == @select_station\")\n",
    "\n",
    "    # Load GPS data for the selected station from CSV, using 'Datetime' and 'dU(mm)' columns\n",
    "    gps_fpath = os.path.join(gps_folder, select_station + \".csv\")\n",
    "    if not os.path.isfile(gps_fpath):\n",
    "        continue  # Skip this station if GPS data is not available\n",
    "\n",
    "    # Read the GPS data\n",
    "    gps_data_arr = pd.read_csv(\n",
    "        gps_fpath,\n",
    "        usecols=[\"Datetime\", \"dU(mm)\"],\n",
    "        parse_dates=[\"Datetime\"],\n",
    "        index_col=[\"Datetime\"],\n",
    "    )\n",
    "\n",
    "    gps_data_arr = gps_data_arr.interpolate(method=\"piecewise_polynomial\")\n",
    "\n",
    "    # Initialize lists to store pairs of station benchmarks and their respective velocities\n",
    "    station_benchmark_pairs = []\n",
    "    velocity_pairs = []\n",
    "\n",
    "    # Track if there's at least one valid leveling benchmark for this station\n",
    "    has_valid_leveling_benchmark = False\n",
    "\n",
    "    # Iterate through each leveling benchmark associated with the selected GPS station\n",
    "    for idx, row in gdf_bySTATION.iterrows():\n",
    "        loc_code, loc_name = row[[\"樁號\", \"點名\"]]\n",
    "\n",
    "        # Search for the corresponding leveling data file for the selected location code and name in the leveling folder\n",
    "        leveling_files = glob(os.path.join(leveling_folder, \"*\", f\"{loc_code}*{loc_name}*.xlsx\"))\n",
    "        if not leveling_files:\n",
    "            continue  # Skip if no matching leveling file is found\n",
    "\n",
    "        # Assuming only one file is expected, take the first match\n",
    "        leveling_fpath_byCODENAME = leveling_files[0]\n",
    "\n",
    "        try:\n",
    "            # Load the leveling data from the identified Excel file\n",
    "            leveling_data_arr = pd.read_excel(leveling_fpath_byCODENAME, usecols=[1, 2], parse_dates=[0], index_col=[0])\n",
    "        except Exception:\n",
    "            continue  # Skip in case of file read errors\n",
    "\n",
    "        if len(leveling_data_arr) <= 3:\n",
    "            continue  # Skip if there are insufficient data points\n",
    "\n",
    "        # Set the flag indicating this station has valid leveling benchmarks\n",
    "        has_valid_leveling_benchmark = True\n",
    "\n",
    "        # Append the leveling data to the cache list\n",
    "        cache.append(\n",
    "            {\n",
    "                select_station: {\n",
    "                    \"LevelingList\": {f\"{loc_code}_{loc_name}\": prepare_dictionary(leveling_data_arr, \"date\", \"h(m)\")}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Determine the overlapping date range shared by both GPS and leveling data\n",
    "        start_idx_date = max(gps_data_arr.first_valid_index(), leveling_data_arr.first_valid_index())\n",
    "        end_idx_date = max(gps_data_arr.last_valid_index(), leveling_data_arr.last_valid_index())\n",
    "\n",
    "        aligned_gps_arr = gps_data_arr.loc[start_idx_date:end_idx_date]\n",
    "        aligned_leveling_arr = leveling_data_arr.loc[start_idx_date:end_idx_date]\n",
    "\n",
    "        leveling_years = aligned_leveling_arr.index.year.unique().size\n",
    "        if leveling_years < 3:\n",
    "            continue  # Skip if the leveling data is insufficient\n",
    "\n",
    "        # Normalize data\n",
    "        aligned_gps_arr -= aligned_gps_arr.iloc[0, 0]\n",
    "        aligned_leveling_arr = (aligned_leveling_arr - aligned_leveling_arr.iloc[0, 0]) * 1000\n",
    "\n",
    "        # Convert to daily intervals\n",
    "        daily_leveling_arr = datetime_handle.fulltime_table(\n",
    "            aligned_leveling_arr, datetime_handle.get_fulltime(aligned_leveling_arr.index)\n",
    "        )\n",
    "\n",
    "        # Calculate linear trends and velocities\n",
    "        gps_linear_trend, gps_velocity = analysis.get_linear_trend(\n",
    "            aligned_gps_arr.iloc[:, 0], force_zero_intercept=True\n",
    "        )\n",
    "        leveling_linear_trend, leveling_velocity = analysis.get_linear_trend(\n",
    "            daily_leveling_arr.iloc[:, 0], force_zero_intercept=True\n",
    "        )\n",
    "\n",
    "        # Convert velocities to mm/year\n",
    "        gps_velocity_year = gps_velocity * 365.25\n",
    "        leveling_velocity_year = leveling_velocity * 365.25\n",
    "\n",
    "        # Append the current station and benchmark pairing\n",
    "        station_benchmark_pairs.append([select_station, f\"{loc_code}_{loc_name}\"])\n",
    "        velocity_pairs.append([gps_velocity_year, leveling_velocity_year])\n",
    "\n",
    "        # Update cache2 dictionary\n",
    "        for key, value in zip(\n",
    "            [\"STATION\", \"GPS(mm/year)\", \"Leveling(mm/year)\"],\n",
    "            [select_station, gps_velocity_year, leveling_velocity_year],\n",
    "        ):\n",
    "            cache2[key].append(value)\n",
    "\n",
    "    # Only store the GPS data and calculated velocities if there was at least one valid leveling benchmark\n",
    "    if has_valid_leveling_benchmark:\n",
    "        cache.append({select_station: prepare_dictionary(gps_data_arr, \"date\", \"dU(mm)\")})\n",
    "        # Encode each element in the station-benchmark pairs array to Big5 encoding\n",
    "        station_benchmark_pairs = np.array(\n",
    "            [[element.encode(\"big5\") for element in row] for row in station_benchmark_pairs]\n",
    "        )\n",
    "        # Append the velocities data to the cache for the current station\n",
    "        cache.append(\n",
    "            {\n",
    "                select_station: {\n",
    "                    \"velocities\": {\"Pairs\": np.array(station_benchmark_pairs), \"Values\": np.array(velocity_pairs)}\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ec6d768-e102-4ad5-bd18-94a7f054a55e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T09:43:38.338734Z",
     "iopub.status.busy": "2024-09-23T09:43:38.338734Z",
     "iopub.status.idle": "2024-09-23T09:43:45.646220Z",
     "shell.execute_reply": "2024-09-23T09:43:45.646220Z",
     "shell.execute_reply.started": "2024-09-23T09:43:38.338734Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Define paths to folders containing GPS and leveling data\n",
    "gps_folder = (\n",
    "    r\"E:\\GPSData+4Calibration+PSInSAR\\AVAILABLE_4CALIBRATION\\GPS_nearby\"  # Path to the folder containing GPS data\n",
    ")\n",
    "leveling_folder = r\"E:\\SUBSIDENCE_PROJECT_DATA\\website_LandSubsidence-wra-gov-tw\\Leveling_Download_20240722\"  # Path to the folder containing leveling data\n",
    "\n",
    "# Initialize a list to store processed data for each station\n",
    "cache = []\n",
    "\n",
    "cache2 = {\n",
    "    \"STATION\": [],\n",
    "    \"GPS(mm/year)\": [],\n",
    "    \"Leveling(mm/year)\": [],\n",
    "}\n",
    "\n",
    "# Extract a list of unique station names from the result_gdf DataFrame\n",
    "available_stations = result_gdf[\"STATION\"].unique().tolist()\n",
    "\n",
    "# Iterate through each available station to process the data\n",
    "for select_station in tqdm(available_stations):\n",
    "    # Extract rows from the 'result_gdf' DataFrame that match the current selected station\n",
    "    gdf_bySTATION = result_gdf.query(\"STATION == @select_station\")\n",
    "\n",
    "    # Load GPS data for the selected station from CSV, using 'Datetime' and 'dU(mm)' columns\n",
    "    gps_fpath = os.path.join(gps_folder, select_station + \".csv\")\n",
    "    if not os.path.isfile(gps_fpath):\n",
    "        continue  # Skip this station if GPS data is not available\n",
    "\n",
    "    # Read the GPS data\n",
    "    gps_data_arr = pd.read_csv(\n",
    "        gps_fpath,\n",
    "        usecols=[\"Datetime\", \"dU(mm)\"],\n",
    "        parse_dates=[\"Datetime\"],\n",
    "        index_col=[\"Datetime\"],\n",
    "    )\n",
    "\n",
    "    # Append the GPS data to the cache list after converting it to the desired format\n",
    "    cache.append({select_station: prepare_dictionary(gps_data_arr, \"date\", \"dU(mm)\")})\n",
    "\n",
    "    gps_data_arr = gps_data_arr.interpolate(method=\"piecewise_polynomial\")\n",
    "\n",
    "    # Initialize lists to store pairs of station benchmarks and their respective velocities\n",
    "    station_benchmark_pairs = []\n",
    "    velocity_pairs = []\n",
    "\n",
    "    # Iterate through each leveling benchmark associated with the selected GPS station\n",
    "    for idx, row in gdf_bySTATION.iterrows():\n",
    "        loc_code, loc_name = row[[\"樁號\", \"點名\"]]\n",
    "\n",
    "        # Search for the corresponding leveling data file for the selected location code and name in the leveling folder\n",
    "        leveling_files = glob(os.path.join(leveling_folder, \"*\", f\"{loc_code}*{loc_name}*.xlsx\"))\n",
    "        if not leveling_files:\n",
    "            continue  # Skip if no matching leveling file is found\n",
    "\n",
    "        # Assuming only one file is expected, take the first match\n",
    "        leveling_fpath_byCODENAME = leveling_files[0]\n",
    "\n",
    "        try:\n",
    "            # Load the leveling data from the identified Excel file\n",
    "            leveling_data_arr = pd.read_excel(leveling_fpath_byCODENAME, usecols=[1, 2], parse_dates=[0], index_col=[0])\n",
    "        except Exception:\n",
    "            continue  # Skip in case of file read errors\n",
    "\n",
    "        if len(leveling_data_arr) <= 3:\n",
    "            continue  # Skip if there are insufficient data points\n",
    "\n",
    "        # Append the leveling data to the cache list\n",
    "        cache.append(\n",
    "            {select_station: {f\"{loc_code}_{loc_name}\": prepare_dictionary(leveling_data_arr, \"date\", \"h(m)\")}}\n",
    "        )\n",
    "\n",
    "        # Determine the overlapping date range shared by both GPS and leveling data\n",
    "        start_idx_date = max(gps_data_arr.first_valid_index(), leveling_data_arr.first_valid_index())\n",
    "        end_idx_date = max(gps_data_arr.last_valid_index(), leveling_data_arr.last_valid_index())\n",
    "\n",
    "        aligned_gps_arr = gps_data_arr.loc[start_idx_date:end_idx_date]\n",
    "        aligned_leveling_arr = leveling_data_arr.loc[start_idx_date:end_idx_date]\n",
    "\n",
    "        leveling_years = aligned_leveling_arr.index.year.unique().size\n",
    "        if leveling_years < 3:\n",
    "            continue  # Skip if the leveling data is insufficient\n",
    "\n",
    "        # Normalize data\n",
    "        aligned_gps_arr -= aligned_gps_arr.iloc[0, 0]\n",
    "        aligned_leveling_arr = (aligned_leveling_arr - aligned_leveling_arr.iloc[0, 0]) * 1000\n",
    "\n",
    "        # Convert to daily intervals\n",
    "        daily_leveling_arr = datetime_handle.fulltime_table(\n",
    "            aligned_leveling_arr, datetime_handle.get_fulltime(aligned_leveling_arr.index)\n",
    "        )\n",
    "\n",
    "        # Calculate linear trends and velocities\n",
    "        gps_linear_trend, gps_velocity = analysis.get_linear_trend(\n",
    "            aligned_gps_arr.iloc[:, 0], force_zero_intercept=True\n",
    "        )\n",
    "        leveling_linear_trend, leveling_velocity = analysis.get_linear_trend(\n",
    "            daily_leveling_arr.iloc[:, 0], force_zero_intercept=True\n",
    "        )\n",
    "\n",
    "        # Convert velocities to mm/year\n",
    "        gps_velocity_year = gps_velocity * 365.25\n",
    "        leveling_velocity_year = leveling_velocity * 365.25\n",
    "\n",
    "        # Append the current station and benchmark pairing\n",
    "        station_benchmark_pairs.append([select_station, f\"{loc_code}_{loc_name}\"])\n",
    "        velocity_pairs.append([gps_velocity_year, leveling_velocity_year])\n",
    "\n",
    "        # Update cache2 dictionary\n",
    "        for key, value in zip(\n",
    "            [\"STATION\", \"GPS(mm/year)\", \"Leveling(mm/year)\"],\n",
    "            [select_station, gps_velocity_year, leveling_velocity_year],\n",
    "        ):\n",
    "            cache2[key].append(value)\n",
    "\n",
    "    # Encode each element in the station-benchmark pairs array to Big5 encoding\n",
    "    station_benchmark_pairs = np.array([[element.encode(\"big5\") for element in row] for row in station_benchmark_pairs])\n",
    "\n",
    "    # Append the velocities data to the cache for the current station\n",
    "    cache.append(\n",
    "        {\n",
    "            select_station: {\n",
    "                \"velocites\": {\"Pairs\": np.array(station_benchmark_pairs), \"Values\": np.array(velocity_pairs)}\n",
    "            }\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1cecbc7-6262-4638-9e38-276728bad6dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T10:06:23.186977Z",
     "iopub.status.busy": "2024-09-23T10:06:23.186977Z",
     "iopub.status.idle": "2024-09-23T10:06:23.298979Z",
     "shell.execute_reply": "2024-09-23T10:06:23.298979Z",
     "shell.execute_reply.started": "2024-09-23T10:06:23.186977Z"
    }
   },
   "outputs": [],
   "source": [
    "dict_to_saveHDF5 = gwatertools.h5pytools.merge_dicts(*cache)\n",
    "\n",
    "today_string = datetime.now().strftime(\"%Y%m%d\")\n",
    "\n",
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(f\"{today_string}_GPS_Leveling_LinearVelocity_{buffer_radius}m.h5\", \"w\") as hdf5_file:\n",
    "    gwatertools.h5pytools.data_to_hdf5(hdf5_file, dict_to_saveHDF5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e53c969-a5c9-489e-a4eb-e4993fec2a85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T10:06:23.300979Z",
     "iopub.status.busy": "2024-09-23T10:06:23.300979Z",
     "iopub.status.idle": "2024-09-23T10:06:23.372979Z",
     "shell.execute_reply": "2024-09-23T10:06:23.372979Z",
     "shell.execute_reply.started": "2024-09-23T10:06:23.300979Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(cache2).to_excel(f\"{today_string}_GPS_Leveling_Compare_{buffer_radius}m.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d0f41-6d60-4bb6-bc21-6b3e8b8ce0b9",
   "metadata": {},
   "source": [
    "#### decode array"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8a0a556a-61f4-42e9-9769-6d62cd6efee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T08:33:28.724170Z",
     "iopub.status.busy": "2024-09-23T08:33:28.723171Z",
     "iopub.status.idle": "2024-09-23T08:33:28.732171Z",
     "shell.execute_reply": "2024-09-23T08:33:28.732171Z",
     "shell.execute_reply.started": "2024-09-23T08:33:28.724170Z"
    }
   },
   "source": [
    "gwl_hdf5_file = \"20240923_test.h5\"\n",
    "\n",
    "# Extract existing data and metadata\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    existing_data_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "872ff23a-71ee-491f-905c-b3d246434e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-23T08:34:57.219912Z",
     "iopub.status.busy": "2024-09-23T08:34:57.219912Z",
     "iopub.status.idle": "2024-09-23T08:34:57.224910Z",
     "shell.execute_reply": "2024-09-23T08:34:57.224910Z",
     "shell.execute_reply.started": "2024-09-23T08:34:57.219912Z"
    }
   },
   "source": [
    "read_data = existing_data_dict['GFES']['velocites'][\"Pairs\"]\n",
    "decoded_data = np.array([[element.decode('big5') for element in row] for row in read_data])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "173b8c0139444207a7d10a5850a521b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "1836dc3e206649c0b6d266f3057a33d9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "46e60a06e57c44708c9f85babfa319e3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_514dfae3dbfe4dde85cff14f84acb71e",
       "style": "IPY_MODEL_bd392705f16d493fb4f375fa842c0cbd",
       "value": " 20/20 [00:03&lt;00:00,  4.80it/s]"
      }
     },
     "514dfae3dbfe4dde85cff14f84acb71e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5260d00b70304c36bccba55f04468923": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "5308995cf6754fbb86e32a60e9a91986": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "children": [
        "IPY_MODEL_bfa141a5bffc441d8e9a7b1f6addb18c",
        "IPY_MODEL_8ed2e3e049de42fa80918788a3e52b67",
        "IPY_MODEL_46e60a06e57c44708c9f85babfa319e3"
       ],
       "layout": "IPY_MODEL_1836dc3e206649c0b6d266f3057a33d9"
      }
     },
     "55a466e072c6402f90f724ff48f0d170": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "description_width": ""
      }
     },
     "8ed2e3e049de42fa80918788a3e52b67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "bar_style": "success",
       "layout": "IPY_MODEL_5260d00b70304c36bccba55f04468923",
       "max": 20,
       "style": "IPY_MODEL_55a466e072c6402f90f724ff48f0d170",
       "value": 20
      }
     },
     "a36a433a5ccc4aef84c6595e9172cbed": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bd392705f16d493fb4f375fa842c0cbd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "bfa141a5bffc441d8e9a7b1f6addb18c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "layout": "IPY_MODEL_a36a433a5ccc4aef84c6595e9172cbed",
       "style": "IPY_MODEL_173b8c0139444207a7d10a5850a521b4",
       "value": "100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
