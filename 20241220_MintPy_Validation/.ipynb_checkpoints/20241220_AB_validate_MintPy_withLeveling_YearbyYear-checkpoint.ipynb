{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ebeea3-a89f-4b09-b56b-aeae361b7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe64da77-de23-4ec3-9021-cd4edc5e14f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Configuration Parameters\n",
    "# ====================================================\n",
    "BUFFER_RADIUS = 100  # Buffer radius for InSAR point search, in meters\n",
    "\n",
    "MAINFOLDER = r\"E:\\001_InSAR_Project\\1100_CHOUSHUI_2025\\CRFP_full_2016_2024\\asc_desc_run003L\\post-analysis2\"\n",
    "PROCESS_FOLDERS = glob(os.path.join(MAINFOLDER, \"20*\"))\n",
    "if not PROCESS_FOLDERS:\n",
    "    raise ValueError(\"No folders found in the main directory matching the pattern.\")\n",
    "\n",
    "# ====================================================\n",
    "# File Paths\n",
    "# ====================================================\n",
    "LEVELING_SHP_PATH = (\n",
    "    r\"E:\\SUBSIDENCE_PROJECT_DATA\\地陷資料整理\\水準點\\LevelingBenchmarks_DavidNCU\\shapefiles\\leveling_benchmark_location.shp\"\n",
    ")\n",
    "VERTICAL_DISP_FILE_PATTERN = f\"*{BUFFER_RADIUS}m*.shp\"\n",
    "HDF5_FILE_PATH = r\"D:\\1000_SCRIPTS\\003_Project002\\20241219_LevelingData_to_HDF5\\20241220_All_LevelingData_LandSubsidence-wra-gov-tw.h5\"\n",
    "\n",
    "# ====================================================\n",
    "# Load Data\n",
    "# ====================================================\n",
    "print(\"Loading leveling benchmark shapefile...\")\n",
    "leveling_shp = gpd.read_file(LEVELING_SHP_PATH)\n",
    "\n",
    "for SELECT_FOLDER in tqdm(PROCESS_FOLDERS, desc=\"Processing folders\"):\n",
    "    BASENAME = os.path.basename(SELECT_FOLDER)\n",
    "    START_YEAR = int(BASENAME.split(\"_\")[0][:4])\n",
    "    END_YEAR = int(BASENAME.split(\"_\")[-1][:4])\n",
    "\n",
    "    print(f\"Searching for vertical displacement shapefile in {SELECT_FOLDER}...\")\n",
    "    vertical_disp_fpath = glob(os.path.join(SELECT_FOLDER, VERTICAL_DISP_FILE_PATTERN))\n",
    "    if not vertical_disp_fpath:\n",
    "        print(f\"No matching vertical displacement shapefile found in {SELECT_FOLDER}\")\n",
    "        continue\n",
    "    vertical_disp_fpath = vertical_disp_fpath[0]\n",
    "    vertical_disp_shp = gpd.read_file(vertical_disp_fpath)\n",
    "\n",
    "    # ====================================================\n",
    "    # Find Neighbors for Each Benchmark\n",
    "    # ====================================================\n",
    "    CENTRAL_KEY_COLUMN = \"樁號\"  # Key column for benchmark identifier\n",
    "    results = []\n",
    "\n",
    "    print(\"Finding neighbors for each leveling benchmark...\")\n",
    "    for idx, row in tqdm(leveling_shp.iterrows(), total=len(leveling_shp)):\n",
    "        try:\n",
    "            result = geospatial.find_point_neighbors(row, vertical_disp_shp, CENTRAL_KEY_COLUMN, BUFFER_RADIUS)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {idx}: {e}\")\n",
    "\n",
    "    # Combine results into a single GeoDataFrame\n",
    "    result_gdf = gpd.GeoDataFrame(pd.concat(results, ignore_index=True))\n",
    "\n",
    "    # Save results as a pickle file\n",
    "    save_path = os.path.join(\n",
    "        SELECT_FOLDER, os.path.basename(vertical_disp_fpath).replace(\".shp\", \"_add_StationNum.pkl\")\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "    result_gdf.to_pickle(save_path)\n",
    "\n",
    "    # ====================================================\n",
    "    # Load HDF5 Leveling Data\n",
    "    # ====================================================\n",
    "    print(\"Loading leveling data from HDF5 file...\")\n",
    "    with h5py.File(HDF5_FILE_PATH, \"r\") as hdf5_file:\n",
    "        leveling_measure_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "        leveling_metadata_dict = gwatertools.h5pytools.hdf5_to_metadata_dict(hdf5_file)\n",
    "\n",
    "    # Extract unique benchmarks in the buffer zone\n",
    "    unique_leveling_StationNumber = result_gdf[CENTRAL_KEY_COLUMN].unique()\n",
    "\n",
    "    # Filter valid benchmarks available in the database\n",
    "    valid_leveling_StationNumber = sorted(\n",
    "        set(leveling_metadata_dict.keys()).intersection(set(unique_leveling_StationNumber))\n",
    "    )\n",
    "\n",
    "    # ====================================================\n",
    "    # Filter Benchmarks by Time Interval\n",
    "    # ====================================================\n",
    "    satisfied_leveling_StationNumber = []\n",
    "    print(\"Filtering benchmarks by time interval...\")\n",
    "    for select_StationNumber in tqdm(valid_leveling_StationNumber):\n",
    "        if select_StationNumber not in leveling_metadata_dict:\n",
    "            print(f\"Skipping {select_StationNumber}: Metadata not found.\")\n",
    "            continue\n",
    "        metadata = leveling_metadata_dict[select_StationNumber]\n",
    "        first_record_date = pd.to_datetime(metadata[\"First_Record\"])\n",
    "        last_record_date = pd.to_datetime(metadata[\"Last_Record\"])\n",
    "        if first_record_date.year <= START_YEAR and last_record_date.year >= END_YEAR:\n",
    "            satisfied_leveling_StationNumber.append(select_StationNumber)\n",
    "\n",
    "    # ====================================================\n",
    "    # Compare Leveling and InSAR Measurements\n",
    "    # ====================================================\n",
    "    cache = {\"StationNumber\": [], \"Leveling_cm_yr\": [], \"InSAR_cm_yr\": []}\n",
    "\n",
    "    print(\"Comparing leveling and InSAR measurements...\")\n",
    "    for select_StationNumber in tqdm(satisfied_leveling_StationNumber):\n",
    "        try:\n",
    "            # Leveling Measurements\n",
    "            leveling_data = pd.DataFrame(leveling_measure_dict[select_StationNumber])\n",
    "            leveling_data[\"date\"] = leveling_data[\"date\"].apply(\n",
    "                lambda x: x.decode(\"utf-8\") if isinstance(x, bytes) else x\n",
    "            )\n",
    "            leveling_data[\"date\"] = pd.to_datetime(leveling_data[\"date\"])\n",
    "            leveling_data.set_index(\"date\", inplace=True)\n",
    "            subset_leveling_data = leveling_data.loc[str(START_YEAR) : str(END_YEAR)]\n",
    "\n",
    "            # Calculate leveling velocity\n",
    "            index_arr = subset_leveling_data.index\n",
    "            y_arr = subset_leveling_data[\"values\"].to_numpy()\n",
    "            days_diff = np.array((index_arr - index_arr[0]).days)\n",
    "            trend, coeffs = analysis.get_polynomial_trend(days_diff, y_arr, order=1)\n",
    "            leveling_slope = coeffs[-1] * 365.25  # m/year\n",
    "            leveling_slope_cm = leveling_slope * 100\n",
    "\n",
    "            # InSAR Measurements\n",
    "            insar_data = result_gdf.query(f\"{CENTRAL_KEY_COLUMN}==@select_StationNumber\")\n",
    "            insar_velocity = insar_data[\"grid_code\"].mean() * 100  # Convert to cm/year\n",
    "\n",
    "            # Append results to cache\n",
    "            cache[\"StationNumber\"].append(select_StationNumber)\n",
    "            cache[\"Leveling_cm_yr\"].append(leveling_slope_cm)\n",
    "            cache[\"InSAR_cm_yr\"].append(insar_velocity)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {select_StationNumber}: {e}\")\n",
    "\n",
    "    # ====================================================\n",
    "    # Save Results\n",
    "    # ====================================================\n",
    "    output_path = os.path.join(os.path.dirname(save_path), f\"Leveling_InSAR_inBuffer_{BUFFER_RADIUS}m.xlsx\")\n",
    "    leveling_insar_df = pd.DataFrame(cache)\n",
    "    leveling_insar_df.to_excel(output_path, index=False)\n",
    "\n",
    "    print(f\"Results saved to {output_path}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
