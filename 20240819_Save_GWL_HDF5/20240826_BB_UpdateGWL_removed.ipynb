{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec81082-aac9-44eb-85f0-c5e7450673bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc38a483-9c8a-43fc-9c4f-da6f03dbc6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the current working directory (script folder)\n",
    "script_folder = os.getcwd()\n",
    "\n",
    "# SECTION 1: Setup and Backup\n",
    "# ----------------------------\n",
    "# Define the path to the HDF5 file to be updated\n",
    "gwl_hdf5_file = r\"D:\\1000_SCRIPTS\\002_PostQE_Scripts\\20240819_Save_GWL_HDF5\\20240826_GWL_CRFP.h5\"\n",
    "\n",
    "# Create a backup of the original HDF5 file\n",
    "shutil.copy2(src=gwl_hdf5_file, dst=gwl_hdf5_file.replace(\".h5\", \"_secure.h5\"))\n",
    "\n",
    "# List available datasets in the HDF5 file for reference\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    available_datasets = gwatertools.h5pytools.list_datasets(hdf5_file)\n",
    "\n",
    "# SECTION 2: Data Collection Setup\n",
    "# --------------------------------\n",
    "# Define the main folder containing new groundwater data to be imported\n",
    "new_data_mainfolder = r\"D:\\VINHTRUONG\\004_MODELING\\001_STUDY_AREA\\GroundwaterObservation\\@DOWNLOAD_WRA_GWOB_YEARBOOK_PROJECT\\@groundwater_level_PDF\\GW_DATA_gweb.wra.gov.tw\\COMBINE\\2023\"\n",
    "os.chdir(new_data_mainfolder)\n",
    "\n",
    "# Initialize containers for new data and error logs\n",
    "stations_fld = [f for f in os.listdir() if os.path.isdir(f)]  # List of station folders\n",
    "new_data = {}  # Dictionary to store new data\n",
    "error_log = {}  # Dictionary to log errors\n",
    "\n",
    "# SECTION 3: Data Extraction and Merging\n",
    "# --------------------------------------\n",
    "# Iterate through each station folder to extract and merge new data\n",
    "for select_station in tqdm(stations_fld):\n",
    "    try:\n",
    "        # Extract English name, Chinese name, and abbreviation from the station folder name\n",
    "        ename, cname, abbrev = select_station.upper().split(\"_\")\n",
    "        temp = {ename: {}}  # Temporary storage for current station data\n",
    "\n",
    "        # List all Excel files in the current station folder\n",
    "        files_byStation = glob(select_station + \"\\\\*.xlsx\")\n",
    "\n",
    "        for select_file in files_byStation:\n",
    "            wellcode = os.path.basename(select_file).split(\".\")[0]  # Extract well code from filename\n",
    "\n",
    "            # Retrieve existing monitoring data from the HDF5 file\n",
    "            df_fromHDF5 = gwatertools.h5pytools.export_data_to_dataframe(\n",
    "                file_name=gwl_hdf5_file,\n",
    "                location_name=ename,\n",
    "                sensor_name=wellcode,\n",
    "            )\n",
    "\n",
    "            if df_fromHDF5 is not None:\n",
    "                df_fromHDF5 = df_fromHDF5.set_index(\"datetime\")\n",
    "\n",
    "                # Load new monitoring data from the Excel file\n",
    "                df_NewData = pd.read_excel(select_file, parse_dates=[0], index_col=[0])\n",
    "\n",
    "                # Combine existing and new data, prioritizing new data where available\n",
    "                df_fromHDF5_filled = df_fromHDF5.combine_first(df_NewData)\n",
    "\n",
    "                # Update the temporary data storage\n",
    "                temp[ename].update({wellcode: df_fromHDF5_filled[\"value\"]})\n",
    "            else:\n",
    "                logging.warning(f\"No data found for {ename}, {wellcode} in HDF5.\")\n",
    "\n",
    "        # Update the main new_data dictionary with the current station data\n",
    "        new_data.update(temp)\n",
    "    except Exception as e:\n",
    "        # Log errors encountered during data extraction and merging\n",
    "        logging.error(f\"Error processing station {select_station}: {e}\")\n",
    "        error_log.update({select_station: e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acdc68d-b36f-4e63-a97a-8b45700467b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SECTION 4: Prepare Update Dictionary\n",
    "# ------------------------------------\n",
    "# Transform the new data into the format required for updating the HDF5 file\n",
    "updates_dict = {}\n",
    "\n",
    "for station, wellcode in new_data.items():\n",
    "    try:\n",
    "        # Generate metadata for each well in the station\n",
    "        well_metadata = {\n",
    "            well: {\n",
    "                \"FIRST_OBS\": series.first_valid_index().strftime(\"%Y%m%d\"),\n",
    "                \"LAST_OBS\": series.last_valid_index().strftime(\"%Y%m%d\"),\n",
    "            }\n",
    "            for well, series in wellcode.items()\n",
    "        }\n",
    "        well_data = {well: series.values for well, series in wellcode.items()}\n",
    "        \n",
    "        # Gather all unique dates from the well data\n",
    "        dates = sorted(\n",
    "            set(\n",
    "                [\n",
    "                    series.index.strftime(\"%Y%m%d\").tolist()\n",
    "                    for series in wellcode.values()\n",
    "                ][0]\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Structure the update dictionary for each station\n",
    "        updates_dict[station] = {\n",
    "            \"sensor_data\": well_data,\n",
    "            \"metadata\": {  # Station-level metadata\n",
    "                \"Updated Date\": pd.Timestamp.now().strftime(\"%Y-%m-%d\"),\n",
    "            },\n",
    "            \"sensor_metadata\": well_metadata,  # Metadata specific to each sensor\n",
    "        }\n",
    "\n",
    "        # Include date information in the sensor data\n",
    "        updates_dict[station][\"sensor_data\"][\"date\"] = np.array(dates, dtype='S10')\n",
    "    except Exception as e:\n",
    "        print(station, e)  # Print errors for debugging\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a241bc-f8b3-460b-845a-b7d8acc49970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log the details of the updates that will be applied\n",
    "logging.info(f\"Updates prepared: {updates_dict}\")\n",
    "\n",
    "# SECTION 5: Apply Updates to HDF5 File\n",
    "# -------------------------------------\n",
    "# Use the predefined function to update the HDF5 file with new data and metadata\n",
    "gwatertools.h5pytools.update_hdf5(gwl_hdf5_file, updates_dict)\n",
    "\n",
    "# Log successful completion of updates\n",
    "logging.info(\"HDF5 file has been successfully updated.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
