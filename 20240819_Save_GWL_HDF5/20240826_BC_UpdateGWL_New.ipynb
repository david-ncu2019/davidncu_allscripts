{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2734371-8f34-444c-a308-a3ab41b0d746",
   "metadata": {},
   "source": [
    "Apply new functions in **`h5pytools`** to implement the transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec81082-aac9-44eb-85f0-c5e7450673bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:15.367478Z",
     "iopub.status.busy": "2024-08-29T03:54:15.366480Z",
     "iopub.status.idle": "2024-08-29T03:54:18.608383Z",
     "shell.execute_reply": "2024-08-29T03:54:18.608383Z",
     "shell.execute_reply.started": "2024-08-29T03:54:15.367478Z"
    }
   },
   "outputs": [],
   "source": [
    "from appgeopy import *\n",
    "from my_packages import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6920c9-ab88-475b-9e5e-617da22f3560",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:18.611384Z",
     "iopub.status.busy": "2024-08-29T03:54:18.610384Z",
     "iopub.status.idle": "2024-08-29T03:54:18.616381Z",
     "shell.execute_reply": "2024-08-29T03:54:18.615384Z",
     "shell.execute_reply.started": "2024-08-29T03:54:18.611384Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set the current working directory (script folder)\n",
    "script_folder = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b073a562-396f-4fba-85ef-620d9cbead3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:18.618382Z",
     "iopub.status.busy": "2024-08-29T03:54:18.617384Z",
     "iopub.status.idle": "2024-08-29T03:54:18.627383Z",
     "shell.execute_reply": "2024-08-29T03:54:18.626382Z",
     "shell.execute_reply.started": "2024-08-29T03:54:18.618382Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the HDF5 file to be updated\n",
    "gwl_hdf5_file = r\"D:\\1000_SCRIPTS\\002_PostQE_Scripts\\20240819_Save_GWL_HDF5\\20240828_GWL_CRFP.h5\"\n",
    "os.path.isfile(gwl_hdf5_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4991333b-c838-460a-8cbf-c7f9ad1d30c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:18.630382Z",
     "iopub.status.busy": "2024-08-29T03:54:18.629381Z",
     "iopub.status.idle": "2024-08-29T03:54:18.782380Z",
     "shell.execute_reply": "2024-08-29T03:54:18.782380Z",
     "shell.execute_reply.started": "2024-08-29T03:54:18.630382Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load station information from an Excel file\n",
    "station_info_excel = pd.read_excel(\n",
    "    r\"D:\\VINHTRUONG\\004_MODELING\\001_STUDY_AREA\\GroundwaterObservation\\@DOWNLOAD_WRA_GWOB_YEARBOOK_PROJECT\\Well_Information_CRAF_Active_Inactive_OneSheetOnly.xlsx\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7f01200-2ada-4f80-8fd6-38d5529c3161",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:18.784383Z",
     "iopub.status.busy": "2024-08-29T03:54:18.783381Z",
     "iopub.status.idle": "2024-08-29T03:54:18.976380Z",
     "shell.execute_reply": "2024-08-29T03:54:18.976380Z",
     "shell.execute_reply.started": "2024-08-29T03:54:18.784383Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a backup of the original HDF5 file\n",
    "shutil.copy2(src=gwl_hdf5_file, dst=gwl_hdf5_file.replace(\".h5\", \"_secure.h5\"))\n",
    "\n",
    "# List available datasets in the HDF5 file for reference\n",
    "with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "    available_datasets = gwatertools.h5pytools.list_datasets(hdf5_file)\n",
    "    datetime_idx = pd.to_datetime(hdf5_file[\"date\"][...], format=\"%Y%m%d\")\n",
    "\n",
    "stations_in_datasets = sorted(\n",
    "    set([ele.split(\"/\")[0] for ele in available_datasets])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7124e41-e280-432d-960a-0408b4a72481",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:18.978381Z",
     "iopub.status.busy": "2024-08-29T03:54:18.978381Z",
     "iopub.status.idle": "2024-08-29T03:54:18.989381Z",
     "shell.execute_reply": "2024-08-29T03:54:18.988383Z",
     "shell.execute_reply.started": "2024-08-29T03:54:18.978381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the main folder containing new groundwater data to be imported\n",
    "new_data_mainfolder = r\"D:\\VINHTRUONG\\004_MODELING\\001_STUDY_AREA\\GroundwaterObservation\\@DOWNLOAD_WRA_GWOB_YEARBOOK_PROJECT\\@groundwater_level_PDF\\GW_DATA_gweb.wra.gov.tw\\COMBINE\\2023\"\n",
    "os.chdir(new_data_mainfolder)\n",
    "\n",
    "# Initialize containers for new data and error logs\n",
    "stations_fld = [\n",
    "    f for f in os.listdir() if os.path.isdir(f)\n",
    "]  # List of station folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edca00db-934c-4b2b-b3cb-ad40ce3276b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:18.991383Z",
     "iopub.status.busy": "2024-08-29T03:54:18.991383Z",
     "iopub.status.idle": "2024-08-29T03:54:26.653396Z",
     "shell.execute_reply": "2024-08-29T03:54:26.653396Z",
     "shell.execute_reply.started": "2024-08-29T03:54:18.991383Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 92/92 [00:07<00:00, 12.04it/s]\n"
     ]
    }
   ],
   "source": [
    "all_stations_measurement_data = {}\n",
    "all_stations_metadata = {}\n",
    "\n",
    "# _____________________________________________\n",
    "# Loop through each station directory specified in `stations_fld`\n",
    "for select_station in tqdm(stations_fld):\n",
    "    # Retrieve all Excel files for the current station\n",
    "    files_byStation = glob(select_station + \"\\\\*.xlsx\")\n",
    "    # Split the station name into components (ename, cname, abbrev)\n",
    "    ename, cname, abbrev = select_station.upper().split(\"_\")\n",
    "\n",
    "    # Check if the station is already available in the existing datasets\n",
    "    flag = ename.upper() in stations_in_datasets\n",
    "\n",
    "    # Initialize dictionaries to hold measurement data and metadata for the current well\n",
    "    well_measurement_data = {}\n",
    "    well_metadata = {\"UpdatedTime\":datetime.now().strftime(\"%Y/%m/%d, %H:%M:%S\")}\n",
    "\n",
    "    # _____________________________________________\n",
    "    # If the station is available in the datasets, update it; otherwise, add new entries\n",
    "    if flag:\n",
    "        # _____________________________________________\n",
    "        # Loop through each file related to the current station\n",
    "        for select_file in files_byStation:\n",
    "            # Extract the well code from the filename (assumed to be the base name without extension)\n",
    "            wellcode = os.path.basename(select_file).split(\".\")[0]\n",
    "\n",
    "            # Find the dataset that matches the current well code\n",
    "            dataset_by_wellcode = next(\n",
    "                (ele for ele in available_datasets if wellcode in ele), None\n",
    "            )\n",
    "\n",
    "            # Open the HDF5 file in read mode to retrieve existing data for the well\n",
    "            with h5py.File(gwl_hdf5_file, \"r\") as hdf5_file:\n",
    "                # Read the existing dataset associated with the well code\n",
    "                value_arr = hdf5_file[dataset_by_wellcode][...]\n",
    "\n",
    "            # Create a DataFrame from the retrieved HDF5 data with datetime index\n",
    "            df_fromHDF5 = pd.DataFrame(\n",
    "                {\"time\": datetime_idx, \"daily_value\": value_arr}\n",
    "            )\n",
    "            df_fromHDF5 = df_fromHDF5.set_index(\"time\")\n",
    "\n",
    "            # _____________________________________________\n",
    "            # Load new monitoring data from the Excel file into a DataFrame\n",
    "            df_NewData = pd.read_excel(\n",
    "                select_file, parse_dates=[0], index_col=[0]\n",
    "            )\n",
    "            old_column_name = df_NewData.columns[0]\n",
    "            df_NewData = df_NewData.rename({old_column_name:\"daily_value\"}, axis=1)\n",
    "\n",
    "            # Combine existing data (from HDF5) with new data (from Excel)\n",
    "            # New data takes precedence over existing data\n",
    "            df_fromHDF5_filled = df_fromHDF5.fillna(df_NewData)\n",
    "\n",
    "            # _____________________________________________\n",
    "            # Update the well measurement data dictionary with combined data\n",
    "            well_measurement_data[wellcode] = {\n",
    "                \"measure\": {\n",
    "                    \"daily_value\": df_fromHDF5_filled[\"daily_value\"].values\n",
    "                }\n",
    "            }\n",
    "            # _____________________________________________\n",
    "            # Update well metadata with the first and last observation dates from combined data\n",
    "            first_obs = df_fromHDF5_filled.first_valid_index().strftime(\n",
    "                \"%Y/%m/%d\"\n",
    "            )\n",
    "            last_obs = df_fromHDF5_filled.last_valid_index().strftime(\n",
    "                \"%Y/%m/%d\"\n",
    "            )\n",
    "            well_metadata[wellcode] = {\n",
    "                \"FIRST_OBS\": first_obs,\n",
    "                \"LAST_OBS\": last_obs,\n",
    "            }\n",
    "    else:\n",
    "        # _____________________________________________\n",
    "        # If station is not available in the datasets, initialize metadata from an external source\n",
    "        station_info = station_info_excel.query(\"ENAME == @ename.lower()\")\n",
    "\n",
    "        # Initialize station metadata, defaulting to \"null\" if no station info is found\n",
    "        if station_info.empty:\n",
    "            station_metadata = {\"metadata\": \"null\"}\n",
    "        else:\n",
    "            # Extract relevant station info such as coordinates, address, etc.\n",
    "            x_twd97, y_twd97 = station_info.iloc[0][[\"X_TWD97\", \"Y_TWD97\"]]\n",
    "            address = station_info[\"ADDRESS\"].iloc[0]\n",
    "            num_of_wells = len(files_byStation)\n",
    "\n",
    "            # Populate station metadata with details\n",
    "            station_metadata = {\n",
    "                \"Chinese\": cname,\n",
    "                \"Abbreviation\": abbrev,\n",
    "                \"EPSG\": 3826,\n",
    "                \"X\": x_twd97,\n",
    "                \"Y\": y_twd97,\n",
    "                \"BasinENG\": \"Choshuichi Fan\",\n",
    "                \"BasinCHN\": \"濁水溪沖積扇\",\n",
    "                \"Num_of_Wells\": num_of_wells,\n",
    "                \"Address\": address,\n",
    "                \"CreatedTime\": datetime.now().strftime(\"%Y/%m/%d, %H:%M:%S\"),\n",
    "            }\n",
    "\n",
    "        # Update the overall metadata dictionary with the current station's metadata\n",
    "        well_metadata.update(station_metadata)\n",
    "        # _____________________________________________\n",
    "        # Loop through each file related to the current station to handle well-specific information\n",
    "        for select_file in files_byStation:\n",
    "            # Extract the well code from the filename\n",
    "            wellcode = os.path.basename(select_file).split(\".\")[0]\n",
    "            # Query well-specific information from the station info dataset\n",
    "            well_info = station_info.query(\"WELL_CODE == @wellcode\")\n",
    "\n",
    "            # Initialize or update well metadata based on the presence of well info\n",
    "            well_metadata[wellcode] = (\n",
    "                {\"metadata\": \"null\"}\n",
    "                if well_info.empty\n",
    "                else {\n",
    "                    \"WellName\": well_info[\"WELL_NAME\"].iloc[0],\n",
    "                    \"Well_Elev(m)\": well_info[\"WELL_ELEV(m)\"].iloc[0],\n",
    "                    \"Well_Depth(m)\": well_info[\"WELL_DEPTH(m)\"].iloc[0],\n",
    "                    \"Well_Screen(m)\": well_info[\"WELL_SCREEN(m)\"].iloc[0],\n",
    "                    \"Status\": \"Active\"\n",
    "                    if well_info[\"ACTIVE\"].iloc[0] == 1\n",
    "                    else \"Inactive\",\n",
    "                }\n",
    "            )\n",
    "            # _____________________________________________\n",
    "            # Load new monitoring data from the Excel file into a DataFrame\n",
    "            df_NewData = pd.read_excel(\n",
    "                select_file, parse_dates=[0], index_col=[0]\n",
    "            )\n",
    "            # Create a temporary DataFrame with a datetime index and map new data to it\n",
    "            temp = pd.DataFrame(index=datetime_idx)\n",
    "            temp[\"daily_value\"] = temp.index.map(df_NewData.iloc[:, 0])\n",
    "            # _____________________________________________\n",
    "            # Update well metadata with observation dates based on the temporary DataFrame\n",
    "            first_obs = temp.first_valid_index().strftime(\"%Y/%m/%d\")\n",
    "            last_obs = temp.last_valid_index().strftime(\"%Y/%m/%d\")\n",
    "            well_metadata[wellcode].update(\n",
    "                {\n",
    "                    \"FIRST_OBS\": first_obs,\n",
    "                    \"LAST_OBS\": last_obs,\n",
    "                }\n",
    "            )\n",
    "            # _____________________________________________\n",
    "            # Update well measurement data with new values from the temporary DataFrame\n",
    "            well_measurement_data[wellcode] = {\n",
    "                \"measure\": {\"daily_value\": temp[\"daily_value\"].values}\n",
    "            }\n",
    "\n",
    "    # _____________________________________________\n",
    "    # Update the overall measurement and metadata dictionaries with current station's data\n",
    "    all_stations_measurement_data[ename] = well_measurement_data\n",
    "    all_stations_metadata[ename] = well_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c69a9b-0084-4bad-b621-9ce44d3d2205",
   "metadata": {},
   "source": [
    "#### Update the HDF5 file with new data from earlier dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dd3fecd4-dd0a-45ee-b205-14581534acff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:27:22.479022Z",
     "iopub.status.busy": "2024-08-29T07:27:22.479022Z",
     "iopub.status.idle": "2024-08-29T07:27:22.499026Z",
     "shell.execute_reply": "2024-08-29T07:27:22.499026Z",
     "shell.execute_reply.started": "2024-08-29T07:27:22.479022Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10070111': {'measure': {'daily_value': array([1.86, 1.67, 1.17, ...,  nan,  nan,  nan])}},\n",
       " '10070121': {'measure': {'daily_value': array([-7.6 , -7.72, -7.81, ...,   nan,   nan,   nan])}},\n",
       " '10070131': {'measure': {'daily_value': array([-10.89, -11.02, -11.31, ...,    nan,    nan,    nan])}},\n",
       " '10070141': {'measure': {'daily_value': array([-12.73, -12.73, -12.71, ...,    nan,    nan,    nan])}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stations_measurement_data['ANHE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02b6ef1c-e450-4bd3-8a85-f34a5138ab00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T07:29:35.726139Z",
     "iopub.status.busy": "2024-08-29T07:29:35.725140Z",
     "iopub.status.idle": "2024-08-29T07:29:35.762139Z",
     "shell.execute_reply": "2024-08-29T07:29:35.762139Z",
     "shell.execute_reply.started": "2024-08-29T07:29:35.726139Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UpdatedTime': '2024/08/29, 11:54:19',\n",
       " '10070111': {'FIRST_OBS': '2001/01/01', 'LAST_OBS': '2024/08/20'},\n",
       " '10070121': {'FIRST_OBS': '2001/01/01', 'LAST_OBS': '2024/08/20'},\n",
       " '10070131': {'FIRST_OBS': '2001/01/01', 'LAST_OBS': '2024/08/20'},\n",
       " '10070141': {'FIRST_OBS': '2001/01/01', 'LAST_OBS': '2024/08/20'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_stations_metadata['ANHE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57ffbcb6-3ac7-441b-8fce-d9c7e1ed5047",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:26.655397Z",
     "iopub.status.busy": "2024-08-29T03:54:26.654396Z",
     "iopub.status.idle": "2024-08-29T03:54:27.046392Z",
     "shell.execute_reply": "2024-08-29T03:54:27.046392Z",
     "shell.execute_reply.started": "2024-08-29T03:54:26.655397Z"
    }
   },
   "outputs": [],
   "source": [
    "HDF5_fpath = os.path.join(script_folder, r\"20240828_GWL_CRFP.h5\")\n",
    "shutil.copy2(src=HDF5_fpath, dst=HDF5_fpath.replace(\".h5\", \"_secure.h5\"))\n",
    "\n",
    "# Extract existing data and metadata\n",
    "with h5py.File(HDF5_fpath, \"r\") as hdf5_file:\n",
    "    existing_data_dict = gwatertools.h5pytools.hdf5_to_data_dict(hdf5_file)\n",
    "    existing_metadata_dict = gwatertools.h5pytools.hdf5_to_metadata_dict(\n",
    "        hdf5_file\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb18df8-1925-4934-8e21-078495f47138",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:27.048396Z",
     "iopub.status.busy": "2024-08-29T03:54:27.047392Z",
     "iopub.status.idle": "2024-08-29T03:54:27.062397Z",
     "shell.execute_reply": "2024-08-29T03:54:27.060397Z",
     "shell.execute_reply.started": "2024-08-29T03:54:27.048396Z"
    }
   },
   "outputs": [],
   "source": [
    "# Update dictionaries\n",
    "updated_data_dict = gwatertools.h5pytools.update_data_dict(\n",
    "    existing_data_dict, all_stations_measurement_data\n",
    ")\n",
    "updated_metadata_dict = gwatertools.h5pytools.update_metadata_dict(\n",
    "    existing_metadata_dict, all_stations_metadata\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "379be60a-25ab-4ae8-a68f-4e8f6cd6e7ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-29T03:54:27.066397Z",
     "iopub.status.busy": "2024-08-29T03:54:27.065393Z",
     "iopub.status.idle": "2024-08-29T03:54:27.391392Z",
     "shell.execute_reply": "2024-08-29T03:54:27.390393Z",
     "shell.execute_reply.started": "2024-08-29T03:54:27.066397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Write updated data and metadata back to the HDF5 file\n",
    "with h5py.File(HDF5_fpath, \"w\") as hdf5_file:\n",
    "    metadata_to_hdf5(hdf5_file, updated_metadata_dict)\n",
    "    data_to_hdf5(hdf5_file, updated_data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
